[
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nIn addition to the standard variables of Height (in centimeters), Weight (in kilograms), and identified gender (Male, Female, Other), I have included twp additional variables: Generation and Salary. Generation represents the respondents age categorized into the self reported generation to which they belong, e.g. Gen Z are those with birth years between 1995 and 2012. Salary is the self reported annual salary in thousands of US dollars, e.g. a value of 70 would represent an annual salary of $70,000."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nfactor\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGeneration\n0\n1\nFALSE\n4\nGen: 5, Gen: 2, Bab: 1, Mil: 1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nSalary\n0\n1\nNA\nNA\nNA\n95.44444\n36.62005\n44\n70\n81\n133\n144\n▂▇▂▁▆"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#publication-quality-chart",
    "href": "presentation-exercise/presentation-exercise.html#publication-quality-chart",
    "title": "Presentation Exercise",
    "section": "Publication Quality Chart",
    "text": "Publication Quality Chart\nFor this exercise we are tasked with recreating a high-quality publication level chart found online using R and various AI tools. I have chosen to recreate the area chart on this page. Since this is coming from FiveThirtyEight and the original plot is interactive I have decided to recreate the plot with Highcharter.\nFirst we are tasked with requesting AI (in this case I have chosen to use ChatGPT) to attempt to recreate the plot, which I do with my initial prompt here:\n  Can you provide code using Highcharter in R that would recreate the area plot titled “Baby boomers are the biggest generation in Congress today” on this page https://fivethirtyeight.com/features/aging-congress-boomers/\n  raw data can be found here: https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\nThe first round of code had a handful of problems, primarily with processing the data, but it came pretty close in terms of structuring the overall request. I provided the column names in the raw data to help with processing, as well as correcting things like unnecessary filters. One of the outputs from ChatGPT ended up with pivoted data, which was not as easy to use for what we were doing, but the unpivoted data was still pretty close so I modified it myself and fed that back as a prompt.\nI’m still having problems with the highcharter section of code. I’ve modified the processing code slightly to adjust the pct values as well, because it did not look like it was previously giving the correct values for pct. I’ve also created a year variable to match what is done in the chart. Here is the processing code after my modifications: data1 &lt;- data %&gt;% mutate(year = as.integer(str_sub(start_date, 1, 4))) %&gt;% group_by(year, generation) %&gt;% summarise(count = n()) %&gt;% ungroup() %&gt;% group_by(year) %&gt;% mutate(pct = count / sum(count) * 100) Can you utilize this dataframe without pivoting to create the chart in Highcharter?\nThe output from this prompt got me most of the way there. The code runs, but some of the generations were out of order, and there were several tweaks needed to get the chart to look like the one on the page. Still the bones were there, so I took over from this point on, aside from some one off prompts to get exactly what I was looking for, with an example below.\nCan you adjust the tooltip in the chart to have the following format: Generation Year Percent (in percent format 00.0%) So for example: Missionary 1937 44.9%\nThe code and output below is the final product after the combined efforts of ChatGPT and myself.\n\n# Load libraries\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(highcharter)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n# Read the data from the URL\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\"\ndata &lt;- read_csv(url)\n\nRows: 29120 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): chamber, state_abbrev, bioname, bioguide_id, generation\ndbl  (6): congress, party_code, cmltv_cong, cmltv_chamber, age_days, age_years\ndate (2): start_date, birthday\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Calculate percentage of each generation in each congress\ndata1 &lt;- data %&gt;%\n  mutate(year = as.integer(str_sub(start_date, 1, 4))) %&gt;% \n  group_by(year, generation) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(year) %&gt;% \n  mutate(pct = count / sum(count) * 100)\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\ngen_order &lt;- data %&gt;% mutate(byear = as.integer(str_sub(birthday, 1, 4))) %&gt;% group_by(generation) %&gt;% summarize(oldyr = min(byear)) %&gt;% arrange(by = oldyr) %&gt;% select(generation)\n\n\ndata1$generation &lt;- fct_rev(fct_relevel(data1$generation, as.vector(gen_order)))\n\nWarning: Outer names are only allowed for unnamed scalar atomic inputs\n\n\n\ngeneration_colors &lt;- c(\n  \"#D26E8C\",\n  \"#92DCE0\",\n  \"#A593E9\",\n  \"#FD867E\",\n  \"#FDE384\",\n  \"#86D09D\",\n  \"#8B8887\",\n  \"#E98CCA\",\n  \"#FFE7E5\",\n  \"#E9E9E9\"\n)\n\n\nhighchart(type = \"chart\") %&gt;%\n  hc_chart(type = \"area\") %&gt;%\n  hc_title(text = \"&lt;b&gt;Baby boomers are the biggest generation in Congress today&lt;/b&gt;\",\n           align = 'left',\n           style = list(fontSize = \"24px\",\n                        fontFamily = 'Calibri')) %&gt;%\n  hc_subtitle(text = 'Share of members in Congress from each generation, 1919 to 2023',\n              align = 'left',\n              style = list(fontSize = \"20px\",\n                           fontFamily = 'Calibri')) %&gt;% \n  hc_xAxis(gridLineWidth = 0,\n    labels = list(\n    style = list(fontSize = \"16px\")\n    )\n    ) %&gt;%\n  hc_yAxis(max = 100,\n           labels = list(\n      formatter = JS(\"function() {\n                      if (this.value == 100) {\n                        return this.value + '%';\n                      } else {\n                        return this.value;\n                      }\n                    }\"),\n      style = list(fontSize = \"16px\")\n           )\n      ) %&gt;%\n  hc_add_series(data1, \"area\", hcaes(x = year, y = pct, group = generation),\n                color = generation_colors) %&gt;%\n  hc_legend(align = \"left\",\n            verticalAlign = \"top\",\n            layout = \"horizontal\",\n            itemStyle = list(fontFamily = 'Copperplate Gothic Light', fontSize = \"12px\")) %&gt;%\n  hc_tooltip(\n    headerFormat = \"&lt;b&gt;{series.name}&lt;/b&gt;&lt;br/&gt;\",\n    pointFormat = \"{point.x}&lt;br/&gt;{point.y:.1f}%\",\n    crosshairs = TRUE\n  ) %&gt;%\n  hc_plotOptions(\n    area = list(\n      stacking = \"normal\",\n      marker = list(enabled = FALSE,\n                    states = list(\n                      hover = list(\n                        enabled = FALSE\n                        )\n                      )\n                    )  # Disable markers\n      ),\n    series = list(\n      lineColor = '#EDF6E9',\n      trackByArea = TRUE,\n      stickyTracking = FALSE\n    )\n  ) %&gt;% hc_add_theme(hc_theme_538())\n\n\n\n\n\nI believe my output is pretty close to the original, with the exception of a few things like fonts, and the hover animations. I could not figure out how to get the crosshair for the tooltip to stay inside of the relevant generation group, nor could I figure out how to change the shape of the color indicators in the legend from circular to square, but even without those changes it still like it was high quality and close enough."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#publication-quality-tables",
    "href": "presentation-exercise/presentation-exercise.html#publication-quality-tables",
    "title": "Presentation Exercise",
    "section": "Publication Quality Tables",
    "text": "Publication Quality Tables\nUsing the same dataset, I wanted create a quality table that would show what the congressional membership distribution but include things like the mean age and count of members. The chart above does a great job of showing the relative distribution and the change over time, but the actual age is still an important factor. For example, we may have a lot of Congress members from the Baby Boomer generation in today’s Congress, but do they themselves skew younger or older?\nI decided I would use GT tables. I can’t say i have any experience with gt tables, so I thought this might be a challenge, but I was intrigued by the ability to use ggplots as images in the tables themself. I started off with ChatGPT again with the following prompt, continued off the prompts from teh Highcharter exercise:\n  Moving on to a new task with the same raw dataset. Can you create a publication quality table using the gt package in R with the following columns:\n  Generation, Mean Age, Count of Members, and an in-table distribution plot\n  And have the rows grouped by year, similar to a pivot table?\nThe output got some of the overall structure but was very lacking on the details, which again proved challenging due to my unfamiliarity with gt tables. For example, it knew I would need to create a way to call the plots formulaically, however it used the base histogram function instead of ggplot, which from what I can tell would not work with gt tables or at least not in an intuitive way. it also seemed to struggle with how to actually add the distribution plot in to the table, the code it generated would have added the plots after creating the tables initially, which is fine but it used the wrong functions to do so repeatedly. After trading prompts a number of times and getting mostly nowhere I decided to take matters into my own hands and researched how to use gt tables via the provided tutorial, which helped tremendously.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.3.3\n\n# Read the data from the URL\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\"\ndata &lt;- read_csv(url)\n\nRows: 29120 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): chamber, state_abbrev, bioname, bioguide_id, generation\ndbl  (6): congress, party_code, cmltv_cong, cmltv_chamber, age_days, age_years\ndate (2): start_date, birthday\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndata2 &lt;- data %&gt;%\n  select(start_date, generation, age_years)\n\n# Extract year from start_date\ndata2$year &lt;- as.integer(substr(data2$start_date, 1, 4))\n\ndata2 &lt;- data2 %&gt;% arrange(by = -year)\n\ngen_order &lt;- data %&gt;% mutate(byear = as.integer(str_sub(birthday, 1, 4))) %&gt;% group_by(generation) %&gt;% summarize(oldyr = min(byear)) %&gt;% arrange(by = oldyr) %&gt;% select(generation)\n\ndata2$generation &lt;- fct_rev(fct_relevel(data2$generation, as.vector(gen_order)))\n\nWarning: Outer names are only allowed for unnamed scalar atomic inputs\n\n# Group by year and generation, calculate mean age and count of members\ndata_summary &lt;- data2 %&gt;%\n  group_by(year, generation) %&gt;%\n  summarise(mean_age = mean(age_years, na.rm = TRUE),\n            count_members = n()) %&gt;%\n  ungroup() %&gt;%\n  filter(!is.na(mean_age)) %&gt;% arrange(by = -year) # Remove rows with NA mean_age for clarity\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\ndist_plot &lt;- function(fun_gen, fun_yr) {\n  full_range &lt;- data2 %&gt;% \n  pull(age_years) %&gt;% \n  range()\n  \n  data2 %&gt;% \n    filter(generation == !!fun_gen, year == !!fun_yr) %&gt;% \n    ggplot() +\n    geom_violin(aes(x=age_years, y = generation), fill = 'black') +\n    theme_minimal() +\n    scale_y_discrete(breaks = NULL) +\n    scale_x_continuous(breaks = NULL) +\n    labs(x = element_blank(), y = element_blank()) +\n    coord_cartesian(xlim = full_range)\n}\ndist_plot('Boomers', 2021) #testing function and plot appearance\n\n\n\n\n\n\n\n\n\n# Initialize the GT table\ngt_table &lt;- data_summary %&gt;%\n  group_by(generation, year) %&gt;% \n  mutate(Distribution = list(c(as.character(generation), year))) %&gt;%\n  ungroup() %&gt;% \n  gt(groupname_col = 'year', rowname_col = 'generation') %&gt;%\n  tab_header(\n    title = md(\"**Generation and Congress Membership Statistics**\"),\n    subtitle = \"Mean Age, Count of Members, and Distribution Plot by Year\"\n  ) %&gt;%\n  cols_label(\n    generation = md(\"**Generation**\"),\n    mean_age = md(\"**Mean Age**\"),\n    count_members = md(\"**Count**\"),\n    Distribution = md('**Distribution**')\n  ) %&gt;%\n  fmt_number(columns = c(mean_age), decimals = 0) %&gt;% \n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = cells_row_groups()\n  ) %&gt;% \n  tab_style(\n    style = cell_text(align = 'center'),\n    locations = cells_column_labels()\n  ) %&gt;% \n  text_transform(\n    locations = cells_body(columns = 'Distribution'),\n    fn = function(column) {\n      map(column, ~str_split_1(., ', ')) %&gt;% \n        map(~dist_plot(.[1], .[2])) %&gt;% \n        ggplot_image(height = px(30), aspect_ratio =  3)\n    }\n  ) %&gt;%\n  tab_footnote(\n    \"Note: Mean Age is rounded to nearest whole year. Distribution plots represent age distributions within each generation.\"\n  ) %&gt;% \n  tab_options(\n    data_row.padding = px(1),\n    row_group.padding = px(4)\n  )\ngt_table %&gt;% \n  opt_stylize(\n    style = 5, color = 'blue'\n    )\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\n\n\n\n\n\n\n\nGeneration and Congress Membership Statistics\n\n\nMean Age, Count of Members, and Distribution Plot by Year\n\n\n\nMean Age\nCount\nDistribution\n\n\n\n\n2023\n\n\nGen Z\n26\n1\n\n\n\nMillennial\n38\n55\n\n\n\nGen X\n50\n192\n\n\n\nBoomers\n67\n259\n\n\n\nSilent\n81\n29\n\n\n\n2021\n\n\nMillennial\n36\n37\n\n\n\nGen X\n49\n175\n\n\n\nBoomers\n65\n301\n\n\n\nSilent\n80\n38\n\n\n\n2019\n\n\nMillennial\n35\n26\n\n\n\nGen X\n47\n162\n\n\n\nBoomers\n63\n303\n\n\n\nSilent\n78\n53\n\n\n\n2017\n\n\nMillennial\n34\n6\n\n\n\nGen X\n45\n136\n\n\n\nBoomers\n61\n351\n\n\n\nSilent\n77\n62\n\n\n\n2015\n\n\nMillennial\n32\n4\n\n\n\nGen X\n44\n122\n\n\n\nBoomers\n60\n340\n\n\n\nSilent\n75\n75\n\n\n\n2013\n\n\nMillennial\n31\n3\n\n\n\nGen X\n42\n101\n\n\n\nBoomers\n58\n346\n\n\n\nSilent\n73\n95\n\n\n\nGreatest\n88\n3\n\n\n\n2011\n\n\nMillennial\n30\n1\n\n\n\nGen X\n41\n80\n\n\n\nBoomers\n56\n336\n\n\n\nSilent\n71\n123\n\n\n\nGreatest\n86\n6\n\n\n\n2009\n\n\nMillennial\n28\n1\n\n\n\nGen X\n39\n55\n\n\n\nBoomers\n55\n330\n\n\n\nSilent\n69\n160\n\n\n\nGreatest\n85\n7\n\n\n\n2007\n\n\nGen X\n38\n35\n\n\n\nBoomers\n53\n328\n\n\n\nSilent\n67\n176\n\n\n\nGreatest\n83\n10\n\n\n\n2005\n\n\nGen X\n36\n25\n\n\n\nBoomers\n52\n305\n\n\n\nSilent\n65\n199\n\n\n\nGreatest\n81\n11\n\n\n\n2003\n\n\nGen X\n35\n19\n\n\n\nBoomers\n50\n289\n\n\n\nSilent\n63\n217\n\n\n\nGreatest\n78\n14\n\n\n\n2001\n\n\nGen X\n33\n12\n\n\n\nBoomers\n48\n274\n\n\n\nSilent\n62\n239\n\n\n\nGreatest\n77\n22\n\n\n\n1999\n\n\nGen X\n32\n7\n\n\n\nBoomers\n46\n251\n\n\n\nSilent\n60\n252\n\n\n\nGreatest\n75\n29\n\n\n\n1997\n\n\nGen X\n30\n6\n\n\n\nBoomers\n45\n235\n\n\n\nSilent\n58\n268\n\n\n\nGreatest\n73\n35\n\n\n\n1995\n\n\nGen X\n29\n3\n\n\n\nBoomers\n43\n207\n\n\n\nSilent\n57\n285\n\n\n\nGreatest\n72\n48\n\n\n\n1993\n\n\nBoomers\n42\n164\n\n\n\nSilent\n55\n314\n\n\n\nGreatest\n70\n68\n\n\n\n1991\n\n\nBoomers\n41\n113\n\n\n\nSilent\n54\n331\n\n\n\nGreatest\n69\n104\n\n\n\n1989\n\n\nBoomers\n39\n95\n\n\n\nSilent\n52\n338\n\n\n\nGreatest\n67\n111\n\n\n\nLost\n88\n1\n\n\n\n1987\n\n\nBoomers\n37\n83\n\n\n\nSilent\n50\n330\n\n\n\nGreatest\n66\n130\n\n\n\nLost\n86\n1\n\n\n\n1985\n\n\nBoomers\n36\n67\n\n\n\nSilent\n48\n318\n\n\n\nGreatest\n64\n155\n\n\n\nLost\n84\n1\n\n\n\n1983\n\n\nBoomers\n34\n59\n\n\n\nSilent\n46\n308\n\n\n\nGreatest\n62\n176\n\n\n\nLost\n82\n1\n\n\n\n1981\n\n\nBoomers\n32\n40\n\n\n\nSilent\n45\n305\n\n\n\nGreatest\n60\n199\n\n\n\nLost\n80\n1\n\n\n\n1979\n\n\nBoomers\n31\n18\n\n\n\nSilent\n43\n277\n\n\n\nGreatest\n59\n247\n\n\n\nLost\n80\n2\n\n\n\n1977\n\n\nBoomers\n29\n10\n\n\n\nSilent\n42\n236\n\n\n\nGreatest\n58\n298\n\n\n\nLost\n78\n7\n\n\n\n1975\n\n\nBoomers\n27\n4\n\n\n\nSilent\n40\n189\n\n\n\nGreatest\n57\n343\n\n\n\nLost\n77\n15\n\n\n\n1973\n\n\nSilent\n40\n129\n\n\n\nGreatest\n55\n398\n\n\n\nLost\n75\n21\n\n\n\n1971\n\n\nSilent\n39\n92\n\n\n\nGreatest\n54\n422\n\n\n\nLost\n74\n34\n\n\n\n1969\n\n\nSilent\n37\n65\n\n\n\nGreatest\n53\n441\n\n\n\nLost\n73\n45\n\n\n\n1967\n\n\nSilent\n36\n45\n\n\n\nGreatest\n51\n436\n\n\n\nLost\n71\n60\n\n\n\nMissionary\n87\n2\n\n\n\n1965\n\n\nSilent\n34\n30\n\n\n\nGreatest\n50\n437\n\n\n\nLost\n69\n79\n\n\n\nMissionary\n85\n2\n\n\n\n1963\n\n\nSilent\n32\n17\n\n\n\nGreatest\n49\n415\n\n\n\nLost\n67\n115\n\n\n\nMissionary\n84\n4\n\n\n\n1961\n\n\nSilent\n32\n5\n\n\n\nGreatest\n48\n390\n\n\n\nLost\n66\n154\n\n\n\nMissionary\n82\n9\n\n\n\n1959\n\n\nSilent\n31\n1\n\n\n\nGreatest\n47\n368\n\n\n\nLost\n64\n169\n\n\n\nMissionary\n81\n14\n\n\n\n1957\n\n\nGreatest\n46\n305\n\n\n\nLost\n63\n215\n\n\n\nMissionary\n79\n25\n\n\n\n1955\n\n\nGreatest\n45\n278\n\n\n\nLost\n61\n235\n\n\n\nMissionary\n76\n27\n\n\n\n1953\n\n\nGreatest\n44\n251\n\n\n\nLost\n59\n263\n\n\n\nMissionary\n75\n38\n\n\n\n1951\n\n\nGreatest\n43\n220\n\n\n\nLost\n57\n283\n\n\n\nMissionary\n74\n50\n\n\n\n1949\n\n\nGreatest\n41\n198\n\n\n\nLost\n56\n289\n\n\n\nMissionary\n72\n66\n\n\n\n1947\n\n\nGreatest\n40\n167\n\n\n\nLost\n54\n297\n\n\n\nMissionary\n70\n89\n\n\n\n1945\n\n\nGreatest\n40\n120\n\n\n\nLost\n53\n316\n\n\n\nMissionary\n68\n127\n\n\n\nProgressive\n87\n1\n\n\n\n1943\n\n\nGreatest\n38\n92\n\n\n\nLost\n51\n306\n\n\n\nMissionary\n67\n155\n\n\n\nProgressive\n85\n1\n\n\n\n1941\n\n\nGreatest\n36\n74\n\n\n\nLost\n49\n311\n\n\n\nMissionary\n65\n173\n\n\n\nProgressive\n84\n3\n\n\n\n1939\n\n\nGreatest\n34\n54\n\n\n\nLost\n47\n293\n\n\n\nMissionary\n64\n211\n\n\n\nProgressive\n80\n3\n\n\n\n1937\n\n\nGreatest\n33\n32\n\n\n\nLost\n46\n271\n\n\n\nMissionary\n62\n249\n\n\n\nProgressive\n79\n2\n\n\n\n1935\n\n\nGreatest\n31\n21\n\n\n\nLost\n44\n239\n\n\n\nMissionary\n61\n284\n\n\n\nProgressive\n77\n5\n\n\n\n1933\n\n\nGreatest\n30\n9\n\n\n\nLost\n43\n198\n\n\n\nMissionary\n60\n329\n\n\n\nProgressive\n76\n11\n\n\n\n1931\n\n\nGreatest\n27\n2\n\n\n\nLost\n41\n146\n\n\n\nMissionary\n58\n396\n\n\n\nProgressive\n74\n19\n\n\n\n1929\n\n\nGreatest\n27\n1\n\n\n\nLost\n40\n107\n\n\n\nMissionary\n57\n424\n\n\n\nProgressive\n73\n34\n\n\n\nGilded\n88\n1\n\n\n\n1927\n\n\nLost\n39\n89\n\n\n\nMissionary\n55\n414\n\n\n\nProgressive\n71\n47\n\n\n\nGilded\n86\n1\n\n\n\n1925\n\n\nLost\n37\n69\n\n\n\nMissionary\n54\n419\n\n\n\nProgressive\n69\n59\n\n\n\nGilded\n84\n1\n\n\n\n1923\n\n\nLost\n36\n49\n\n\n\nMissionary\n52\n431\n\n\n\nProgressive\n68\n74\n\n\n\nGilded\n84\n3\n\n\n\n1921\n\n\nLost\n36\n31\n\n\n\nMissionary\n50\n418\n\n\n\nProgressive\n66\n107\n\n\n\nGilded\n83\n4\n\n\n\n1919\n\n\nLost\n33\n19\n\n\n\nMissionary\n49\n412\n\n\n\nProgressive\n65\n119\n\n\n\nGilded\n80\n5\n\n\n\n\nNote: Mean Age is rounded to nearest whole year. Distribution plots represent age distributions within each generation.\n\n\n\n\n\n\n\n\n\nI’m fairly satisfied with the output. The inline violin charts were especially satisfying to make, and I’m happy with the availability of all the different customization options, while simultaneously there are several quick theme options to make styling even easier. That said, the syntax is less intuitive than I would prefer, I was truly hoping for a more ggplot-like experience."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#general-background-information",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#general-background-information",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "General Background Information",
    "text": "General Background Information\nLast year, Austin was ranked among the top US cities with a problem in homicide rate [1]. Earlier this year, the claim was made that crime has dropped to a new low since 2020 [2]. The intention of this analysis will be to investigate both claims, and gain a better understanding of Austin’s criminal activity. While these claims will be in mind throughout the steps of the analysis, I intend to approach it from more of an exploratory standpoint, so not to only go out and investigate the claims but really any insights that can be gleaned around Austin criminal activity and how it has changed over time. What the analysis will not cover would be things like causes of the changes in crime, at least not directly. I may identify shifts that could inform the cause, but the goal will not be to drill down into every potential socioeconomic factor and pick the primary drivers, instead simply to observe what crime in the city of Austin looks like and what we might be able to expect going forward. Aside from resources outlined in our course resources, I will also draw from this book [3]. The final output of this project, the manuscript you are reading, is rendered in my github analytics portfolio. The original repo dedicated to just this project can be found here."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#description-of-data-and-data-source",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#description-of-data-and-data-source",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Description of data and data source",
    "text": "Description of data and data source\nMy primary data source will come from here. The data is updated weekly by the Austin Police Department, and each record in the dataset represents an Incident Report, with the highest offense within an incident taking precedence in things like the description and categorization Each Incident can have other offense tied to it, however since each record is a unique incident then only the aforementioned Higehest Offense is the one represented (NOTE: At the time of this writing, 7/31/2024, the dataset has been taken down to be replaced with one that aligns more closely with the FBI National Incident Based Reporting System. The datasets are not one-to-one, so reproducibility would require more than a lift and shift, however some of the methods could still be employed).\nThe raw data is represented by several categorical, location, and time-based variables, many of which have missing values, or are formatted incorrectly for the data type, so they will need to be cleaned or recoded. After cleaning the data (done in a separate file found in this repo) we can take a look a more meaningful look.\n\n\nCode\nskim(d1)\n\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n2461621\n\n\nNumber of columns\n28\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n12\n\n\nDate\n3\n\n\ndifftime\n2\n\n\nnumeric\n9\n\n\nPOSIXct\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHighest.Offense.Description\n0\n1\n3\n48\n0\n436\n0\n\n\nFamily.Violence\n0\n1\n1\n1\n0\n2\n0\n\n\nLocation.Type\n0\n1\n7\n47\n0\n47\n0\n\n\nAddress\n0\n1\n8\n74\n0\n246951\n0\n\n\nAPD.Sector\n0\n1\n2\n5\n0\n14\n0\n\n\nAPD.District\n0\n1\n1\n2\n0\n21\n0\n\n\nPRA\n0\n1\n1\n4\n0\n742\n0\n\n\nClearance.Status\n0\n1\n0\n1\n615856\n4\n0\n\n\nUCR.Category\n0\n1\n0\n3\n1550375\n17\n0\n\n\nCategory.Description\n0\n1\n0\n18\n1550375\n8\n0\n\n\nLocation\n0\n1\n0\n27\n32335\n219842\n0\n\n\nCrime.Category\n0\n1\n4\n29\n0\n33\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Date\n0\n1.00\n2003-01-01\n2024-06-01\n2012-05-28\n7823\n\n\nReport.Date\n0\n1.00\n2002-11-29\n2024-06-02\n2012-06-06\n7825\n\n\nClearance.Date\n348308\n0.86\n2003-01-01\n2024-06-02\n2012-10-17\n7814\n\n\n\nVariable type: difftime\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Time\n0\n1\n0 secs\n86340 secs\n14:25:00\n1440\n\n\nReport.Time\n0\n1\n0 secs\n86340 secs\n14:06:00\n1440\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nIncident.Number\n0\n1.00\n6.031558e+10\n2.896224e+11\n20035.00\n2.005329e+10\n2.010505e+10\n2.017186e+10\n2.024242e+12\n▇▁▁▁▁\n\n\nHighest.Offense.Code\n0\n1.00\n1.689080e+03\n1.218280e+03\n100.00\n6.010000e+02\n1.199000e+03\n2.716000e+03\n8.905000e+03\n▇▅▁▁▁\n\n\nZip.Code\n0\n1.00\n7.873243e+04\n2.510000e+01\n76574.00\n7.871700e+04\n7.874100e+04\n7.875200e+04\n7.875900e+04\n▁▁▁▁▇\n\n\nCouncil.District\n30699\n0.99\n4.960000e+00\n2.840000e+00\n1.00\n3.000000e+00\n4.000000e+00\n7.000000e+00\n1.000000e+01\n▅▇▃▃▅\n\n\nCensus.Tract\n8822\n1.00\n2.453700e+02\n3.363970e+03\n1.00\n1.500000e+01\n2.324000e+01\n3.380000e+02\n9.508000e+05\n▇▁▁▁▁\n\n\nX.coordinate\n0\n1.00\n3.075787e+06\n3.551571e+05\n0.00\n3.108421e+06\n3.117292e+06\n3.126595e+06\n3.231806e+06\n▁▁▁▁▇\n\n\nY.coordinate\n0\n1.00\n9.946761e+06\n1.147895e+06\n0.00\n1.005743e+07\n1.007300e+07\n1.010056e+07\n1.021550e+07\n▁▁▁▁▇\n\n\nLatitude\n32335\n0.99\n3.029000e+01\n8.000000e-02\n30.01\n3.023000e+01\n3.028000e+01\n3.035000e+01\n3.067000e+01\n▁▇▇▂▁\n\n\nLongitude\n32335\n0.99\n-9.773000e+01\n5.000000e-02\n-98.18\n-9.776000e+01\n-9.773000e+01\n-9.770000e+01\n-9.737000e+01\n▁▁▇▂▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Date.Time\n0\n1\n2003-01-01 00:00:00\n2024-06-01 23:46:00\n2012-05-28 23:09:00\n1738386\n\n\nReport.Date.Time\n0\n1\n2002-11-29 05:30:00\n2024-06-02 01:20:00\n2012-06-06 11:15:00\n2169726\n\n\n\n\n\nI preserved quite a few variables from the original dataset, but the primary ones of interest will be Occurred.Date.Time, Zip.Code, Highest.Offense.Description, Category.Description, and Crime.Category. The last few variables pertaining to the type of crime committed are really all rolled into the last one, Crime.Category for the purposes of this analysis. Crime Category is a derived field that categorizes crimes into several different categories for the sake of understanding what crime occurred but without getting flooded with minute details. The categorizations were done manually by myself, and can be seen in the processing file as well as a description of the method performed. The short version is the categories are based on the UCR Crime descriptions given by the FBI when available, and are bucketed similarly from the Highest Offense field via string detection. This brings the number of unique categories in the Highest Offense field from 436 to 32 in the derived field."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#questionshypotheses-to-be-addressed",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#questionshypotheses-to-be-addressed",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Questions/Hypotheses to be addressed",
    "text": "Questions/Hypotheses to be addressed\nUltimately, I would like to explore a few questions:\n\nHas crime truly dropped, or is it expected to rise again as the year progresses?\nHas the homicide rate dropped with crime?\nHas the homicide rate been a problem for long time, or was it truly an emerging problem last year?"
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#schematic-of-workflow",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#schematic-of-workflow",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Schematic of workflow",
    "text": "Schematic of workflow\nThe intention of this analysis is to be exploratory; while I do have some questions I would like to chase down, I intend for them to be more of a compass than a map. With that said, the general strategy taken can be boiled down as follows:\n\nCheck for data quality, understand data representation\nClean the data, address problems identified in previous step, some exploration\nMore “formal” exploration. Consider what variables might be most important, and how those variables can be represented in the analysis.\nRepeat any steps performed above as necessary.\nResolve remaining or initial questions using statistical methods."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#exploratorydescriptive-analysis",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#exploratorydescriptive-analysis",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Exploratory/Descriptive analysis",
    "text": "Exploratory/Descriptive analysis\n\nA time series plot is a very natural starting part for the exploration of this analysis. given that the data is at the incident level, it is worth the effort to aggregate the count of occurrences up to some interval of time. We have values of all the way down to reported time of occurrence, but given the imputations that needed to occur in the data cleaning step, as well as the questionable reliability of whether the reported occurrence time is accurate to begin with, daily seems like the smallest interval of time that could be worth aggregating over. Figure 1 below is the corresponding aggregated time series plot. While we can see a general trend, the plot is rather dense since there are so many points plotted along the x-axis. If we want to do anything like isolating down to specific crimes like homicide, we will needed to aggregate at less fine of a grain, otherwise we will have too many intervals with 0 values for the observations.\n\n\n\n\n\n\n\n\nFigure 1: Crime Forecast Residual Plots\n\n\n\n\n\nFigure 2 below is the corresponding time series plot aggregated over each month. We still see a similar rise and fall trend like we did in the daily plot, but it is more pronounced since there are fewer outliers like there were in the daily plot. Outside of the overall trend, we can also see some potential patterns like a seasonal trend, and a change in the overall trend around 2018 until around 2020 where it briefly increases again before returning to a general decrease.\n\n\n\n\n\n\n\n\nFigure 2: Crime Forecast Residual Plots\n\n\n\n\n\nJumping ahead just a bit, after performing the simple lexicon based classification and properly identifying murder crimes, we can isolate to those crimes specifically for a time series plot. Figure 3 is exactly this, and we can see the trend is very different from what is seen in the overall time-series plot. For the most part, the rate of murder is relatively low up through 2015, and it is around 2016 that we see it spike and then slowly increase up until about 2022, where it then starts to slow back down.\n\n\n\n\n\n\n\n\nFigure 3: Monthly Murder Line Plot\n\n\n\n\n\nThese changes are interesting, primarily because we see the same shift before 2020 across overall crime and murder, but the number of murders is too small to be the primary driver in the overall crime trend. It’s also interesting because of the way the increase in murders start happening before the overall increase, and continues after the overall has started to decrease again. These may be caused by correlated factors like population, economic, or political factors, that the rate of murder could be more sensitive to than other crimes are. Furthermore, because the rate of murder is so low (relative to other types of crime), it’s likely that the variance in the trend is always going to be more sensitive to external factors. Once again the goal of this analysis is not intended to explain the why, but I think these observations could make for some good future studies into the potential causes or correlations. For now, the main question that these visuals bring me are the apparent trends; are they actually material, is the decrease seen the result of a change in the actual rate, or merely happenstance and we can expect the rate of murder to increase in future months? Before I attempt to explore this question, I want to see what variables might help me to explain it. The last chart considers the type of crime, which we will see in a moment, but it does not consider the location. For location, I have created a visualization based on Zip Code that I think can be revealing.\n\n\nCode\ncplotly\n\n\n\n\n\n\n\nThe interactive plot above shows us the number of crimes that occurred each year within each zip code, with the number of crimes represented by the continuous color scale, and the years represented by the animation so that values will change as the animation plays. We can see the overall decrease in crime from the way the Zip Codes that are more red slowly start to shift to blue or white. The zip codes that are more blue to start with don’t seem to change much as the animation plays. This suggests that the bulk of the crime is isolated to a handful of locations, and either factors that typically lead to crime are becoming less relevant in those areas, or the areas with heavier crime are seeing a greater focus in terms of prevention. This visualization is great, but with the analysis being more geared for a focus on Murder, it is unlikely to be of much help. There are probably Zip Codes where murder is more likely to occur, but the lower quantities would make it difficult to use that as a method to predict the number of murders that will happen in an interval of time small enough to have enough observations to make such a prediction in the first place. This chart does leave some questions that would be good for a future study, namely what has changed in those Zip Codes that are decreasing? Are there specific crimes that we are seeing fewer occurrences of, or is there better prevention in those areas?\nThe exploration has considered time and location as a variable, but the type of crime is still largely unexplored. The raw data, as stated before, has 434 different descriptions for the highest offense in each incident that was committed over the time period observed. The number of these descriptions makes it difficult to do anything with as a category, but coupled with other variables in the datset like the UCR Category may help us understand a better categorical representation. To accomplish this, crime that already had a UCR description for the observation was assigned this description as its category. A general description of each of these and several others can be found here[4]. Observations in this dataset were given a UCR Category code when they were considered a more serious crime identified by the FBI (this was given in the data dictionary that came with the data. The data dictionary is no longer available since the dataset was removed, but can be found in the repo for this analysis on github in the assets folder, or from the processing file directly since I have it copied there). Because there were many observations that were not given a category in the variable field, a lexicon was built by observing each individual Offense descriptions to find a way to group each offense description into a smaller categorical variable. Make no mistake, this categorization is not official categorization of the crimes themselves, nor is it intended to be a one for one match to any other categorization used by either APD or the FBI. However, it can be interpreted as one person’s layman’s terms attempt at categorizing these offenses. Essentially, how might an average person who does not regularly encounter law enforcement or crime think of each of these offenses? With that in mind, after observing each of the Highest Offense description, key words and phrases were identified that could capture either unique or similar (when possible) descriptions of the crimes, then string detection and regular expressions were used to identify observations with those descripions and then group them into a category of other similar offenses. We can see exactly how each offense description was grouped in the table below.\n\n\nCode\nRecatTable\n\n\n\n\n\nCrime Recategorization Key\n\n\n\n\n\n\n\n\nThe total count, yearly mean, and yearly standard deviation of occurrences of the offenses is provided to illustrate the disparity of the frequency of several of the offense descriptions. This is not entirely resolved by the New Category, but it does help for many of them. Again, the new categories are meant to generalize the descriptions. Page 21 of the table above contains the descriptions that I categorized as murder. A good example of how this method is imperfect is there, because of the presence of manslaughter. My initial thought process was to group instances of manslaughter into its own category, which is ultimately where I would have put Justified Homicide as well, however there were occurrences of manslaughter that were already categorized as murder by the UCR categorization, so bringing in those to the murder category would leave Justified Homicide in its own category, despite being a crime with a relatively low rate of occurrence. Ultimately this turned out to be the right call, because the FBI UCR website referenced earlier states that the “program classifies justifiable homicides separately,”[4] but for other cases not explicitly stated on this or other sites I may have just gotten it wrong judging by my first instinct. All said, a more summarized version of this chart is below, complete with sparklines as well to give an idea of the rate over time of each category.\n\n\nThis table is intended to give a general idea of the change of each crime over time and the general quantity relative to others. We can again see that our specific crime of focus, Murder, is lower than many other types, but is far from the lowest and still has a quantifiable rate. The sparklines don’t do the difference between crimes justice, because there are several that have a far greater rate of occurrence than others, but we can visualize this and the year over year change with the chart below.\n\n\nCode\ncatbarly\n\n\n\n\n\n\n\nAgain, the rate of murder is generally so low that it is dwarfed by the occurrences of most other crimes (particularly theft). We can see the bar chart peak out towards the end in 2021 and beyond, which is exactly what we expect. Overall, the rate of murder is so low compared to others that we may not be able to give it the same treatment as others, but begs the question if the rate of crime as a whole could somehow be used to help us predict each individual type of crime. We will use this idea during the Forecasting steps described below."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#time-series-and-forecasting",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#time-series-and-forecasting",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Time Series and Forecasting",
    "text": "Time Series and Forecasting\n\nTo determine if the recent decreases seen in crime overall and murder are coincidence or not, we need to establish what our expectation is. There are many ways we could do this, but a forecast seems the best, given the time series nature of our data. A few different time series models were attempted, like ARIMA and Exponential Smoothing, but the confidence interval for these was too wide for any group to make any reasonable inference with. However, utilizing the Fable package, a top-down hierarchical Seasonal ARIMA model is fit to the data and gives us a much more reasonable fit, both for the aggregated level and for the individual crime level for Murder. To fit the model the specific model, automatic model selection in the fable package was utilized while reconciling with a top-down hierarchical method. The selected model for the aggregate level is an ARIMA(0,1,2)(2,1,1)12. We can see from the residual plots for All Crime that the fit is generally fair. Innovation residuals are mostly centered around 0, the ACF plot suggests no autocorrelation, and even the distribution of the residuals is pretty normal, though there is a high frequency at 0 and just below 0 that may be questionable. The Ljung-Box test was not found to be significant, meaning we can reject the null hypothesis that the residuals are distinguishable from white noise agreeing with our observations from the ACF plot. The most questionable aspect here is likely the heteroscedasticity of the residuals, which appears to decrease and then increase over time, but this is not a requirement for a good time series model.\n\n\n\n\n\n\n\n\nFigure 4: Crime Forecast Residual Plots\n\n\n\n\n\n\n\nIt is not quite the same story regarding the residuals for murder. The selected model for the frequency of murder is ARIMA(0,1,1)(0,0,1)12. The plot of the residuals over time is still mostly centered at zero but the variance is less steady than it was for all crime, increasing quite a bit in the last few years, which makes sense since that is when we started to see the rate of Murder increase. Two spikes in the ACF plot are found to be significant, but this is not terribly abormal since 24 spikes are plotted, and the Ljung-Box test results are also not found to be significant. The distribution of the residuals is the most questionable, it is fairly asymmetrical so it is unlikely to be normally distributed, but again this is less important than the requirements for uncorrelated and mean 0 residuals.\n\n\n\n\n\n\n\n\nFigure 5: Murder Forecast Residual Plots\n\n\n\n\n\n\n\nThe plots below show these forecasts, the aggregation all crime and for murder, as well as two other crimes picked for their similarity to murder, Aggregated Assault and Unlawful Restraint (kidnapping and similar crimes). Aggravated Assault was chosen due to its similarity in how the crime is performed, and Unlawful Restraint was chosen due to the number of occurrences being similar to Murder. The plot for Murder is repeated enlarged for readability.\n\n\n\n\n\n\n\n\nFigure 6: Monthly Crime Forecast\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Murder Forecast\n\n\n\n\n\nFrom these forecasts, we can see that for overall crime the decrease observed earlier is expected to continue, however the confidence interval is quite wide so it could just as easily change course and begin to increase . For Murder, the decrease after the observed period is much less pronounced, but the confidence interval is much smaller . The difference in confidence interval is likely due to the smaller magnitude of crime counts compared to the aggregated model, and in this case the bounds are often not enough for more than one integer value between . That’s somewhat problematic early on, because it means the forecast is truly only predicting one value since it is a count, but it matters very little since the last value is well outside of any of the confidence bounds . The table below shows the point estimates of the forecast versus the exact value, and we can see that the the estimates were not terribly bad except for March, where we see the large dip in the plot . It is worth noting that April and May were not plotted since the article that spawned the question at the heart of this analysis was only observing the count of murders through the first quarter of the year ."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#interpretation-and-conlusions",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#interpretation-and-conlusions",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Interpretation and Conlusions",
    "text": "Interpretation and Conlusions\n\nThe findings in the analysis are somewhat inconclusive, though they were really intended to be exploratory. There are some clear trends in crime rate over time, and forecasting methods do not look to be beyond the realm of possibility but should should certainly be taken with a grain of salt. There was a drop in the count of Murders in the first quarter, primarily due to a drop in the month of March. The forecast suggested it was expected to be higher, but we are talking about a difference of 2.7 actual vs forecast. The counts of murder are so low that there is bound to be error, and if not it would likely be due confidence intervals so wide they would be almost meaningless. with that in mind, the table above suggests that we have 5 fewer murders in the first 5 months of 2024 than expected, so cautious optimism may be warranted.\nThe model itself is certainly not perfect, and perhaps future analyses could explore ways to improve the predictability. Hierarchical time series models can have multiple levels in their hierarchy, so it’s not impossible that other variables could have been combined with the category to improve results. Other crimes may also be more worthwhile in predicting; theft as an example dwarfed all other crimes and may very well be the biggest opportunity for the city of Austin. Models built on other higher frequency crimes can also likely make better use of location variables as well. More recent crime reporting data on the Austin data portal site have different methods of classification, and include individual charges with each incident, so it seems that they would be ripe for new variables to be implemented. Socio-economic variable could also potentially be used to see the impacts that they have on the rates of crime as whole or certain specific crimes. Comparisons with other cities would also be interesting, especially those with similarities to Austin to see how individual differences could lead to changes in the city’s environment. These are all ideas I could see myself returning to this analyses for, and I hope this analyses can provide myself or anyone else some inspiration for a more detailed analysis."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#the-following-section-is-contributed-by-zane-chumley.",
    "href": "coding-exercise/coding-exercise.html#the-following-section-is-contributed-by-zane-chumley.",
    "title": "R Coding Exercise",
    "section": "The following section is contributed by ZANE CHUMLEY.",
    "text": "The following section is contributed by ZANE CHUMLEY.\n\nPick a dataset\n\n# It's an election year, so let's look at the polls from the year Trump eventually won.\nZaneA03 &lt;- results_us_election_2016\n\n\n\nExplore the dataset\n\n# look at the data's type\nclass(ZaneA03)\n\n[1] \"data.frame\"\n\n# look at the data's structure\nstr(ZaneA03)\n\n'data.frame':   51 obs. of  5 variables:\n $ state          : chr  \"California\" \"Texas\" \"Florida\" \"New York\" ...\n $ electoral_votes: int  55 38 29 29 20 20 18 16 16 15 ...\n $ clinton        : num  61.7 43.2 47.8 59 55.8 47.9 43.5 45.9 47.3 46.2 ...\n $ trump          : num  31.6 52.2 49 36.5 38.8 48.6 51.7 51 47.5 49.8 ...\n $ others         : num  6.7 4.5 3.2 4.5 5.4 3.6 4.8 3.1 5.2 4 ...\n\n# look at a summary of the data\nsummary(ZaneA03)\n\n    state           electoral_votes    clinton          trump      \n Length:51          Min.   : 3.00   Min.   :21.90   Min.   : 4.10  \n Class :character   1st Qu.: 4.50   1st Qu.:36.00   1st Qu.:41.15  \n Mode  :character   Median : 8.00   Median :46.20   Median :48.70  \n                    Mean   :10.55   Mean   :44.79   Mean   :48.45  \n                    3rd Qu.:11.50   3rd Qu.:51.75   3rd Qu.:57.40  \n                    Max.   :55.00   Max.   :90.90   Max.   :68.60  \n     others      \n Min.   : 1.900  \n 1st Qu.: 4.650  \n Median : 5.800  \n Mean   : 6.767  \n 3rd Qu.: 7.450  \n Max.   :27.000  \n\n\nIt is worth noting that the dataset is significantly less detailed than described in https://cran.r-project.org/web/packages/dslabs/dslabs.pdf. While there are only 5 columns in the dataset, the description indicated many more columns would be provided:\n\nstate. State in which poll was taken. ’U.S‘ is for national polls.\nstartdate. Poll’s start date.\nenddate. Poll’s end date.\npollster. Pollster conducting the poll.\ngrade. Grade assigned by fivethirtyeight to pollster.\nsamplesize. Sample size.\npopulation. Type of population being polled.\nrawpoll_clinton. Percentage for Hillary Clinton.\nrawpoll_trump. Percentage for Donald Trump\nrawpoll_johnson. Percentage for Gary Johnson\nrawpoll_mcmullin. Percentage for Evan McMullin.\nadjpoll_clinton. Fivethirtyeight adjusted percentage for Hillary Clinton.\najdpoll_trump. Fivethirtyeight adjusted percentage for Donald Trump\nadjpoll_johnson. Fivethirtyeight adjusted percentage for Gary Johnson\nadjpoll_mcmullin. Fivethirtyeight adjusted percentage for Evan McMullin\n\n\n\nDo any processing/cleaning you want to do\nFrom the exploration above it does not appear there are any NA values in the data. Let’s check to be sure.\n\n# Any NA values in the state column?\nZaneA03.state.NAs &lt;- ZaneA03[ZaneA03$state==\"NA\",]\nstr(ZaneA03.state.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n# Any NA values in the electoral_votes column?\nZaneA03.electorals.NAs &lt;- ZaneA03[ZaneA03$electoral_votes==\"NA\",]\nstr(ZaneA03.electorals.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n# Any NA values in the clinton column?\nZaneA03.clinton.NAs &lt;- ZaneA03[ZaneA03$clinton==\"NA\",]\nstr(ZaneA03.clinton.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n# Any NA values in the trump column?\nZaneA03.trump.NAs &lt;- ZaneA03[ZaneA03$trump==\"NA\",]\nstr(ZaneA03.trump.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n# Any NA values in the others column?\nZaneA03.others.NAs &lt;- ZaneA03[ZaneA03$others==\"NA\",]\nstr(ZaneA03.others.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n\nThere are no objects in any of the five (5) datasets housing NA values. Therefore, no cleaning is warranted.\nBut are there any outliers?\n\n\nMake a few exploratory figures.\n\n# Let's use boxplots to see if there are any outliers in the four (4) columns containing numerical data\nboxplot(ZaneA03$electoral_votes\n        , main=\"Boxplot of Electorcal Votes\"\n        , xlab=\"\"\n        , horizontal=TRUE\n        )\n\n\n\n\n\n\n\nboxplot(ZaneA03$clinton\n        , main=\"Boxplot of Clinton poll readings\"\n        , xlab=\"\"\n        , horizontal=TRUE\n        )\n\n\n\n\n\n\n\nboxplot(ZaneA03$trump\n        , main=\"Boxplot of Trump poll readings\"\n        , xlab=\"\"\n        , horizontal=TRUE\n        )\n\n\n\n\n\n\n\nboxplot(ZaneA03$others\n        , main=\"Boxplot of pool readings for other candidates\"\n        , xlab=\"\"\n        , horizontal=TRUE\n        )\n\n\n\n\n\n\n\n\n\n\nOptionally, also some tables.\n\n# Let's display all the rows containing an outlier revealed by the boxplots above.\n\n# We'll sorting the rows by the values in each column into new datasets.\n# The sorting will be largest values first.\n# Then we will display the top and/or the bottom of the dataset corresponding to the upper and lower outliers, respectively.\nZaneA03.electorals.sorted &lt;- ZaneA03[order(-ZaneA03$electoral_votes),]\nhead(ZaneA03.electorals.sorted\n     , n=3\n     )\n\n       state electoral_votes clinton trump others\n1 California              55    61.7  31.6    6.7\n2      Texas              38    43.2  52.2    4.5\n3    Florida              29    47.8  49.0    3.2\n\nZaneA03.clinton.sorted &lt;- ZaneA03[order(-ZaneA03$clinton),]\nhead(ZaneA03.clinton.sorted\n     , n=1\n     )\n\n                  state electoral_votes clinton trump others\n51 District of Columbia               3    90.9   4.1      5\n\nZaneA03.trump.sorted &lt;- ZaneA03[order(-ZaneA03$trump),]\ntail(ZaneA03.trump.sorted\n     , n=1\n     )\n\n                  state electoral_votes clinton trump others\n51 District of Columbia               3    90.9   4.1      5\n\nZaneA03.others.sorted &lt;- ZaneA03[order(-ZaneA03$others),]\nhead(ZaneA03.others.sorted\n     , n=5\n     )\n\n        state electoral_votes clinton trump others\n35       Utah               6    27.5  45.5   27.0\n40      Idaho               4    27.5  59.3   13.2\n49    Vermont               3    56.7  30.3   13.1\n44     Alaska               3    36.6  51.3   12.2\n37 New Mexico               5    48.3  40.0   11.7\n\n\n\n\nRun some simple statistical model(s). Your choice.\nHow successful were the polls in predicting which candidate ultimately carried the state in the election? Well, we’ll need another dataset … the results from the voting.\n\n# Load actual votes from 2016 \nZaneA03.votedata &lt;- read_xlsx(\"1976-2020-president.xlsx\"\n                              , sheet=\"2016Flat\"\n                              , col_names = TRUE\n                              )\n# look at the data's type\nclass(ZaneA03.votedata)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# look at the data's structure\nstr(ZaneA03.votedata)\n\ntibble [51 × 9] (S3: tbl_df/tbl/data.frame)\n $ State       : chr [1:51] \"ALABAMA\" \"ALASKA\" \"ARIZONA\" \"ARKANSAS\" ...\n $ VotesClinton: num [1:51] 729547 116454 1161167 380494 8753788 ...\n $ VotesTrump  : num [1:51] 1318255 163387 1252401 684872 4483810 ...\n $ VotesOther  : num [1:51] 75570 38767 159597 65269 943997 ...\n $ TotalVotes  : num [1:51] 2123372 318608 2573165 1130635 14181595 ...\n $ ActClinton  : num [1:51] 34.4 36.6 45.1 33.7 61.7 ...\n $ ActTrump    : num [1:51] 62.1 51.3 48.7 60.6 31.6 ...\n $ ActOthers   : num [1:51] 3.56 12.17 6.2 5.77 6.66 ...\n $ TrumpWin    : logi [1:51] TRUE TRUE TRUE TRUE FALSE FALSE ...\n\n# look at a summary of the data\nsummary(ZaneA03.votedata)\n\n    State            VotesClinton       VotesTrump        VotesOther    \n Length:51          Min.   :  55973   Min.   :  12723   Min.   : 17022  \n Class :character   1st Qu.: 297584   1st Qu.: 377422   1st Qu.: 56876  \n Mode  :character   Median : 780154   Median : 949136   Median : 93418  \n                    Mean   :1291247   Mean   :1235001   Mean   :155854  \n                    3rd Qu.:1810340   3rd Qu.:1575898   3rd Qu.:225032  \n                    Max.   :8753788   Max.   :4685047   Max.   :943997  \n   TotalVotes         ActClinton       ActTrump       ActOthers     \n Min.   :  258788   Min.   :21.63   Min.   : 4.07   Min.   : 1.944  \n 1st Qu.:  758094   1st Qu.:35.99   1st Qu.:41.14   1st Qu.: 4.739  \n Median : 2001336   Median :46.17   Median :48.67   Median : 5.821  \n Mean   : 2682102   Mean   :44.61   Mean   :48.32   Mean   : 7.071  \n 3rd Qu.: 3347920   3rd Qu.:51.31   3rd Qu.:57.44   3rd Qu.: 8.611  \n Max.   :14181595   Max.   :90.48   Max.   :68.63   Max.   :26.998  \n  TrumpWin      \n Mode :logical  \n FALSE:21       \n TRUE :30       \n                \n                \n                \n\n\n\n# Are we fortunate enough that the dataset of polls and the dataset of votes are in the same order by state?   One way to check is through visual inspection.\n\nZaneA03.state.sorted &lt;- ZaneA03[order(ZaneA03$state),]\nZaneA03.sortcheck &lt;- rbind(ZaneA03.state.sorted$state\n                           , ZaneA03.votedata$State\n                            )\nhead(ZaneA03.sortcheck\n     , n=51\n     )\n\n     [,1]      [,2]     [,3]      [,4]       [,5]         [,6]      \n[1,] \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" \"California\" \"Colorado\"\n[2,] \"ALABAMA\" \"ALASKA\" \"ARIZONA\" \"ARKANSAS\" \"CALIFORNIA\" \"COLORADO\"\n     [,7]          [,8]       [,9]                   [,10]     [,11]    \n[1,] \"Connecticut\" \"Delaware\" \"District of Columbia\" \"Florida\" \"Georgia\"\n[2,] \"CONNECTICUT\" \"DELAWARE\" \"DISTRICT OF COLUMBIA\" \"FLORIDA\" \"GEORGIA\"\n     [,12]    [,13]   [,14]      [,15]     [,16]  [,17]    [,18]     \n[1,] \"Hawaii\" \"Idaho\" \"Illinois\" \"Indiana\" \"Iowa\" \"Kansas\" \"Kentucky\"\n[2,] \"HAWAII\" \"IDAHO\" \"ILLINOIS\" \"INDIANA\" \"IOWA\" \"KANSAS\" \"KENTUCKY\"\n     [,19]       [,20]   [,21]      [,22]           [,23]      [,24]      \n[1,] \"Louisiana\" \"Maine\" \"Maryland\" \"Massachusetts\" \"Michigan\" \"Minnesota\"\n[2,] \"LOUISIANA\" \"MAINE\" \"MARYLAND\" \"MASSACHUSETTS\" \"MICHIGAN\" \"MINNESOTA\"\n     [,25]         [,26]      [,27]     [,28]      [,29]    [,30]          \n[1,] \"Mississippi\" \"Missouri\" \"Montana\" \"Nebraska\" \"Nevada\" \"New Hampshire\"\n[2,] \"MISSISSIPPI\" \"MISSOURI\" \"MONTANA\" \"NEBRASKA\" \"NEVADA\" \"NEW HAMPSHIRE\"\n     [,31]        [,32]        [,33]      [,34]            [,35]         \n[1,] \"New Jersey\" \"New Mexico\" \"New York\" \"North Carolina\" \"North Dakota\"\n[2,] \"NEW JERSEY\" \"NEW MEXICO\" \"NEW YORK\" \"NORTH CAROLINA\" \"NORTH DAKOTA\"\n     [,36]  [,37]      [,38]    [,39]          [,40]          [,41]           \n[1,] \"Ohio\" \"Oklahoma\" \"Oregon\" \"Pennsylvania\" \"Rhode Island\" \"South Carolina\"\n[2,] \"OHIO\" \"OKLAHOMA\" \"OREGON\" \"PENNSYLVANIA\" \"RHODE ISLAND\" \"SOUTH CAROLINA\"\n     [,42]          [,43]       [,44]   [,45]  [,46]     [,47]     \n[1,] \"South Dakota\" \"Tennessee\" \"Texas\" \"Utah\" \"Vermont\" \"Virginia\"\n[2,] \"SOUTH DAKOTA\" \"TENNESSEE\" \"TEXAS\" \"UTAH\" \"VERMONT\" \"VIRGINIA\"\n     [,48]        [,49]           [,50]       [,51]    \n[1,] \"Washington\" \"West Virginia\" \"Wisconsin\" \"Wyoming\"\n[2,] \"WASHINGTON\" \"WEST VIRGINIA\" \"WISCONSIN\" \"WYOMING\"\n\n\nThe visual inspection reveals the data is aligned by state.\n\n# The visual inspection reveals the data is aligned by state. \n# So, polls and votes, by the power invested in me by R,\n# I pronounce your merged!\n\n# Build the list of datasets to merge\n\n# ZaneA03.datasetlist &lt;- c(\"ZaneA03.state.sorted\"\n#                         , \"ZaneA03.votedata\"\n#                         )\n# class(ZaneA03.datasetlist)\n\n\n# Go forth and merge!\n\n# Rest of this section commented out so you can see everything above in the render\n# ZaneA03.PollsAndVotes &lt;- rbind(ZaneA03.state.sorted\n#                               , ZaneA03.votedata)\n\n\n# \n#ZaneA03.PollsAndVotes &lt;- ZaneA03.state.sorted + ZaneA03.votedata"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#exploring-the-original-data-set",
    "href": "cdcdata-exercise/cdcdata-exercise.html#exploring-the-original-data-set",
    "title": "CDC Data Exercise",
    "section": "Exploring the original data set",
    "text": "Exploring the original data set\nThe CDC data set I have chosen for this exercise is the Tobacco Usage Behavioral Risk Factor data set. The data comes from a survey administered by the CDC as part of the State Tobacco Activities Tracking and Evaluation (STATE) system. The data set contains information about the year the survey was administered, respondent location (State or territory), demographics (namely Race, Age, Education, and Gender), and it asks questions about a person’s tobacco usage status (current, former, never used), their cessation status (whether they have quit or attempted to quit in the last year), and the frequency that they use tobacco for those that do (daily, some days). The usage questions are asked for three different types of Tobacco products, Cigarettes, Smokeless Tobacco, and E-cigarettes.\n\npacman::p_load(here, tidyverse, skimr, plotly, synthpop, patchwork)\n\n\nrawdata &lt;- read_csv('cdc-data-raw.csv')\n\nRows: 43341 Columns: 31\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (25): YEAR, LocationAbbr, LocationDesc, TopicType, TopicDesc, MeasureDes...\ndbl  (6): Data_Value, Data_Value_Std_Err, Low_Confidence_Limit, High_Confide...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nstr(rawdata) # getting an idea for data structure\n\nspc_tbl_ [43,341 × 31] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ YEAR                      : chr [1:43341] \"2017\" \"2018\" \"2017\" \"2016\" ...\n $ LocationAbbr              : chr [1:43341] \"GU\" \"US\" \"US\" \"GU\" ...\n $ LocationDesc              : chr [1:43341] \"Guam\" \"National Median (States and DC)\" \"National Median (States and DC)\" \"Guam\" ...\n $ TopicType                 : chr [1:43341] \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" ...\n $ TopicDesc                 : chr [1:43341] \"Cigarette Use (Adults)\" \"Cigarette Use (Adults)\" \"Cigarette Use (Adults)\" \"Smokeless Tobacco Use (Adults)\" ...\n $ MeasureDesc               : chr [1:43341] \"Current Smoking\" \"Smoking Status\" \"Smoking Status\" \"Current Use\" ...\n $ DataSource                : chr [1:43341] \"BRFSS\" \"BRFSS\" \"BRFSS\" \"BRFSS\" ...\n $ Response                  : chr [1:43341] NA \"Current\" \"Never\" NA ...\n $ Data_Value_Unit           : chr [1:43341] \"%\" \"%\" \"%\" \"%\" ...\n $ Data_Value_Type           : chr [1:43341] \"Percentage\" \"Percentage\" \"Percentage\" \"Percentage\" ...\n $ Data_Value                : num [1:43341] 30 16.1 58.2 0.5 21.9 21.6 2.8 3.6 29.2 14.8 ...\n $ Data_Value_Footnote_Symbol: chr [1:43341] NA NA NA NA ...\n $ Data_Value_Footnote       : chr [1:43341] NA NA NA NA ...\n $ Data_Value_Std_Err        : num [1:43341] 2.3 NA NA 0.4 4.9 0.8 0.3 0.4 2.4 0.6 ...\n $ Low_Confidence_Limit      : num [1:43341] 25.4 NA NA 0 12.4 20 2.2 2.7 24.5 13.6 ...\n $ High_Confidence_Limit     : num [1:43341] 34.6 NA NA 1.2 31.4 23.2 3.4 4.5 33.9 16 ...\n $ Sample_Size               : num [1:43341] 692 NA NA 123 209 ...\n $ Gender                    : chr [1:43341] \"Male\" \"Overall\" \"Overall\" \"Overall\" ...\n $ Race                      : chr [1:43341] \"All Races\" \"All Races\" \"All Races\" \"Hispanic\" ...\n $ Age                       : chr [1:43341] \"All Ages\" \"All Ages\" \"All Ages\" \"All Ages\" ...\n $ Education                 : chr [1:43341] \"All Grades\" \"All Grades\" \"All Grades\" \"All Grades\" ...\n $ GeoLocation               : chr [1:43341] \"(13.444304, 144.793731)\" NA NA \"(13.444304, 144.793731)\" ...\n $ TopicTypeId               : chr [1:43341] \"BEH\" \"BEH\" \"BEH\" \"BEH\" ...\n $ TopicId                   : chr [1:43341] \"100BEH\" \"100BEH\" \"100BEH\" \"150BEH\" ...\n $ MeasureId                 : chr [1:43341] \"110CSA\" \"165SSA\" \"165SSA\" \"177SCU\" ...\n $ StratificationID1         : chr [1:43341] \"2GEN\" \"1GEN\" \"1GEN\" \"1GEN\" ...\n $ StratificationID2         : chr [1:43341] \"8AGE\" \"8AGE\" \"8AGE\" \"8AGE\" ...\n $ StratificationID3         : chr [1:43341] \"6RAC\" \"6RAC\" \"6RAC\" \"4RAC\" ...\n $ StratificationID4         : chr [1:43341] \"6EDU\" \"6EDU\" \"6EDU\" \"6EDU\" ...\n $ SubMeasureID              : chr [1:43341] \"BRF21\" \"BRF27\" \"BRF28\" \"BRF69\" ...\n $ DisplayOrder              : num [1:43341] 21 27 28 69 22 21 77 71 26 27 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   YEAR = col_character(),\n  ..   LocationAbbr = col_character(),\n  ..   LocationDesc = col_character(),\n  ..   TopicType = col_character(),\n  ..   TopicDesc = col_character(),\n  ..   MeasureDesc = col_character(),\n  ..   DataSource = col_character(),\n  ..   Response = col_character(),\n  ..   Data_Value_Unit = col_character(),\n  ..   Data_Value_Type = col_character(),\n  ..   Data_Value = col_double(),\n  ..   Data_Value_Footnote_Symbol = col_character(),\n  ..   Data_Value_Footnote = col_character(),\n  ..   Data_Value_Std_Err = col_double(),\n  ..   Low_Confidence_Limit = col_double(),\n  ..   High_Confidence_Limit = col_double(),\n  ..   Sample_Size = col_double(),\n  ..   Gender = col_character(),\n  ..   Race = col_character(),\n  ..   Age = col_character(),\n  ..   Education = col_character(),\n  ..   GeoLocation = col_character(),\n  ..   TopicTypeId = col_character(),\n  ..   TopicId = col_character(),\n  ..   MeasureId = col_character(),\n  ..   StratificationID1 = col_character(),\n  ..   StratificationID2 = col_character(),\n  ..   StratificationID3 = col_character(),\n  ..   StratificationID4 = col_character(),\n  ..   SubMeasureID = col_character(),\n  ..   DisplayOrder = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(rawdata) # summary statistics\n\n     YEAR           LocationAbbr       LocationDesc        TopicType        \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  TopicDesc         MeasureDesc         DataSource          Response        \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n Data_Value_Unit    Data_Value_Type      Data_Value   \n Length:43341       Length:43341       Min.   : 0.00  \n Class :character   Class :character   1st Qu.: 5.10  \n Mode  :character   Mode  :character   Median :17.10  \n                                       Mean   :25.36  \n                                       3rd Qu.:38.90  \n                                       Max.   :99.90  \n                                       NA's   :2117   \n Data_Value_Footnote_Symbol Data_Value_Footnote Data_Value_Std_Err\n Length:43341               Length:43341        Min.   : 0.000    \n Class :character           Class :character    1st Qu.: 0.600    \n Mode  :character           Mode  :character    Median : 1.100    \n                                                Mean   : 1.711    \n                                                3rd Qu.: 2.200    \n                                                Max.   :16.600    \n                                                NA's   :2195      \n Low_Confidence_Limit High_Confidence_Limit  Sample_Size       Gender         \n Min.   : 0.00        Min.   :  0.00        Min.   :   50   Length:43341      \n 1st Qu.: 3.20        1st Qu.:  6.90        1st Qu.:  480   Class :character  \n Median :14.30        Median : 19.80        Median : 1798   Mode  :character  \n Mean   :22.01        Mean   : 28.68        Mean   : 3050                     \n 3rd Qu.:30.50        3rd Qu.: 47.40        3rd Qu.: 4203                     \n Max.   :99.80        Max.   :100.00        Max.   :40726                     \n NA's   :2195         NA's   :2195          NA's   :2195                      \n     Race               Age             Education         GeoLocation       \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n TopicTypeId          TopicId           MeasureId         StratificationID1 \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n StratificationID2  StratificationID3  StratificationID4  SubMeasureID      \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  DisplayOrder  \n Min.   : 5.00  \n 1st Qu.:24.00  \n Median :51.00  \n Mean   :47.08  \n 3rd Qu.:71.00  \n Max.   :81.00  \n                \n\nskim(rawdata) # primarily to get completion rates\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n43341\n\n\nNumber of columns\n31\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n25\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nYEAR\n0\n1.00\n4\n9\n0\n17\n0\n\n\nLocationAbbr\n0\n1.00\n2\n2\n0\n54\n0\n\n\nLocationDesc\n0\n1.00\n4\n31\n0\n54\n0\n\n\nTopicType\n0\n1.00\n25\n25\n0\n1\n0\n\n\nTopicDesc\n0\n1.00\n18\n30\n0\n4\n0\n\n\nMeasureDesc\n0\n1.00\n11\n59\n0\n10\n0\n\n\nDataSource\n0\n1.00\n5\n5\n0\n1\n0\n\n\nResponse\n28323\n0.35\n5\n11\n0\n6\n0\n\n\nData_Value_Unit\n0\n1.00\n1\n1\n0\n1\n0\n\n\nData_Value_Type\n0\n1.00\n10\n10\n0\n1\n0\n\n\nData_Value_Footnote_Symbol\n41224\n0.05\n1\n1\n0\n1\n0\n\n\nData_Value_Footnote\n41224\n0.05\n71\n71\n0\n1\n0\n\n\nGender\n0\n1.00\n4\n7\n0\n3\n0\n\n\nRace\n0\n1.00\n5\n29\n0\n6\n0\n\n\nAge\n0\n1.00\n8\n18\n0\n8\n0\n\n\nEducation\n0\n1.00\n10\n12\n0\n4\n0\n\n\nGeoLocation\n78\n1.00\n23\n41\n0\n53\n0\n\n\nTopicTypeId\n0\n1.00\n3\n3\n0\n1\n0\n\n\nTopicId\n0\n1.00\n6\n6\n0\n4\n0\n\n\nMeasureId\n0\n1.00\n6\n6\n0\n14\n0\n\n\nStratificationID1\n0\n1.00\n4\n4\n0\n3\n0\n\n\nStratificationID2\n0\n1.00\n4\n4\n0\n8\n0\n\n\nStratificationID3\n0\n1.00\n4\n4\n0\n6\n0\n\n\nStratificationID4\n0\n1.00\n4\n4\n0\n4\n0\n\n\nSubMeasureID\n0\n1.00\n5\n5\n0\n53\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nData_Value\n2117\n0.95\n25.36\n25.00\n0\n5.1\n17.1\n38.9\n99.9\n▇▃▂▁▁\n\n\nData_Value_Std_Err\n2195\n0.95\n1.71\n1.72\n0\n0.6\n1.1\n2.2\n16.6\n▇▁▁▁▁\n\n\nLow_Confidence_Limit\n2195\n0.95\n22.01\n24.14\n0\n3.2\n14.3\n30.5\n99.8\n▇▂▂▁▁\n\n\nHigh_Confidence_Limit\n2195\n0.95\n28.68\n26.21\n0\n6.9\n19.8\n47.4\n100.0\n▇▃▂▂▁\n\n\nSample_Size\n2195\n0.95\n3050.17\n3798.87\n50\n480.0\n1798.0\n4203.0\n40726.0\n▇▁▁▁▁\n\n\nDisplayOrder\n0\n1.00\n47.08\n24.49\n5\n24.0\n51.0\n71.0\n81.0\n▂▇▁▂▇\n\n\n\n\n\n\nhead(rawdata, 20) # getting first 20 rows\n\n# A tibble: 20 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2017     GU           Guam         Tobacco … Cigarett… Current Sm… BRFSS     \n 2 2018     US           National Me… Tobacco … Cigarett… Smoking St… BRFSS     \n 3 2017     US           National Me… Tobacco … Cigarett… Smoking St… BRFSS     \n 4 2016     GU           Guam         Tobacco … Smokeles… Current Use BRFSS     \n 5 2014     GU           Guam         Tobacco … Cigarett… Current Sm… BRFSS     \n 6 2012     IN           Indiana      Tobacco … Cigarett… Current Sm… BRFSS     \n 7 2018-20… DE           Delaware     Tobacco … Smokeles… Current Us… BRFSS     \n 8 2011     MO           Missouri     Tobacco … Smokeles… Current Use BRFSS     \n 9 2013     ME           Maine        Tobacco … Cigarett… Smoking Fr… BRFSS     \n10 2017     WA           Washington   Tobacco … Cigarett… Smoking St… BRFSS     \n11 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n12 2011     MN           Minnesota    Tobacco … Smokeles… User Status BRFSS     \n13 2018     VA           Virginia     Tobacco … Cigarett… Smoking Fr… BRFSS     \n14 2014     MN           Minnesota    Tobacco … Cigarett… Smoking St… BRFSS     \n15 2017-20… WA           Washington   Tobacco … Cigarett… Current Sm… BRFSS     \n16 2017     UT           Utah         Tobacco … Smokeles… Current Use BRFSS     \n17 2012     CA           California   Tobacco … Smokeles… Frequency … BRFSS     \n18 2015     FL           Florida      Tobacco … Smokeles… Current Use BRFSS     \n19 2015     RI           Rhode Island Tobacco … Smokeles… Current Use BRFSS     \n20 2019     AZ           Arizona      Tobacco … Smokeles… Current Use BRFSS     \n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, StratificationID1 &lt;chr&gt;,\n#   StratificationID2 &lt;chr&gt;, StratificationID3 &lt;chr&gt;, …\n\n\nThe data set is not the most analyst friendly format. Generally the ideal format would be variables in the columns, observations in the rows. This data set has variables across several rows, and it is mostly aggregated. The questions are broken up by the different response options, which are spread across rows even though the values are proportions and therefore the samples sizes listed for the different response options for a single question come from the same audience. Aggregations grouped by each of the demographics, locations, and timeframes are also in the rows, similar to a pivot table, so getting things like summary statistics will require filtering these aggregated values. Some survey question variables have missing values, but with the data in this format it’s difficult to make much of them, but we will come back to that. There are a few variables that only serve to give information about the data set itself, like Data_Value_Unit, which only contains one value, “Percentage”, to inform that the Value column is a percentage. Variable like this (Zero or Near Zero Variance) can be removed, since they give no information about the individual observations.\nVariables to be removed: Zero Variance variables: Data_Value_Unit, Data_Value_Type, DataSource, TopicType, TopicTypeId NZV: Data_Value_Footnote_Symbol, Data_Value_Footnote\n\nrawdata %&gt;% filter(is.na(Data_Value)) #looking at nulls to determine why they are there\n\n# A tibble: 2,117 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n 2 2016     IA           Iowa         Tobacco … Smokeles… Frequency … BRFSS     \n 3 2014     ID           Idaho        Tobacco … Smokeles… Frequency … BRFSS     \n 4 2019     MI           Michigan     Tobacco … Smokeles… Frequency … BRFSS     \n 5 2016-20… MA           Massachuset… Tobacco … Smokeles… Current Us… BRFSS     \n 6 2013-20… DC           District of… Tobacco … Cigarett… Current Sm… BRFSS     \n 7 2013     ME           Maine        Tobacco … Cigarett… Current Sm… BRFSS     \n 8 2011     AZ           Arizona      Tobacco … Smokeles… Frequency … BRFSS     \n 9 2018     AL           Alabama      Tobacco … Smokeles… Frequency … BRFSS     \n10 2018     NC           North Carol… Tobacco … Smokeles… Frequency … BRFSS     \n# ℹ 2,107 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\nMissing values from the Data_Value column occur when the sample size is not large enough to report results. Some of these missing values may have the opportunity for imputation, and filtering these could cause some confusion in the actual structure of the data (complete and consistent combinations of categorical values) so we won’t filter anything out just yet. note this is evidenced by the Footnote column with the following message: “Data in these cells have been suppressed because of a small sample size.”\n\nunique(paste(rawdata$LocationDesc,rawdata$LocationAbbr)) # paste concatenates columns, unique to get distinct values.\n\n [1] \"Guam GU\"                            \"National Median (States and DC) US\"\n [3] \"Indiana IN\"                         \"Delaware DE\"                       \n [5] \"Missouri MO\"                        \"Maine ME\"                          \n [7] \"Washington WA\"                      \"Illinois IL\"                       \n [9] \"Minnesota MN\"                       \"Virginia VA\"                       \n[11] \"Utah UT\"                            \"California CA\"                     \n[13] \"Florida FL\"                         \"Rhode Island RI\"                   \n[15] \"Arizona AZ\"                         \"District of Columbia DC\"           \n[17] \"Kansas KS\"                          \"Nevada NV\"                         \n[19] \"Alabama AL\"                         \"West Virginia WV\"                  \n[21] \"Wisconsin WI\"                       \"Oklahoma OK\"                       \n[23] \"New York NY\"                        \"Iowa IA\"                           \n[25] \"Colorado CO\"                        \"Idaho ID\"                          \n[27] \"Alaska AK\"                          \"North Dakota ND\"                   \n[29] \"North Carolina NC\"                  \"New Hampshire NH\"                  \n[31] \"Vermont VT\"                         \"Texas TX\"                          \n[33] \"Nebraska NE\"                        \"Hawaii HI\"                         \n[35] \"Wyoming WY\"                         \"South Dakota SD\"                   \n[37] \"Louisiana LA\"                       \"Kentucky KY\"                       \n[39] \"Puerto Rico PR\"                     \"Arkansas AR\"                       \n[41] \"Maryland MD\"                        \"Ohio OH\"                           \n[43] \"South Carolina SC\"                  \"Massachusetts MA\"                  \n[45] \"Michigan MI\"                        \"Oregon OR\"                         \n[47] \"Montana MT\"                         \"Tennessee TN\"                      \n[49] \"Connecticut CT\"                     \"Mississippi MS\"                    \n[51] \"New Mexico NM\"                      \"Pennsylvania PA\"                   \n[53] \"New Jersey NJ\"                      \"Georgia GA\"                        \n\n\nTo understand the scope of the data set and check for things like inconsistencies, mismatched abbreviations, etc we look at all the unique values for LocationDesc and LocationAbbr. For simplicity sake I will remove the Abbreviated column in the future.\n\nunique(rawdata$YEAR) # seeing unique values of year\n\n [1] \"2017\"      \"2018\"      \"2016\"      \"2014\"      \"2012\"      \"2018-2019\"\n [7] \"2011\"      \"2013\"      \"2014-2015\" \"2017-2018\" \"2015\"      \"2019\"     \n[13] \"2016-2017\" \"2013-2014\" \"2015-2016\" \"2011-2012\" \"2012-2013\"\n\n\n\nrawdata %&gt;% filter(nchar(YEAR) &gt; 4) # confirming what the two-year values are\n\n# A tibble: 4,670 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2018-20… DE           Delaware     Tobacco … Smokeles… Current Us… BRFSS     \n 2 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n 3 2017-20… WA           Washington   Tobacco … Cigarett… Current Sm… BRFSS     \n 4 2018-20… DC           District of… Tobacco … Smokeles… Current Us… BRFSS     \n 5 2016-20… ID           Idaho        Tobacco … Cigarett… Current Sm… BRFSS     \n 6 2017-20… CO           Colorado     Tobacco … Smokeles… Current Us… BRFSS     \n 7 2018-20… TX           Texas        Tobacco … Cigarett… Current Sm… BRFSS     \n 8 2018-20… KS           Kansas       Tobacco … Cigarett… Current Sm… BRFSS     \n 9 2014-20… KS           Kansas       Tobacco … Smokeles… Current Us… BRFSS     \n10 2013-20… VT           Vermont      Tobacco … Cigarett… Current Sm… BRFSS     \n# ℹ 4,660 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\nAgain, understanding the scope of the data. Two things I’m noticing, first being that the year column is a character variable, and not a number. Second being that there are some two-year aggregations that are not necessary since we have every year between 2011 and 2019, these can likely be filtered out.\n\n# filtering down to one Locationa and a smaller year set, again to diagnose the two-year observations\nrawdata %&gt;% filter(LocationAbbr == 'IL', YEAR %in% c('2014-2015', '2014', '2015')) %&gt;% arrange(DisplayOrder)\n\n# A tibble: 152 × 31\n   YEAR  LocationAbbr LocationDesc TopicType    TopicDesc MeasureDesc DataSource\n   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 2 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 3 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 4 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 5 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 6 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 7 2014  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n 8 2015  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n 9 2015  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n10 2014  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n# ℹ 142 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\n\nrawdata %&gt;% filter(LocationAbbr == 'IL', YEAR %in% c('2014-2015', '2014', '2015')) %&gt;% \n  group_by(YEAR, Race, Gender, MeasureDesc) %&gt;% # groups picked to confirm aggregation hierarchy\n  summarize(\n    Samp = sum(Sample_Size, na.rm = TRUE) # sums sample size across the above groups\n  )\n\n`summarise()` has grouped output by 'YEAR', 'Race', 'Gender'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 78 × 5\n# Groups:   YEAR, Race, Gender [21]\n   YEAR  Race             Gender  MeasureDesc                               Samp\n   &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;                                    &lt;dbl&gt;\n 1 2014  African American Overall Current Smoking                            488\n 2 2014  African American Overall Current Use                                489\n 3 2014  All Races        Female  Current Smoking                           3459\n 4 2014  All Races        Female  Current Use                               3454\n 5 2014  All Races        Female  Frequency of Use                             0\n 6 2014  All Races        Female  Percent of Former Smokers Among Ever Sm…  1056\n 7 2014  All Races        Female  Quit Attempt in Past Year Among Every D…   225\n 8 2014  All Races        Female  Smoking Frequency                          686\n 9 2014  All Races        Female  Smoking Status                            8241\n10 2014  All Races        Female  User Status                               5486\n# ℹ 68 more rows\n\n\nThis was primarily to confirm that the two-year rows are in fact aggregations of the data we have in the single-year rows, which we can see that they are. Filtering to a specific location and timeframe also gives us an idea of data structure.\nNote: Values like Current Use and Current Smoking are intentionally different. ‘Use’ Corresponds to smokeless tobacco use, ‘Smoking’ refers to cigarrette/ non-electronic usage.\nThis is enough exploration to give me an idea of what I would want my final data set to look like after cleaning and processing.\nGoal Data Set Mapping:\n|Year|Location|Age|Race|Gender|Education|Value for Measure Desc combined with Values for Response, Surveyed then Freq\nThis would ultimately be a wider data set than we have now, and we will get there in steps.\n\nd1 &lt;- rawdata %&gt;% filter(nchar(YEAR) == 4) %&gt;% \n  mutate(\n  QA = paste(TopicDesc, MeasureDesc,Response), # concatenating all of the question-response related columns\n  Data_Value = as.numeric(Data_Value)/100, # transforming percentages into proper proportions\n  Year = as.numeric(YEAR) # Year variable was previously a string\n) %&gt;% select(Year, LocationDesc, Age, Race, Gender, Education, QA, Data_Value, Sample_Size) # reordering some but mostly getting rid of the original columns that were concatenated\n\nhead(d1)\n\n# A tibble: 6 × 9\n   Year LocationDesc   Age   Race  Gender Education QA    Data_Value Sample_Size\n  &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1  2017 Guam           All … All … Male   All Grad… Ciga…      0.3           692\n2  2018 National Medi… All … All … Overa… All Grad… Ciga…      0.161          NA\n3  2017 National Medi… All … All … Overa… All Grad… Ciga…      0.582          NA\n4  2016 Guam           All … Hisp… Overa… All Grad… Smok…      0.005         123\n5  2014 Guam           All … White Overa… All Grad… Ciga…      0.219         209\n6  2012 Indiana        All … All … Female All Grad… Ciga…      0.216        5165\n\n\nCombining Topic, Measures, and response effectively gives us a unique identifier for each answer choice for each question. Since the data set is already aggregated, we can use these combined question identifiers as variables and line them up with our categorical variables. This gets us one step closer to the rows as observations, or aggregations of observations in this case. Note that we essentially have two numeric values, the sample size and the observed proportion of interest. The sample size is not the same for each question or demographic, so we cant really solve for one without solving for both. To make this easier, I will split these up into two data sets then bring them back together at the end.\n\nd2 &lt;- d1 %&gt;% select(-Sample_Size) %&gt;% # removing sample size so the pivot over Data_Value works correctly\n  pivot_wider(names_from = QA, values_from = Data_Value) \n# transposes unique values of question-response identifier (QA) into columns, \n#essentially grouping by year, Location, and Demographics\n\nhead(d2)\n\n# A tibble: 6 × 25\n   Year LocationDesc         Age   Race  Gender Education Cigarette Use (Adult…¹\n  &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;dbl&gt;\n1  2017 Guam                 All … All … Male   All Grad…                  0.3  \n2  2018 National Median (St… All … All … Overa… All Grad…                 NA    \n3  2017 National Median (St… All … All … Overa… All Grad…                 NA    \n4  2016 Guam                 All … Hisp… Overa… All Grad…                  0.283\n5  2014 Guam                 All … White Overa… All Grad…                  0.219\n6  2012 Indiana              All … All … Female All Grad…                  0.216\n# ℹ abbreviated name: ¹​`Cigarette Use (Adults) Current Smoking NA`\n# ℹ 18 more variables: `Cigarette Use (Adults) Smoking Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Never` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) Current Use NA` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Frequency Some Days` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) User Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Former` &lt;dbl&gt;, …\n\n\n\nd3 &lt;- d1 %&gt;% select(-Data_Value) %&gt;% # removing Date_Value so the pivot over Sample_Size works correctly\n  pivot_wider(names_from = QA, values_from = Sample_Size)\n# transposes unique values of question-response identifier (QA) into columns, \n#essentially grouping by year, Location, and Demographics\n\nhead(d3)\n\n# A tibble: 6 × 25\n   Year LocationDesc         Age   Race  Gender Education Cigarette Use (Adult…¹\n  &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;dbl&gt;\n1  2017 Guam                 All … All … Male   All Grad…                    692\n2  2018 National Median (St… All … All … Overa… All Grad…                     NA\n3  2017 National Median (St… All … All … Overa… All Grad…                     NA\n4  2016 Guam                 All … Hisp… Overa… All Grad…                    123\n5  2014 Guam                 All … White Overa… All Grad…                    209\n6  2012 Indiana              All … All … Female All Grad…                   5165\n# ℹ abbreviated name: ¹​`Cigarette Use (Adults) Current Smoking NA`\n# ℹ 18 more variables: `Cigarette Use (Adults) Smoking Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Never` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) Current Use NA` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Frequency Some Days` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) User Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Former` &lt;dbl&gt;, …\n\n\n\nsum(d2[,1:6] != d3[,1:6]) # counts number of mismatches between categorical columns in the split dataset \n\n[1] 0\n\n\nThe pivot allows us to get each question-response identifier into a variable and line them up with categorical variables that are the same. Our d2 data frame is now our response frequency data set, and d3 is now our sample size data set. The last chunk is just checking that my categorical variables maintained the same structure after pivoting, which they should.\n\nd2 &lt;- d2 %&gt;%  select(c(names(d2[,1:6]),sort(names(d2[,7:25])))) %&gt;% \n  arrange(Year, LocationDesc, Age, Race)\n# Reorder the columns so like questions are next to each other, then reorders rows so like observations are next to each other\nd3 &lt;- d3 %&gt;%  select(c(names(d3[,1:6]),sort(names(d3[,7:25])))) %&gt;% \n  arrange(Year, LocationDesc, Age, Race)\n\nnames(d3)\n\n [1] \"Year\"                                                                             \n [2] \"LocationDesc\"                                                                     \n [3] \"Age\"                                                                              \n [4] \"Race\"                                                                             \n [5] \"Gender\"                                                                           \n [6] \"Education\"                                                                        \n [7] \"Cessation (Adults) Percent of Former Smokers Among Ever Smokers NA\"               \n [8] \"Cessation (Adults) Quit Attempt in Past Year Among Every Day Cigarette Smokers NA\"\n [9] \"Cigarette Use (Adults) Current Smoking NA\"                                        \n[10] \"Cigarette Use (Adults) Smoking Frequency Every Day\"                               \n[11] \"Cigarette Use (Adults) Smoking Frequency Some Days\"                               \n[12] \"Cigarette Use (Adults) Smoking Status Current\"                                    \n[13] \"Cigarette Use (Adults) Smoking Status Former\"                                     \n[14] \"Cigarette Use (Adults) Smoking Status Never\"                                      \n[15] \"E-Cigarette Use (Adults) Current Use NA\"                                          \n[16] \"E-Cigarette Use (Adults) Frequency of Use Every Day\"                              \n[17] \"E-Cigarette Use (Adults) Frequency of Use Some Days\"                              \n[18] \"E-Cigarette Use (Adults) User Status Current\"                                     \n[19] \"E-Cigarette Use (Adults) User Status Former\"                                      \n[20] \"E-Cigarette Use (Adults) User Status Never\"                                       \n[21] \"Smokeless Tobacco Use (Adults) Current Use NA\"                                    \n[22] \"Smokeless Tobacco Use (Adults) Frequency of Use Every Day\"                        \n[23] \"Smokeless Tobacco Use (Adults) Frequency of Use Some Days\"                        \n[24] \"Smokeless Tobacco Use (Adults) User Status Current\"                               \n[25] \"Smokeless Tobacco Use (Adults) User Status Not Current\"                           \n\n\nSince we lead off the question-response identifiers with the topic and Measure, we can get sort the variables alphabetically to get like questions next to each other. We do this for each data frame.\n\n# Manually renaming columns for conciseness\nnames(d3) &lt;- c(names(d3[,1:6]), \n               'QuitPctFrmr',\n               'QuitAttmpt',\n               \n               'CigCurrSmker',\n               'CigFreqDaily',\n               'CigFreqSome',\n               'CigStatCurr',\n               'CigStatFrmr',\n               'CigStatNvr',\n               \n               'EcigCurrUse',\n               'EcigFreqDaily',\n               'EcigFreqSome',\n               'EcigStatCurr',\n               'EcigStatFrmr',\n               'EcigStatNvr',\n               \n               'TobCurrUse',\n               'TobFreqDaily',\n               'TobFreqSome',\n               'TobStatCurr',\n               'TobStatNonCurr'\n               )\n\nredict &lt;- cbind(names(d3), names(d2)) # creates pseduo-dictionary\n\nnames(d2) &lt;- c(names(d3)) # copies new name convention from d3 to d2\n\n# concatenates the type of value identifier onto the columns, makes it easier to interpret\nnames(d2) &lt;- c(names(d2[,1:6]),paste0(names(d2[,7:25]), 'RespFreq')) \nnames(d3) &lt;- c(names(d3[,1:6]),paste0(names(d3[,7:25]), 'SrvCnt'))\n\nThere are shorter ways to make the column names usable, but I want column names that are also more concise. Here I have renamed each question-response identifier individually, to make them much easier to undrstand while I work with them. the redict part also creates a pseudo-data dictionary so I can remember what each renamed column corresponds to, if i forget. I rename the columns for d2 the lazy way, by copying the naming conventions from d3 since the structre is the same. Finally I add ‘RespFreq’ to the column names of my response frequency dataset, and ‘SrvCnt’ to the Survey Count sample size for my Sample Size dataset.\n\nsum(d2[,1:6] != d3[,1:6]) # one last check to make sure nothing got shuffled incorrectly\n\n[1] 0\n\nd4 &lt;- merge(d2,d3) # merging the two datasets into a master\n\nAgain, one last check to make sure everything is in the correct order, then merging the data sets into one master data set.\n\n# filter to narrow scope and see if transformations worked correctly and see what can be removed.\nd4 %&gt;% filter(Year == 2016, LocationDesc == 'Wyoming')\n\n   Year LocationDesc                Age                          Race  Gender\n1  2016      Wyoming     18 to 24 Years                     All Races Overall\n2  2016      Wyoming     18 to 44 Years                     All Races  Female\n3  2016      Wyoming     25 to 44 Years                     All Races Overall\n4  2016      Wyoming     45 to 64 Years                     All Races Overall\n5  2016      Wyoming 65 Years and Older                     All Races Overall\n6  2016      Wyoming   Age 20 and Older                     All Races Overall\n7  2016      Wyoming   Age 20 and Older                     All Races Overall\n8  2016      Wyoming   Age 20 and Older                     All Races Overall\n9  2016      Wyoming   Age 25 and Older                     All Races Overall\n10 2016      Wyoming   Age 25 and Older                     All Races Overall\n11 2016      Wyoming   Age 25 and Older                     All Races Overall\n12 2016      Wyoming           All Ages              African American Overall\n13 2016      Wyoming           All Ages                     All Races  Female\n14 2016      Wyoming           All Ages                     All Races    Male\n15 2016      Wyoming           All Ages                     All Races Overall\n16 2016      Wyoming           All Ages American Indian/Alaska Native Overall\n17 2016      Wyoming           All Ages        Asian/Pacific Islander Overall\n18 2016      Wyoming           All Ages                      Hispanic Overall\n19 2016      Wyoming           All Ages                         White Overall\n      Education QuitPctFrmrRespFreq QuitAttmptRespFreq CigCurrSmkerRespFreq\n1    All Grades                  NA                 NA                0.225\n2    All Grades                  NA                 NA                0.239\n3    All Grades                  NA                 NA                0.240\n4    All Grades                  NA                 NA                0.181\n5    All Grades                  NA                 NA                0.095\n6  &lt; 12th Grade                  NA                 NA                0.402\n7  &gt; 12th Grade                  NA                 NA                0.122\n8    12th Grade                  NA                 NA                0.267\n9  &lt; 12th Grade                  NA                 NA                0.386\n10 &gt; 12th Grade                  NA                 NA                0.122\n11   12th Grade                  NA                 NA                0.270\n12   All Grades                  NA                 NA                   NA\n13   All Grades               0.542              0.518                0.191\n14   All Grades               0.611              0.393                0.188\n15   All Grades               0.580              0.457                0.189\n16   All Grades                  NA                 NA                   NA\n17   All Grades                  NA                 NA                   NA\n18   All Grades                  NA                 NA                0.201\n19   All Grades                  NA                 NA                0.181\n   CigFreqDailyRespFreq CigFreqSomeRespFreq CigStatCurrRespFreq\n1                    NA                  NA                  NA\n2                    NA                  NA                  NA\n3                    NA                  NA                  NA\n4                    NA                  NA                  NA\n5                    NA                  NA                  NA\n6                    NA                  NA                  NA\n7                    NA                  NA                  NA\n8                    NA                  NA                  NA\n9                    NA                  NA                  NA\n10                   NA                  NA                  NA\n11                   NA                  NA                  NA\n12                   NA                  NA                  NA\n13                0.796               0.204               0.191\n14                0.745               0.255               0.188\n15                0.771               0.229               0.189\n16                   NA                  NA                  NA\n17                   NA                  NA                  NA\n18                   NA                  NA                  NA\n19                   NA                  NA                  NA\n   CigStatFrmrRespFreq CigStatNvrRespFreq EcigCurrUseRespFreq\n1                   NA                 NA               0.130\n2                   NA                 NA               0.075\n3                   NA                 NA               0.077\n4                   NA                 NA               0.033\n5                   NA                 NA               0.010\n6                   NA                 NA               0.100\n7                   NA                 NA               0.039\n8                   NA                 NA               0.066\n9                   NA                 NA               0.036\n10                  NA                 NA               0.042\n11                  NA                 NA               0.055\n12                  NA                 NA                  NA\n13               0.227              0.582               0.050\n14               0.295              0.518               0.061\n15               0.261              0.549               0.055\n16                  NA                 NA                  NA\n17                  NA                 NA                  NA\n18                  NA                 NA               0.033\n19                  NA                 NA               0.055\n   EcigFreqDailyRespFreq EcigFreqSomeRespFreq EcigStatCurrRespFreq\n1                     NA                   NA                   NA\n2                     NA                   NA                   NA\n3                     NA                   NA                   NA\n4                     NA                   NA                   NA\n5                     NA                   NA                   NA\n6                     NA                   NA                   NA\n7                     NA                   NA                   NA\n8                     NA                   NA                   NA\n9                     NA                   NA                   NA\n10                    NA                   NA                   NA\n11                    NA                   NA                   NA\n12                    NA                   NA                   NA\n13                 0.290                0.710                0.050\n14                 0.419                0.581                0.061\n15                 0.362                0.638                0.055\n16                    NA                   NA                   NA\n17                    NA                   NA                   NA\n18                    NA                   NA                   NA\n19                    NA                   NA                   NA\n   EcigStatFrmrRespFreq EcigStatNvrRespFreq TobCurrUseRespFreq\n1                    NA                  NA              0.175\n2                    NA                  NA              0.044\n3                    NA                  NA              0.139\n4                    NA                  NA              0.063\n5                    NA                  NA              0.038\n6                    NA                  NA              0.084\n7                    NA                  NA              0.079\n8                    NA                  NA              0.137\n9                    NA                  NA              0.086\n10                   NA                  NA              0.068\n11                   NA                  NA              0.127\n12                   NA                  NA                 NA\n13                0.157               0.793              0.023\n14                0.215               0.725              0.170\n15                0.186               0.758              0.098\n16                   NA                  NA                 NA\n17                   NA                  NA                 NA\n18                   NA                  NA              0.061\n19                   NA                  NA              0.097\n   TobFreqDailyRespFreq TobFreqSomeRespFreq TobStatCurrRespFreq\n1                    NA                  NA                  NA\n2                    NA                  NA                  NA\n3                    NA                  NA                  NA\n4                    NA                  NA                  NA\n5                    NA                  NA                  NA\n6                    NA                  NA                  NA\n7                    NA                  NA                  NA\n8                    NA                  NA                  NA\n9                    NA                  NA                  NA\n10                   NA                  NA                  NA\n11                   NA                  NA                  NA\n12                   NA                  NA                  NA\n13                   NA                  NA               0.023\n14                0.693               0.307               0.170\n15                0.672               0.328               0.098\n16                   NA                  NA                  NA\n17                   NA                  NA                  NA\n18                   NA                  NA                  NA\n19                   NA                  NA                  NA\n   TobStatNonCurrRespFreq QuitPctFrmrSrvCnt QuitAttmptSrvCnt CigCurrSmkerSrvCnt\n1                      NA                NA               NA                140\n2                      NA                NA               NA                469\n3                      NA                NA               NA                758\n4                      NA                NA               NA               1707\n5                      NA                NA               NA               1788\n6                      NA                NA               NA                211\n7                      NA                NA               NA               2896\n8                      NA                NA               NA               1231\n9                      NA                NA               NA                203\n10                     NA                NA               NA               2840\n11                     NA                NA               NA               1200\n12                     NA                NA               NA                 NA\n13                  0.977              1019              258               2492\n14                  0.830               951              217               1901\n15                  0.902              1970              475               4393\n16                     NA                NA               NA                 NA\n17                     NA                NA               NA                 NA\n18                     NA                NA               NA                213\n19                     NA                NA               NA               3961\n   CigFreqDailySrvCnt CigFreqSomeSrvCnt CigStatCurrSrvCnt CigStatFrmrSrvCnt\n1                  NA                NA                NA                NA\n2                  NA                NA                NA                NA\n3                  NA                NA                NA                NA\n4                  NA                NA                NA                NA\n5                  NA                NA                NA                NA\n6                  NA                NA                NA                NA\n7                  NA                NA                NA                NA\n8                  NA                NA                NA                NA\n9                  NA                NA                NA                NA\n10                 NA                NA                NA                NA\n11                 NA                NA                NA                NA\n12                 NA                NA                NA                NA\n13                339               339              2492              2492\n14                285               285              1901              1901\n15                624               624              4393              4393\n16                 NA                NA                NA                NA\n17                 NA                NA                NA                NA\n18                 NA                NA                NA                NA\n19                 NA                NA                NA                NA\n   CigStatNvrSrvCnt EcigCurrUseSrvCnt EcigFreqDailySrvCnt EcigFreqSomeSrvCnt\n1                NA               139                  NA                 NA\n2                NA               470                  NA                 NA\n3                NA               759                  NA                 NA\n4                NA              1713                  NA                 NA\n5                NA              1799                  NA                 NA\n6                NA               212                  NA                 NA\n7                NA              2902                  NA                 NA\n8                NA              1239                  NA                 NA\n9                NA               204                  NA                 NA\n10               NA              2847                  NA                 NA\n11               NA              1208                  NA                 NA\n12               NA                NA                  NA                 NA\n13             2492              2500                  76                 76\n14             1901              1910                  65                 65\n15             4393              4410                 141                141\n16               NA                NA                  NA                 NA\n17               NA                NA                  NA                 NA\n18               NA               213                  NA                 NA\n19               NA              3978                  NA                 NA\n   EcigStatCurrSrvCnt EcigStatFrmrSrvCnt EcigStatNvrSrvCnt TobCurrUseSrvCnt\n1                  NA                 NA                NA              140\n2                  NA                 NA                NA              470\n3                  NA                 NA                NA              756\n4                  NA                 NA                NA             1715\n5                  NA                 NA                NA             1799\n6                  NA                 NA                NA              213\n7                  NA                 NA                NA             2901\n8                  NA                 NA                NA             1239\n9                  NA                 NA                NA              205\n10                 NA                 NA                NA             2845\n11                 NA                 NA                NA             1208\n12                 NA                 NA                NA               NA\n13               2500               2500              2500             2503\n14               1910               1910              1910             1907\n15               4410               4410              4410             4410\n16                 NA                 NA                NA               NA\n17                 NA                 NA                NA               NA\n18                 NA                 NA                NA              214\n19                 NA                 NA                NA             3978\n   TobFreqDailySrvCnt TobFreqSomeSrvCnt TobStatCurrSrvCnt TobStatNonCurrSrvCnt\n1                  NA                NA                NA                   NA\n2                  NA                NA                NA                   NA\n3                  NA                NA                NA                   NA\n4                  NA                NA                NA                   NA\n5                  NA                NA                NA                   NA\n6                  NA                NA                NA                   NA\n7                  NA                NA                NA                   NA\n8                  NA                NA                NA                   NA\n9                  NA                NA                NA                   NA\n10                 NA                NA                NA                   NA\n11                 NA                NA                NA                   NA\n12                 NA                NA                NA                   NA\n13                 NA                NA              2503                 2503\n14                252               252              1907                 1907\n15                278               278              4410                 4410\n16                 NA                NA                NA                   NA\n17                 NA                NA                NA                   NA\n18                 NA                NA                NA                   NA\n19                 NA                NA                NA                   NA\n\n\nFiltering down to one Year and Location, so I can see how some of the aggregated values roll up and how they combine with other demographic variables. This gives me some insight into which questions were missing at what grains, and also allows me to check my merge to make sure it looks right. We finally have the data in a shape and layout that makes sense, and we see that its still kind of a mess. The intention was to maintain the separated demographic columns, and get each survey question into a column or columns. We accomplished that, however we can see that the demographics don’t stack in most cases. For example, I can see aggregated sample sizes for each Gender and the frequency for each question’s response, but I cannot see the age, education, or race makeup for each gender, or any combination of the demographic variables really. This was likely a deliberate choice for sample size constraints, nonetheless it complicates the next steps of this exercise. For a more in depth exercise, I would probably consider imputing the missing sample sizes or rates to get stratification by gender, age and race combined. However for the sake of this exercise, I will break each of the demographics into separate data sets since they have different questions considered. Once done, I will focus in on the Gender data set, since it looks like the gender demographic is the most complete across all questions.\nThe aggregated values by demographic aren’t necessary in the final version of any of these data sets, but we may need them to get the response volumes of some of the demographic values if there few responses. So, we can filter against all of the other demographic variables to only included their roll-up or overall value. First we will check for what those values are for each variable.\n\nunique(d4$Age) #'All Ages', also note some of the age bins are overlapping.\n\n[1] \"18 to 24 Years\"     \"18 to 44 Years\"     \"25 to 44 Years\"    \n[4] \"45 to 64 Years\"     \"65 Years and Older\" \"Age 20 and Older\"  \n[7] \"Age 25 and Older\"   \"All Ages\"          \n\nunique(d4$Gender) #'Overall'\n\n[1] \"Overall\" \"Female\"  \"Male\"   \n\nunique(d4$Race) #'All Races'\n\n[1] \"All Races\"                     \"African American\"             \n[3] \"American Indian/Alaska Native\" \"Asian/Pacific Islander\"       \n[5] \"Hispanic\"                      \"White\"                        \n\nunique(d4$Education) #'All Grades'\n\n[1] \"All Grades\"   \"&lt; 12th Grade\" \"&gt; 12th Grade\" \"12th Grade\"  \n\n\n\n# Age dataset, looking at rollup for all other variables except age, \n# then selecting relevant columns and filtering any last missing values\nAgeSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age != 'All Ages'\n              ,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\n# Gender Dataset. This time keeping the rollup value as well as the others,\n# but still filtering all other variables to their rollup\n# Also filtering out the national Median rows since its not the same grain as the others\n# finally removing last missing values. Will select relevant columns later.\nGndSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age == 'All Ages'\n              #,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  filter(LocationDesc != 'National Median (States and DC)',\n         !is.na(CigCurrSmkerRespFreq)\n         )\n\n# Race dataset, looking at rollup for all other variables except Race, \n# then selecting relevant columns and filtering any last missing values\nRaceSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age == 'All Ages'\n              ,Gender == 'Overall'\n              #,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\n# Education dataset, looking at rollup for all other variables except Education and Age,\n#since age actually is startified with education (interestingly, not vice-versa)\n# then selecting relevant columns and filtering any last missing values\nEdSrv &lt;- d4 %&gt;% filter(1==1\n              #,Age == 'All Ages'\n              ,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education != 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\nNow to focus in on the gender data set. I did not select specific columns earlier because the treatment was a bit different than the other data sets. This data set in particular has a lot more opportunity for imputing and other transformations, but for the simplicity of this exercise we will select only a few interesting and complete columns. Also one last note, the Gender variables has three values, Male, Female, and Overall. Since the Response frequency is the true variable of interest, I’m going to leave the “Overall” values in. Under other circumstances I would remove these and calculate overall values only when they apply, that way there aren’t “repeated” observations in the data set. However, the response frequency variables are pre-processed and rounded, so it would only unnecessarily complicate things to try and calculate response counts to get a proper aggregation.\n\n#Selecting relevant columns for the remainder of the exercise\nGndSrv &lt;- GndSrv %&gt;% \n  select(\n    Year\n    ,LocationDesc\n    ,Gender\n    ,QuitAttmptRespFreq\n    ,CigStatCurrRespFreq\n    ,CigStatFrmrRespFreq\n    ,CigStatNvrRespFreq\n    )\n\nI have decided to focus in on the questions for Cigarette usage, namely status and if they have attempted to quit in the last year. I also kept Year and Location since those variables were the least problematic, and of course Gender since this is the Gender-specific dataset.\n\nGndSrv %&gt;% group_by(Gender) %&gt;% #grouped by general to see potential differences in mean.\n  summarize( # getting mean values for each of my questions\n    AvgQuitAttempt = mean(QuitAttmptRespFreq)\n    ,AvgCurentSmokers = mean(CigStatCurrRespFreq)\n    ,AvgFormerSmokers = mean(CigStatFrmrRespFreq)\n    ,AvgNeverSmoked = mean(CigStatNvrRespFreq)\n  )\n\n# A tibble: 3 × 5\n  Gender  AvgQuitAttempt AvgCurentSmokers AvgFormerSmokers AvgNeverSmoked\n  &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1 Female           0.526            0.163            0.215          0.622\n2 Male             0.510            0.203            0.283          0.514\n3 Overall          0.518            0.183            0.248          0.569\n\n\nLet’s start by getting some quick descriptive statistics. Notice that this table will get the average Rate for each of the questions across all states and timeframes. The number of combinations we have between categorical variables, especially since the Location variable has so many categories, complicates our ability to get a closer look. Faceting and framing will help with this, as we will see in a moment.\n\nbox1o &lt;- GndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_boxplot() +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\nbox1o\n\n\n\n\n\n\n\nhist1o &lt;- GndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\nhist1o\n\n\n\n\n\n\n\n\nthe boxplots should show the distribution of the Percent of Current Smokers across the different locations. I have filter to 2011, the earliest year in the data set, and 2019, the latest year. Here I wanted to see if there would be a large difference from the beginning of the timeframe to the end, and for the most part we can see that there is. It appears as though we have a lower rate of “Current” Smokers from beginning to end. the distribution is not particularly clean, but it also looks to be shifting left while keeping some outliers on the higher end to make for a bit of a right skew.\n\nGndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=QuitAttmptRespFreq)) + # Quit Attempt Rate observed\n  geom_boxplot() +\n  labs(x = 'Percent of Quit Attempts\\nAmong Current Smokers') +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\n\n\n\n\n\n\n\nGndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=QuitAttmptRespFreq)) + # Quit Attempt Rate observed\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Quit Attempts\\nAmong Current Smokers', y = NULL) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\n\n\n\n\n\n\n\n\nGoing into the quit rate, it’s a bit of a different story here. There is maybe a little bit of a left shift in the Males and Overall, but it is far less pronounced than it was for the percent of current smokers. This makes sense, since it is the quit rate for current smokers, meaning we would expect it to either stay the same or potentially increase over time. This begs the question if the decrease in current smokers is coming from more people quitting or fewer people starting. The distribution is also all over the place, so there is a wider variance. This will be apparent in the next visual.\n\naniplot &lt;- GndSrv %&gt;% filter(Gender != 'Overall') %&gt;% # wanting Male and Female only\n  ggplot(aes(x=CigStatCurrRespFreq, y = QuitAttmptRespFreq)) + #plotting current smokers against quit attempt rate\n  geom_point(aes(frame = Year, color = Gender, ids = LocationDesc)) + #frame for animation, ids for hover text, color for comparison\n  labs(x = 'Percent of Current Smokers', y = 'Percent of Quit Attempts\\nAmong Current Smokers')\n\nWarning in geom_point(aes(frame = Year, color = Gender, ids = LocationDesc)):\nIgnoring unknown aesthetics: frame and ids\n\nggplotly(aniplot) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animaton, also slows down the slider when i click play\n\n\n\n\n\nThis was a pretty telling visual, so a few observations. First, there tend to be a higher rate of men who are current smokers over women. Second, the distribution is slowly but surely shifting left over time, it was not simply a fluke between 2011 and 2019 earlier. Finally, the wide variance in the percent of quit attempts is apparent, with the amount of vertical movement each point had in the visual. Still the distribution did not look to have any large shift up or down over time, which tells me the rate of people quitting is roughly the same, so more like the decrease in current smokers is happening because fewer people ever start smoking.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in later chunks\n  ggplot(aes(x = CigStatCurrRespFreq, y = fct_reorder(LocationDesc, CigStatCurrRespFreq), frame = Year)) + #fct_reorder orders location by Current smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of Current Smokers', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nMostly wanting to see if any outliers are more apparent here. Guam looks to have the highest rate of people who smoke, but they also have wide swings, likely due to a smaller sample size. Worth noting that the ranking of rates by state does not change significantly over time either.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in other chunks\n  ggplot(aes(x = CigStatFrmrRespFreq, y = fct_reorder(LocationDesc, CigStatFrmrRespFreq), frame = Year)) + #fct_reorder orders location by former smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of Former Smokers', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nThe rate of Former smokers does not look like it shares the trend. Again, if we saw an increase in the rates of people quitting we might expect the rate of Former Smokers to increase over time, but we don’t really see that much here.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in other chunks\n  ggplot(aes(x = CigStatNvrRespFreq, y = fct_reorder(LocationDesc, CigStatNvrRespFreq), frame = Year)) + #fct_reorder orders location by Never smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of People Who Have Never Smoked', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nHere we can see that increase in rate of people who have never smoked, though admittedly it still seems less pronounced than it was for the decrease in Current smokers. Still, it’s in line with the hypothesis that people are more likely are more likely to have never started smoking than they were in the past, and that is why we are seeing such a decrease in the rate of Current Smokers. Not to end on a darker note, but this is also likely a result of those who were Current Smokers in the earlier years dying off at a higher rate than non-smokers. This may suggest that younger people are much less likely to start smoking as well."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#generating-and-comparing-synthetic-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#generating-and-comparing-synthetic-data",
    "title": "CDC Data Exercise",
    "section": "Generating and comparing synthetic data",
    "text": "Generating and comparing synthetic data\n\nThis section contributed by Sean O’Sullivan\n\n# generating a reproducible set of synthetic data based on the characteristics\n# of the original GndSrv data frame\nset.seed(42)\nsynth &lt;- syn(GndSrv, method = c(\"\",\"\",\"\",\"norm\", \"norm\", \"norm\", \"norm\"))\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nsyn.GndSrv &lt;- as.data.frame(synth[3])\n\nIn order to generate some synthetic data that takes on the characteristics of the original data set we utilized the synthpop package’s syn() function and saved the returned data as a new data frame.\n\n# converting a few columns in both the original and synthetic data frames\n# to factor so that the summary function output can be better interpreted\nsyn.GndSrv_fac &lt;- as.data.frame(synth[3]) %&gt;%\n  mutate(syn.Year = as.factor(syn.Year),\n         syn.LocationDesc = as.factor(syn.LocationDesc),\n         syn.Gender = as.factor(syn.Gender))\nGndSrv_fac &lt;- GndSrv %&gt;%\n  mutate(Year = as.factor(Year),\n         LocationDesc = as.factor(LocationDesc),\n         Gender = as.factor(Gender))\n\n# outputting variable summaries for both data frames\nskim(GndSrv_fac)\n\n\nData summary\n\n\nName\nGndSrv_fac\n\n\nNumber of rows\n1428\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nYear\n0\n1\nFALSE\n9\n201: 159, 201: 159, 201: 159, 201: 159\n\n\nLocationDesc\n0\n1\nFALSE\n53\nAla: 27, Ala: 27, Ari: 27, Ark: 27\n\n\nGender\n0\n1\nFALSE\n3\nFem: 476, Mal: 476, Ove: 476\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nQuitAttmptRespFreq\n0\n1\n0.52\n0.05\n0.36\n0.48\n0.52\n0.54\n0.72\n▁▆▇▂▁\n\n\nCigStatCurrRespFreq\n0\n1\n0.18\n0.04\n0.06\n0.15\n0.18\n0.21\n0.36\n▁▇▇▂▁\n\n\nCigStatFrmrRespFreq\n0\n1\n0.25\n0.04\n0.10\n0.22\n0.25\n0.28\n0.35\n▁▂▆▇▂\n\n\nCigStatNvrRespFreq\n0\n1\n0.57\n0.07\n0.39\n0.52\n0.56\n0.61\n0.82\n▂▇▇▂▁\n\n\n\n\nskim(syn.GndSrv_fac)\n\n\nData summary\n\n\nName\nsyn.GndSrv_fac\n\n\nNumber of rows\n1428\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nsyn.Year\n0\n1\nFALSE\n9\n201: 159, 201: 159, 201: 159, 201: 159\n\n\nsyn.LocationDesc\n0\n1\nFALSE\n53\nAla: 27, Ala: 27, Ari: 27, Ark: 27\n\n\nsyn.Gender\n0\n1\nFALSE\n3\nFem: 476, Mal: 476, Ove: 476\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsyn.QuitAttmptRespFreq\n0\n1\n0.52\n0.05\n0.38\n0.48\n0.52\n0.55\n0.71\n▁▇▇▂▁\n\n\nsyn.CigStatCurrRespFreq\n0\n1\n0.18\n0.04\n0.04\n0.15\n0.18\n0.21\n0.33\n▁▅▇▃▁\n\n\nsyn.CigStatFrmrRespFreq\n0\n1\n0.25\n0.04\n0.11\n0.22\n0.25\n0.28\n0.36\n▁▂▇▆▁\n\n\nsyn.CigStatNvrRespFreq\n0\n1\n0.57\n0.07\n0.38\n0.52\n0.56\n0.62\n0.84\n▁▇▇▂▁\n\n\n\n\n# comparing overall distributions\n# subset just the numeric features\nGndSrv_num &lt;- GndSrv %&gt;%\n  select(where(is.numeric)) %&gt;% \n  select(-Year)\n\n# function for plotting multiple columns iteratively through the data frame\nhistfunco &lt;- function(colname) {\ncolname &lt;- sym(colname)\nplot &lt;- GndSrv_num %&gt;% \n  ggplot(aes(x = !!colname)) +\n  geom_histogram(aes(y = after_stat(density)), col =\"white\", fill = \"aquamarine2\", bins = 30) +\n  geom_density(col = \"aquamarine3\") +\n  ylab(NULL) +\n  theme(axis.text.y=element_blank(),\n  axis.ticks.y=element_blank())\n}\n\n# iterating through the numeric columns of the data frame with the above function and plotting the results\nhistso &lt;- lapply(colnames(GndSrv_num), FUN = histfunco)\n\n# subset just the numeric features\nsyn.GndSrv_num &lt;- syn.GndSrv %&gt;%\n  select(where(is.numeric)) %&gt;% \n  select(-syn.Year)\n\n# function for plotting multiple columns iteratively through the data frame\nhistfuncs &lt;- function(colname) {\ncolname &lt;- sym(colname)\nplot &lt;- syn.GndSrv_num %&gt;% \n  ggplot(aes(x = !!colname)) +\n  geom_histogram(aes(y = after_stat(density)), col =\"white\", fill = \"coral2\", bins = 30) +\n  geom_density(col = \"coral\") +\n  ylab(NULL) +\n  theme(axis.text.y=element_blank(),\n  axis.ticks.y=element_blank())\n}\n\n# iterating through the numeric columns of the data frame with the above function and plotting the results\nhistss &lt;- lapply(colnames(syn.GndSrv_num), FUN = histfuncs)\n\n#plot all plots\nwrap_plots(wrap_plots(histso, nrow = 1), wrap_plots(histss, nrow = 1), nrow = 2)\n\n\n\n\n\n\n\n\nComparing the overall distribution of the data in the original data frame and the new synthesized data frame we see that the overall shape of the data is very similar. Both data frames contain the same number of total observations, as well as the same composition of observations for each grouping within each factor (year, state, gender). Similarly we see nearly identical mean, standard deviation, and percentiles across each of the numeric variables. We can also see that the variable distributions when plotted are effectively visually indistinguishable overall.\n\n# comparing distribution plots for percent of current smokers by year and gender\nbox1s &lt;- syn.GndSrv %&gt;% filter(syn.Year %in% c(max(syn.Year), min(syn.Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=syn.CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_boxplot() +\n  labs(x = 'Percent of Current Smokers', title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(syn.Year), rows = vars(syn.Gender)) #want year and gender stratification\nwrap_plots(box1o, box1s, ncol = 1)\n\n\n\n\n\n\n\nhist1s &lt;- syn.GndSrv %&gt;% filter(syn.Year %in% c(max(syn.Year), min(syn.Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=syn.CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Synthetic Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(syn.Year), rows = vars(syn.Gender)) #want year and gender stratification\nwrap_plots(hist1o, hist1s, ncol = 1)\n\n\n\n\n\n\n\n\nHowever, when we examine the distributions of these features along combinations of Year and Gender we start to see some deviation from the underlying relationships at such a low level. They aren’t entirely dissimilar, but they differ significantly. This is perhaps because the synthpop package lacks enough data points for each combination of strata to create anything that isn’t simply a 1:1 copy of the original data.\nWhen attempting to use the syn.strata() function provided by the synthpop package we are able to better maintain relationships at a lower level by supplying a strata argument with the variable Year, but even the sample size for each strata of Year is too small for the function’s liking – as can be seen below.\n\n# generating a reproducible set of synthetic data based on the characteristics\n# of the original GndSrv data frame\nset.seed(42)\nsynth2 &lt;- syn.strata(GndSrv, strata = \"Year\", method = c(\"\",\"\",\"\",\"norm\", \"norm\", \"norm\", \"norm\"))\n\nNumber of observations in strata (original data):\n2011 2012 2013 2014 2015 2016 2017 2018 2019 \n 159  159  159  159  159  159  159  159  156 \nCAUTION: In the original data some strata (2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019) have limited numbers of observations.\nWe advise that there should be at least 170 observations (100 + 10 * no. of variables\nused in prediction).\n\nm = 1, strata = 2011\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 181 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2012\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 165 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2013\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 146 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2014\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 166 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2015\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 176 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2016\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 137 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2017\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 133 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2018\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2019\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (156) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 165 will be generated from original data of size 156.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nsyn.GndSrv2 &lt;- as.data.frame(synth2[3])\n\n# comparing distribution plots for percent of current smokers by year and gender\nbox2s &lt;- syn.GndSrv2 %&gt;% filter(syn.Year %in% c(max(syn.Year), min(syn.Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=syn.CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_boxplot() +\n  labs(x = 'Percent of Current Smokers', title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(syn.Year), rows = vars(syn.Gender)) #want year and gender stratification\nwrap_plots(box1o, box2s, ncol = 1)\n\n\n\n\n\n\n\nhist2s &lt;- syn.GndSrv2 %&gt;% filter(syn.Year %in% c(max(syn.Year), min(syn.Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=syn.CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Synthetic Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(syn.Year), rows = vars(syn.Gender)) #want year and gender stratification\nwrap_plots(hist1o, hist2s, ncol = 1)"
  }
]