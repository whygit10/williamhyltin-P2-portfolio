[
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "For this exercise we are tasked with performing an analysis on the latest Tidy Tuesday dataset. Given that the exercise is intended to give us an opportunity for an end to end analysis, I want to use this as an opportunity to show my thought process when it comes to an analysis. Likely this means there will be plenty of rambling and word vomit, but my intended focus is the work flow itself and less the end result. I will save the polished up manuscripts and final results for larger projects.\nThis week’s dataset is one on American Idol data, scraped from Wikipedia tables as I understand. I don’t have a lot of familiarity with the show since I’ve never been a fan myself, so one way or another this will absolutely be a learning experience.\nWe start by loading the dataset.\n\n#loading packages\npacman::p_load(tidyverse, here, tidytuesdayR, tidymodels, parsnip, skimr, viridis, caret, recipes, workflows, yardstick, kknn, earth, vip)\n\nInstalling package into 'C:/Users/Hylti/AppData/Local/R/win-library/4.3'\n(as 'lib' is unspecified)\n\n\nalso installing the dependency 'cli'\n\n\n\n  There is a binary version available but the source version is later:\n    binary source needs_compilation\ncli  3.6.3  3.6.4              TRUE\n\npackage 'tidymodels' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\Hylti\\AppData\\Local\\Temp\\RtmpYFNnFi\\downloaded_packages\n\n\ninstalling the source package 'cli'\n\n\n\ntidymodels installed\n\n#code below is taken from the tidy tuesday github\n#tuesdata &lt;- tidytuesdayR::tt_load('2024-07-23')\n\n#auditions &lt;- tuesdata$auditions\n#eliminations &lt;- tuesdata$eliminations\n#finalists &lt;- tuesdata$finalists\n#ratings &lt;- tuesdata$ratings\n#seasons &lt;- tuesdata$seasons\n#songs &lt;- tuesdata$songs\n\n# Clean data provided by &lt;https://github.com/kkakey/American_Idol&gt;. No cleaning was necessary.\nauditions &lt;- readr::read_csv(\"https://raw.githubusercontent.com/kkakey/American_Idol/main/metadata/auditions.csv\")\n\nRows: 142 Columns: 12\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): audition_city, audition_venue, episodes, episode_air_date, callbac...\ndbl  (2): season, tickets_to_hollywood\ndate (4): audition_date_start, audition_date_end, callback_date_start, callb...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\neliminations &lt;- readr::read_csv(\"https://raw.githubusercontent.com/kkakey/American_Idol/main/metadata/elimination_chart.csv\")\n\nRows: 456 Columns: 46\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (44): place, gender, contestant, top_36, top_36_2, top_36_3, top_36_4, t...\ndbl  (1): season\nlgl  (1): comeback\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfinalists &lt;- readr::read_csv(\"https://raw.githubusercontent.com/kkakey/American_Idol/main/metadata/finalists.csv\")\n\nRows: 190 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Contestant, Birthday, Birthplace, Hometown, Description\ndbl (1): Season\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nratings &lt;- readr::read_csv(\"https://raw.githubusercontent.com/kkakey/American_Idol/main/metadata/ratings.csv\")\n\nRows: 593 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): episode, airdate, 18_49_rating_share, timeslot_et, dvr_18_49, dvr_...\ndbl  (4): season, show_number, viewers_in_millions, nightlyrank\nlgl  (1): ref\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nseasons &lt;- readr::read_csv(\"https://raw.githubusercontent.com/kkakey/American_Idol/main/metadata/seasons.csv\")\n\nRows: 18 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): winner, runner_up, original_release, original_network, hosted_by, j...\ndbl (2): season, no_of_episodes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsongs &lt;- readr::read_csv(\"https://raw.githubusercontent.com/kkakey/American_Idol/main/Songs/songs_all.csv\")\n\nRows: 2429 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): season, week, contestant, song, artist, song_theme, result\ndbl (1): order\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nThen looking at the raw data.\n\n#lots of tables, so I'm using lapply to save myself some typing\nlapply(list(auditions, eliminations, finalists, ratings, seasons, songs), head)\n\n[[1]]\n# A tibble: 6 × 12\n  season audition_date_start audition_date_end audition_city      audition_venue\n   &lt;dbl&gt; &lt;date&gt;              &lt;date&gt;            &lt;chr&gt;              &lt;chr&gt;         \n1      1 2002-04-20          2002-04-22        Los Angeles, Cali… Westin Bonave…\n2      1 2002-04-23          2002-04-25        Seattle, Washingt… Hyatt Regency…\n3      1 2002-04-26          2002-04-28        Chicago, Illinois  Congress Plaz…\n4      1 2002-04-29          2002-05-01        New York City, Ne… Millenium Hil…\n5      1 2002-05-03          2002-05-05        Atlanta, Georgia   AmericasMart/…\n6      1 2002-05-05          2002-05-07        Dallas, Texas      Wyndham Anato…\n# ℹ 7 more variables: episodes &lt;chr&gt;, episode_air_date &lt;chr&gt;,\n#   callback_venue &lt;chr&gt;, callback_date_start &lt;date&gt;, callback_date_end &lt;date&gt;,\n#   tickets_to_hollywood &lt;dbl&gt;, guest_judge &lt;chr&gt;\n\n[[2]]\n# A tibble: 6 × 46\n  season place gender contestant        top_36 top_36_2 top_36_3 top_36_4 top_32\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; \n1      1 1     Female Kelly Clarkson    &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n2      1 2     Male   Justin Guarini    &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n3      1 3     Female Nikki McKibbin    &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n4      1 4     Female Tamyra Gray       &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n5      1 5     Male   R. J. Helton      &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n6      1 6     Female Christina Christ… &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n# ℹ 37 more variables: top_32_2 &lt;chr&gt;, top_32_3 &lt;chr&gt;, top_32_4 &lt;chr&gt;,\n#   top_30 &lt;chr&gt;, top_30_2 &lt;chr&gt;, top_30_3 &lt;chr&gt;, top_25 &lt;chr&gt;, top_25_2 &lt;chr&gt;,\n#   top_25_3 &lt;chr&gt;, top_24 &lt;chr&gt;, top_24_2 &lt;chr&gt;, top_24_3 &lt;chr&gt;, top_20 &lt;chr&gt;,\n#   top_20_2 &lt;chr&gt;, top_16 &lt;chr&gt;, top_14 &lt;chr&gt;, top_13 &lt;chr&gt;, top_12 &lt;chr&gt;,\n#   top_11 &lt;chr&gt;, top_11_2 &lt;chr&gt;, wildcard &lt;chr&gt;, comeback &lt;lgl&gt;, top_10 &lt;chr&gt;,\n#   top_9 &lt;chr&gt;, top_9_2 &lt;chr&gt;, top_8 &lt;chr&gt;, top_8_2 &lt;chr&gt;, top_7 &lt;chr&gt;,\n#   top_7_2 &lt;chr&gt;, top_6 &lt;chr&gt;, top_6_2 &lt;chr&gt;, top_5 &lt;chr&gt;, top_5_2 &lt;chr&gt;, …\n\n[[3]]\n# A tibble: 6 × 6\n  Contestant          Birthday  Birthplace           Hometown Description Season\n  &lt;chr&gt;               &lt;chr&gt;     &lt;chr&gt;                &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;\n1 Kelly Clarkson      24-Apr-82 Fort Worth, Texas    Burleso… \"She perfo…      1\n2 Justin Guarini      28-Oct-78 Columbus, Georgia    Doylest… \"He perfor…      1\n3 Nikki McKibbin      28-Sep-78 Grand Prairie, Texas &lt;NA&gt;     \"She had p…      1\n4 Tamyra Gray         26-Jul-79 Takoma Park, Maryla… Atlanta… \"She had a…      1\n5 R. J. Helton        17-May-81 Pasadena, Texas      Cumming… \"J. Helton…      1\n6 Christina Christian 21-Jun-81 Brooklyn, New York   &lt;NA&gt;     \".Christin…      1\n\n[[4]]\n# A tibble: 6 × 17\n  season show_number episode    airdate `18_49_rating_share` viewers_in_millions\n   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;                              &lt;dbl&gt;\n1      1           1 Auditions  June 1… 4.8                                 9.85\n2      1           2 Hollywood… June 1… 5.2                                11.2 \n3      1           3 Top 30: G… June 1… 5.2                                10.3 \n4      1           4 Top 30: G… June 1… 4.7                                 9.47\n5      1           5 Top 30: G… June 2… 4.5                                 9.08\n6      1           6 Top 30: G… June 2… 4.2                                 8.53\n# ℹ 11 more variables: timeslot_et &lt;chr&gt;, dvr_18_49 &lt;chr&gt;,\n#   dvr_viewers_millions &lt;chr&gt;, total_18_49 &lt;chr&gt;,\n#   total_viewers_millions &lt;chr&gt;, weekrank &lt;chr&gt;, ref &lt;lgl&gt;, share &lt;chr&gt;,\n#   nightlyrank &lt;dbl&gt;, rating_share_households &lt;chr&gt;, rating_share &lt;chr&gt;\n\n[[5]]\n# A tibble: 6 × 10\n  season winner     runner_up original_release original_network hosted_by judges\n   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;            &lt;chr&gt;     &lt;chr&gt; \n1      1 Kelly Cla… Justin G… June 11 (2002-0… Fox              Ryan Sea… Paula…\n2      2 Ruben Stu… Clay Aik… January 21 (200… Fox              Ryan Sea… Paula…\n3      3 Fantasia … Diana De… January 19 (200… Fox              Ryan Sea… Paula…\n4      4 Carrie Un… Bo Bice   January 18 (200… Fox              Ryan Sea… Paula…\n5      5 Taylor Hi… Katharin… January 17 (200… Fox              Ryan Sea… Paula…\n6      6 Jordin Sp… Blake Le… January 16 (200… Fox              Ryan Sea… Paula…\n# ℹ 3 more variables: no_of_episodes &lt;dbl&gt;, finals_venue &lt;chr&gt;, mentor &lt;chr&gt;\n\n[[6]]\n# A tibble: 6 × 8\n  season    week                 order contestant song  artist song_theme result\n  &lt;chr&gt;     &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt; \n1 Season_01 20020618_top_30_gro…     1 Tamyra Gr… And … Jenni… &lt;NA&gt;       Advan…\n2 Season_01 20020618_top_30_gro…     2 Jim Verra… When… Doris… &lt;NA&gt;       Advan…\n3 Season_01 20020618_top_30_gro…     3 Adriel He… I'll… Edwin… &lt;NA&gt;       Elimi…\n4 Season_01 20020618_top_30_gro…     4 Rodesia E… Dayd… The M… &lt;NA&gt;       Elimi…\n5 Season_01 20020618_top_30_gro…     5 Natalie B… Crazy Patsy… &lt;NA&gt;       Elimi…\n6 Season_01 20020618_top_30_gro…     6 Brad Estr… Just… James… &lt;NA&gt;       Elimi…\n\n\n\nlapply(list(auditions, eliminations, finalists, ratings, seasons, songs), skim)\n\nWarning: There was 1 warning in `dplyr::summarize()`.\nℹ In argument: `dplyr::across(tidyselect::any_of(variable_names),\n  mangled_skimmers$funs)`.\nℹ In group 0: .\nCaused by warning:\n! There were 20 warnings in `dplyr::summarize()`.\nThe first warning was:\nℹ In argument: `dplyr::across(tidyselect::any_of(variable_names),\n  mangled_skimmers$funs)`.\nCaused by warning in `grepl()`:\n! unable to translate 'Jos&lt;8e&gt; \"Sway\" Penala' to a wide string\nℹ Run `dplyr::last_dplyr_warnings()` to see the 19 remaining warnings.\n\n\n[[1]]\n── Data Summary ────────────────────────\n                           Values\nName                       X[[i]]\nNumber of rows             142   \nNumber of columns          12    \n_______________________          \nColumn type frequency:           \n  character                6     \n  Date                     4     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable    n_missing complete_rate min max empty n_unique whitespace\n1 audition_city            0        1       11  32     0       66          0\n2 audition_venue           0        1        7  50     0      115          0\n3 episodes               126        0.113    1  13     0        9          0\n4 episode_air_date        42        0.704    8  20     0       81          0\n5 callback_venue          43        0.697    7  51     0       84          0\n6 guest_judge            133        0.0634   9  19     0        8          0\n\n── Variable type: Date ─────────────────────────────────────────────────────────\n  skim_variable       n_missing complete_rate min        max        median    \n1 audition_date_start         0         1     2002-04-20 2019-09-21 2010-09-05\n2 audition_date_end           0         1     2002-04-22 2019-09-21 2010-09-05\n3 callback_date_start        13         0.908 2002-02-06 2019-09-21 2010-11-09\n4 callback_date_end          13         0.908 2002-02-06 2019-09-21 2010-11-10\n  n_unique\n1      131\n2      131\n3      118\n4      118\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable        n_missing complete_rate mean    sd p0 p25 p50 p75 p100\n1 season                       0         1     10.4  5.53  1   6  10  15   18\n2 tickets_to_hollywood        48         0.662 41.8 76.9   6  20  29  37  561\n  hist \n1 ▅▅▆▅▇\n2 ▇▁▁▁▁\n\n[[2]]\n── Data Summary ────────────────────────\n                           Values\nName                       X[[i]]\nNumber of rows             456   \nNumber of columns          46    \n_______________________          \nColumn type frequency:           \n  character                44    \n  logical                  1     \n  numeric                  1     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n   skim_variable n_missing complete_rate min max empty n_unique whitespace\n 1 place                 1       0.998     1   5     0       44          0\n 2 gender                1       0.998     4   6     0        2          0\n 3 contestant            1       0.998     3  21     0      454          0\n 4 top_36              384       0.158     3  15     0        7          0\n 5 top_36_2            394       0.136     3  15     0        7          0\n 6 top_36_3            404       0.114     3  16     0        7          0\n 7 top_36_4            435       0.0461    3  16     0        6          0\n 8 top_32              424       0.0702    3  15     0        6          0\n 9 top_32_2            427       0.0636    3  16     0        5          0\n10 top_32_3            433       0.0504    3  15     0        6          0\n11 top_32_4            436       0.0439    3  15     0        6          0\n12 top_30              426       0.0658    3  10     0        6          0\n13 top_30_2            431       0.0548    3  10     0        6          0\n14 top_30_3            436       0.0439    3  10     0        6          0\n15 top_25              431       0.0548    3  10     0        4          0\n16 top_25_2            436       0.0439    3  10     0        4          0\n17 top_25_3            440       0.0351    4  10     0        3          0\n18 top_24              240       0.474     3  10     0        5          0\n19 top_24_2            278       0.390     3  12     0        6          0\n20 top_24_3            360       0.211     4  13     0        4          0\n21 top_20              376       0.175     3  10     0        4          0\n22 top_20_2            424       0.0702    3  10     0        4          0\n23 top_16              440       0.0351    4  10     0        2          0\n24 top_14              428       0.0614    4  10     0        2          0\n25 top_13              404       0.114     4  14     0        5          0\n26 top_12              335       0.265     4  12     0        7          0\n27 top_11              312       0.316     4  15     0        8          0\n28 top_11_2            434       0.0482    4  12     0        3          0\n29 wildcard            350       0.232     3  21     0       11          0\n30 top_10              306       0.329     3  12     0       13          0\n31 top_9               348       0.237     4  15     0        9          0\n32 top_9_2             447       0.0197    4  10     0        2          0\n33 top_8               335       0.265     4  15     0        9          0\n34 top_8_2             440       0.0351    4  12     0        5          0\n35 top_7               344       0.246     4  15     0       10          0\n36 top_7_2             442       0.0307    4  12     0        3          0\n37 top_6               366       0.197     4  14     0       10          0\n38 top_6_2             450       0.0132    4  10     0        2          0\n39 top_5               381       0.164     4  12     0        5          0\n40 top_5_2             451       0.0110    4  13     0        2          0\n41 top_4               396       0.132     4  10     0        4          0\n42 top_4_2             452       0.00877   4  10     0        2          0\n43 top_3               411       0.0987    4  10     0        2          0\n44 finale              417       0.0855    6   9     0        6          0\n\n── Variable type: logical ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate mean count\n1 comeback            456             0  NaN \": \" \n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate mean   sd p0 p25 p50 p75 p100 hist \n1 season                0             1 8.86 5.21  1   4   8  13   18 ▇▅▇▅▆\n\n[[3]]\n── Data Summary ────────────────────────\n                           Values\nName                       X[[i]]\nNumber of rows             190   \nNumber of columns          6     \n_______________________          \nColumn type frequency:           \n  character                5     \n  numeric                  1     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min  max empty n_unique whitespace\n1 Contestant            0         1       3   25     0      190          0\n2 Birthday              1         0.995   8    9     0      186          0\n3 Birthplace            6         0.968  11   30     0      163          0\n4 Hometown             88         0.537  11   26     0       96          0\n5 Description          12         0.937  26 1206     0      178          0\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate mean   sd p0 p25 p50 p75 p100 hist \n1 Season                0             1 8.86 4.86  1   5   9  13   17 ▇▆▅▆▇\n\n[[4]]\n── Data Summary ────────────────────────\n                           Values\nName                       X[[i]]\nNumber of rows             593   \nNumber of columns          17    \n_______________________          \nColumn type frequency:           \n  character                12    \n  logical                  1     \n  numeric                  4     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n   skim_variable           n_missing complete_rate min max empty n_unique\n 1 episode                         0        1        6  62     0      313\n 2 airdate                         0        1        5  17     0      590\n 3 18_49_rating_share              0        1        1   9     0      373\n 4 timeslot_et                   515        0.132   16  19     0        8\n 5 dvr_18_49                     539        0.0911   1   3     0        5\n 6 dvr_viewers_millions          539        0.0911   1   4     0       45\n 7 total_18_49                   539        0.0911   1   3     0       16\n 8 total_viewers_millions        539        0.0911   1   5     0       49\n 9 weekrank                      101        0.830    1   4     0       28\n10 share                         449        0.243    1   3     0       21\n11 rating_share_households       515        0.132    7   9     0       55\n12 rating_share                  284        0.521    1   9     0      195\n   whitespace\n 1          0\n 2          0\n 3          0\n 4          0\n 5          0\n 6          0\n 7          0\n 8          0\n 9          0\n10          0\n11          0\n12          0\n\n── Variable type: logical ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate mean count\n1 ref                 593             0  NaN \": \" \n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable       n_missing complete_rate  mean     sd   p0  p25  p50  p75\n1 season                      0        1       8.30  4.63  1     4    8   12  \n2 show_number                 0        1      19.2  11.7   1     9   18   29  \n3 viewers_in_millions         3        0.995  19.9   7.76  5.38 12.6 21.8 26.1\n4 nightlyrank               569        0.0405  2.08  0.929 1     1    2    3  \n  p100 hist \n1 18   ▇▆▇▃▃\n2 44   ▇▇▆▆▃\n3 38.1 ▆▃▇▆▁\n4  4   ▆▇▁▃▂\n\n[[5]]\n── Data Summary ────────────────────────\n                           Values\nName                       X[[i]]\nNumber of rows             18    \nNumber of columns          10    \n_______________________          \nColumn type frequency:           \n  character                8     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable    n_missing complete_rate min max empty n_unique whitespace\n1 winner                   0         1       8  16     0       18          0\n2 runner_up                0         1       7  20     0       18          0\n3 original_release         0         1      47  53     0       18          0\n4 original_network         0         1       3   3     0        2          0\n5 hosted_by                0         1      13  30     0        2          0\n6 judges                   0         1      37  64     0        8          0\n7 finals_venue             3         0.833  13  23     0        4          0\n8 mentor                  16         0.111  12  15     0        2          0\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable  n_missing complete_rate mean   sd p0   p25  p50  p75 p100 hist \n1 season                 0         1      9.5 5.34  1  5.25  9.5 13.8   18 ▇▆▇▆▇\n2 no_of_episodes        14         0.222 19.5 3.32 16 18.2  19   20.2   24 ▃▇▁▁▃\n\n[[6]]\n── Data Summary ────────────────────────\n                           Values\nName                       X[[i]]\nNumber of rows             2429  \nNumber of columns          8     \n_______________________          \nColumn type frequency:           \n  character                7     \n  numeric                  1     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 season                0         1       9   9     0       18          0\n2 week                  0         1       8  54     0      186          0\n3 contestant            0         1       1  51     0      565          0\n4 song                  0         1       1  64     0     1512          0\n5 artist                0         1       2 513     0      921          0\n6 song_theme         1656         0.318   4  33     0      157          0\n7 result               30         0.988   1  25     0       53          0\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate mean   sd p0 p25 p50 p75 p100 hist \n1 order                 0             1 5.93 4.21  1   3   5   8   40 ▇▂▁▁▁\n\n\n\nA lot of missing values across several different variables and tables. I want to start with the contestants, namely the finalists. The finalists dataset has Hometown and Birthplace, with many missing hometown but few missing birthplace. Looking at the tables on Wikipedia, it appears that many of the people with missing Hometowns in the dataset have their birthplace listed as their hometown. I check around 10 of them across different seasons, which may not be a lot but for the sake of this exercise is enough to make me comfortable to impute the hometown with their birthplace when it’s missing. I think Hometown is a more meaningful variable, so if I do something like determining the impact of location on advancement I think hometown is the more reasonable method. Last thing to note, season 18 is included in the eliminations and song datasets but not the finalists dataset.\n\n# logical if_else statement to impute birthplace only when hometown is missing\nfinalists2 &lt;- finalists %&gt;% mutate(\n  Hometown = if_else(is.na(Hometown), Birthplace, Hometown)\n)\n\n\nThere are only a few with missing hometowns now, and I’m tempted to impute these manually by referencing the material found in Wikipedia since it looks like it’s mostly available there. I will resist the urge for now, moving on to joining some of the dataframes for a master finalist dataset.\nEliminations has the placement of the contestants, as well as their gender, so I want to bring those in. There’s much more in eliminations, but the format of the show is inconsistent across seasons so there would be a lot of cleaning to do to make the data more analysis-friendly. For example, I’d be interested in getting the episode someone is eliminated to then bring in things like ratings and viewership of said episodes, but there aren’t common keys across ratings, songs, or eliminations datasets, which would have the data necessary to accomplish that. For now, I think the placement can serve as the contestants success variable and ratings and viewership will have to be a little more disjointed.\n\n# left join to keep everything from finalists\nfinalists3 &lt;- finalists2 %&gt;% \n  left_join(eliminations,\n            join_by(Contestant == contestant, Season == season)\n            ) %&gt;% \n  select(c(names(finalists2), place, gender))\nfinalists3 %&gt;% head()\n\n# A tibble: 6 × 8\n  Contestant        Birthday Birthplace Hometown Description Season place gender\n  &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Kelly Clarkson    24-Apr-… Fort Wort… Burleso… \"She perfo…      1 1     Female\n2 Justin Guarini    28-Oct-… Columbus,… Doylest… \"He perfor…      1 2     Male  \n3 Nikki McKibbin    28-Sep-… Grand Pra… Grand P… \"She had p…      1 3     Female\n4 Tamyra Gray       26-Jul-… Takoma Pa… Atlanta… \"She had a…      1 4     Female\n5 R. J. Helton      17-May-… Pasadena,… Cumming… \"J. Helton…      1 5     Male  \n6 Christina Christ… 21-Jun-… Brooklyn,… Brookly… \".Christin…      1 6     Female\n\nfinalists3 %&gt;% filter(is.na(place))\n\n# A tibble: 6 × 8\n  Contestant        Birthday Birthplace Hometown Description Season place gender\n  &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 \"Rebecca \\\"Becky… 13-Jul-… Dobbs Fer… Dobbs F… \"Her origi…      5 &lt;NA&gt;  &lt;NA&gt;  \n2 \"William \\\"Will\\… 2-Mar-89 The Woodl… The Woo… \"In high s…      5 &lt;NA&gt;  &lt;NA&gt;  \n3 \"Jos\\x8e \\\"Sway\\… 23-Jan-… &lt;NA&gt;       &lt;NA&gt;     \"He was th…      5 &lt;NA&gt;  &lt;NA&gt;  \n4 \"Bobby Bennett, … 4-Jun-86 Denver, C… Denver,… \"(born Jun…      5 &lt;NA&gt;  &lt;NA&gt;  \n5 \"Chikezie Eze\"    11-Sep-… Inglewood… Inglewo… \"During th…      7 &lt;NA&gt;  &lt;NA&gt;  \n6 \"Uch\\x8e\"         15-Jul-… Sugarland… Sugarla… \"As a chil…     17 &lt;NA&gt;  &lt;NA&gt;  \n\n\n\nA few contestants have some foreign characters characters in their names, and the way the tables were read looks like there is some discrepancy that caused different values across our datasets. I will have to correct this first and then join the datasets.\n\n# Some simple string replacements and conditionals to fix names prior to join\nfinalists4 &lt;- finalists2 %&gt;% mutate(\n  Contestant = str_replace_all(Contestant, '\\x8e', 'é'),\n  Contestant = if_else(Contestant == 'Chikezie Eze', 'Chikezie', Contestant),\n  Nickname = trimws(str_extract(Contestant, '\".*?\"'), whitespace = '\"'),\n  Contestant = if_else(!is.na(Nickname) & Nickname != 'Sway', paste(Nickname, word(Contestant, -1)), Contestant),\n  Contestant = if_else(Contestant == 'Bobby Bennett, Jr.', 'Bobby Bennett', Contestant)\n) %&gt;% \n  select(-Nickname) %&gt;% \n  left_join(eliminations,\n            join_by(Contestant == contestant, Season == season)\n            ) %&gt;% \n  select(c(names(finalists2), place, gender)) # only want two variables from eliminations\n\nfinalists4 %&gt;% head() # checking new variables look right\n\n# A tibble: 6 × 8\n  Contestant        Birthday Birthplace Hometown Description Season place gender\n  &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Kelly Clarkson    24-Apr-… Fort Wort… Burleso… \"She perfo…      1 1     Female\n2 Justin Guarini    28-Oct-… Columbus,… Doylest… \"He perfor…      1 2     Male  \n3 Nikki McKibbin    28-Sep-… Grand Pra… Grand P… \"She had p…      1 3     Female\n4 Tamyra Gray       26-Jul-… Takoma Pa… Atlanta… \"She had a…      1 4     Female\n5 R. J. Helton      17-May-… Pasadena,… Cumming… \"J. Helton…      1 5     Male  \n6 Christina Christ… 21-Jun-… Brooklyn,… Brookly… \".Christin…      1 6     Female\n\nfinalists4 %&gt;% filter(is.na(place)) # checking for any nulls from poor join\n\n# A tibble: 0 × 8\n# ℹ 8 variables: Contestant &lt;chr&gt;, Birthday &lt;chr&gt;, Birthplace &lt;chr&gt;,\n#   Hometown &lt;chr&gt;, Description &lt;chr&gt;, Season &lt;dbl&gt;, place &lt;chr&gt;, gender &lt;chr&gt;\n\n\n\nNote that I had to manually override Bobby Bennett’s name because there are other contestants with a “Jr.” suffix that were able to join across the tables.\nFrom the eliminations dataset we brought over the place that each finalist ultimately ended the show in. However, in several instances multiple contestants are eliminated at a time, so they’re placements are listed in the table as a range, e.g. 9-10. We can arguably say these contestants tied for the higher place in the range, so in the 9-10 example two contestants tied for 9th place. So let’s recode those placements accordingly.\n\n# regex to extract only the first number from range placements\nfinalists5 &lt;- finalists4 %&gt;% mutate(\n  numplace = as.numeric(str_extract(place,'(^[0-9]+)'))\n  )\n\nNow the new variable is coded to have only one value and is numeric. At this point it’s a little unclear how useful that will really be, since it’s an integer scale and for many regression problems wouldn’t be appropriate, but we can come back to that later.\n\n# splits up Hometown into city and state, since State will be a smaller category\nfinalists6 &lt;- finalists5 %&gt;% mutate(\n  HomeState = trimws(str_split_fixed(Hometown, ',',2)[,2]),\n  HomeCity = str_split_fixed(Hometown, ',',2)[,1]\n)\n\nfinalists6$HomeState %&gt;% unique()\n\n [1] \"Texas\"             \"Pennsylvania\"      \"Georgia\"          \n [4] \"New York\"          \"California\"        \"Washington\"       \n [7] \"Illinois\"          \"Alabama\"           \"North Carolina\"   \n[10] \"Tennessee\"         \"Utah\"              \"Connecticut\"      \n[13] \"Ohio\"              \"Hawaii\"            \"\"                 \n[16] \"Idaho\"             \"Maui\"              \"Canada\"           \n[19] \"Oklahoma\"          \"New Jersey\"        \"Florida\"          \n[22] \"Nevada\"            \"Louisiana\"         \"Massachusetts\"    \n[25] \"Arkansas\"          \"Colorado\"          \"Arizona\"          \n[28] \"Michigan\"          \"Virginia\"          \"South Carolina\"   \n[31] \"Ireland\"           \"Oregon\"            \"Western Australia\"\n[34] \"Indiana\"           \"Wisconsin\"         \"Merseyside\"       \n[37] \"Mississippi\"       \"Rhode Island\"      \"Karen Carpenter\"  \n[40] \"New Hampshire\"     \"Iowa\"              \"Maryland\"         \n\nfinalists6 %&gt;% filter(HomeState == 'Karen Carpenter')\n\n# A tibble: 1 × 11\n  Contestant    Birthday  Birthplace    Hometown Description Season place gender\n  &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Amber Holcomb 17-Mar-94 e Dion, Kare… e Dion,… \"She origi…     12 4     Female\n# ℹ 3 more variables: numplace &lt;dbl&gt;, HomeState &lt;chr&gt;, HomeCity &lt;chr&gt;\n\n\n\nThere’s a stray ‘Karen Carpenter’ in one value for the new State variable, and looking at the value it appears to be an issue with the original Birthplace variable. We can take care of this easily by recoding the variable manually.\n\n# Manually correcting incorrect location variables for the one contestant\nfinalists7 &lt;- finalists6 %&gt;% mutate(\n  Birthplace = if_else(Birthplace == 'e Dion, Karen Carpenter', NA, Birthplace),\n  Hometown = if_else(Hometown == 'e Dion, Karen Carpenter', NA, Hometown),\n  HomeState = if_else(HomeState == 'Karen Carpenter', NA, HomeState),\n  HomeCity = if_else(HomeCity == 'e Dion', NA, HomeCity)\n)\nfinalists7 %&gt;% filter(HomeState == 'Karen Carpenter'|\n                        Birthplace == 'e Dion, Karen Carpenter'| \n                        Hometown == 'e Dion, Karen Carpenter' | \n                        HomeState == 'Karen Carpenter')\n\n# A tibble: 0 × 11\n# ℹ 11 variables: Contestant &lt;chr&gt;, Birthday &lt;chr&gt;, Birthplace &lt;chr&gt;,\n#   Hometown &lt;chr&gt;, Description &lt;chr&gt;, Season &lt;dbl&gt;, place &lt;chr&gt;, gender &lt;chr&gt;,\n#   numplace &lt;dbl&gt;, HomeState &lt;chr&gt;, HomeCity &lt;chr&gt;\n\n\nAnother variable that might be interesting is age. We have birthdate, however since we have multiple seasons across different years it’s not valuable to us like it is. To remedy this, I will use the auditions dataset to extract a contestant’s age at the time of audition start.\n\n# simplifies Audition data to just get needed dates\nAuditionStarts &lt;- auditions %&gt;% \n  group_by(season) %&gt;% \n  summarize(\n    AudStart = min(audition_date_start)\n  )\n#joins simplified audition data, converts Birthday to date, \n#imputes missing contest birthday, and calculates Age at start of auditions\nfinalists8 &lt;- finalists7 %&gt;% \n  left_join(AuditionStarts, join_by(Season == season)) %&gt;% \n  mutate(Birthday = dmy(Birthday),\n         Birthday = if_else(Contestant == 'Jax', ymd('1996-05-05'), Birthday),\n         Age = floor(as.numeric(interval(Birthday, AudStart), 'years'))\n         )\nfinalists8 %&gt;% filter(is.na(Birthday))\n\n# A tibble: 0 × 13\n# ℹ 13 variables: Contestant &lt;chr&gt;, Birthday &lt;date&gt;, Birthplace &lt;chr&gt;,\n#   Hometown &lt;chr&gt;, Description &lt;chr&gt;, Season &lt;dbl&gt;, place &lt;chr&gt;, gender &lt;chr&gt;,\n#   numplace &lt;dbl&gt;, HomeState &lt;chr&gt;, HomeCity &lt;chr&gt;, AudStart &lt;date&gt;, Age &lt;dbl&gt;\n\n\n\nThis gets us the Age for every finalist at the start of their respective season’s audition start. I could have tried to extract the location of their audition from the Description column and then used that to match up the audition date instead, but this is quicker, quite frankly. One person did not have a value in the Birthday column, and in this case I imputed it manually because the information is available from Wikipedia quickly, and missing numeric values are a much greater problem for model building than missing categorical variables are.\nThat should have the values fixed and additional variables added. Now let’s do a little exploring.\n\n# group bar chart of placement counts by gender\nfinalists8 %&gt;% \n  filter(numplace &lt;= 10) %&gt;% \n  group_by(numplace, gender) %&gt;% \n  summarize(cnt = n()) %&gt;%\n  ggplot() +\n  geom_bar(aes(fill = gender, y = cnt, x=numplace), position = 'dodge', stat = 'identity') +\n  theme_minimal() +\n  scale_fill_manual(values = c('#D81561', '#1379BF')) +\n  labs(title = 'Distribution of Top 10 Male and Female Finalists', x = 'Placement', y = 'Count', fill = 'Gender') +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(color = 'gray')) +\n  scale_y_continuous(expand = c(0,0),limits = c(0, NA)) +\n  scale_x_continuous(breaks = seq(1,10))\n\n`summarise()` has grouped output by 'numplace'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\n# creates a simplified dataset of Homestate contestant frequencies\nHSOrder &lt;- finalists8 %&gt;%\n  filter(numplace &lt;= 10) %&gt;%\n  group_by(HomeState) %&gt;% \n  summarize(\n    ordercnt = n()\n  )\n#joins above HomeState data so that it can used to reorder HomeState variable by\n#total frequency. Horizontal bar chart of location color-coded by placement\nfinalists8 %&gt;% \n  filter(numplace &lt;= 10) %&gt;%\n  group_by(numplace, HomeState) %&gt;% \n  summarize(cnt = n()) %&gt;%\n  left_join(HSOrder, join_by(HomeState==HomeState)) %&gt;% \n  ggplot() +\n  geom_bar(aes(fill = numplace, y = fct_reorder(HomeState, ordercnt), x=cnt), position = 'stack', stat = 'identity') +\n  theme_minimal() +\n  scale_fill_viridis(option = 'mako') +\n  labs(title = 'Count and Placement of Finalists by State', x = 'Count', y = 'Home State', fill = 'Placement') +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(color = 'gray'), axis.text.y = element_text(size = 7)) +\n  scale_x_continuous(expand = c(0,0),limits = c(0, NA))\n\n`summarise()` has grouped output by 'numplace'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\n# Scatterplot of Age by Placement, stratified by Gender. Jitter used for readability\nfinalists8 %&gt;% \n  ggplot() +\n  geom_jitter(aes(color = gender, y = numplace, x=Age)) +\n  theme_minimal() +\n  labs(title = 'Placement vs Age, Stratified by Gender', x = 'Age', y = 'Placement', color = 'Gender') +\n  theme(panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank(), \n        axis.line = element_line(color = 'gray'))\n\n\n\n\n\n\n\n\n\nThe exploratory charts were made to see if there was a relationship between a contestant’s Home State, their gender, or Age and their placement in the competition. Truthfully looking at these charts nothing is really jumping out at me, which surprises me at least a little, especially for Age and Gender. Despite seeing no obvious relationship, I think this is a good opportunity to see if a model would tell us any differently. On paper it looks like the competition is fairly non-discriminatory, at least with the people that make it a far as a finalist, so if a more rigorous statistical method like a model says otherwise then that would be interesting. If not, well that’s good for American Idol right?.\nFirst we’ll need to prepare the data a bit:\n\n# Preparing data for only needed predictors\nmodeldata &lt;- finalists8 %&gt;% \n  mutate(\n    top3 = as.factor(if_else(numplace &lt;= 3, 1,0))\n      ) %&gt;% \n  select(gender, HomeState, Age, top3)\n\n\nWe now have only the variables of interest for our models in the dataset. It’s only four predictors, but given that the number of observations is also relatively small, a simple model is probably for the best. Let’s start with a logistic regression model:\n\nLogistic Regression\n\nlogreg &lt;- logistic_reg() %&gt;% \n  set_engine('glm')\n\nset.seed(42)\ntrainpart &lt;- createDataPartition(modeldata$top3, p=.7)[[1]]\nmodeltrain &lt;- modeldata[trainpart,]\nmodeltest &lt;- modeldata[-trainpart,]\n\nsimprecipe &lt;- recipe(top3 ~ ., data = modeltrain) %&gt;% \n  step_interact(terms = ~ Age:gender) %&gt;%\n  step_novel(HomeState) %&gt;% \n  step_unknown(HomeState) %&gt;% \n  step_dummy(HomeState,gender)\n\n\nlogrecipe &lt;- simprecipe %&gt;% \n  step_normalize(Age)\n  \n\nset.seed(42)\n\nlogreg_wflow &lt;- workflow() %&gt;% \n  add_model(logreg) %&gt;% \n  add_recipe(logrecipe)\n\nlogreg_fit &lt;- logreg_wflow %&gt;% fit(data = modeltrain)\nlogpreds_results &lt;- logreg_fit %&gt;% extract_fit_parsnip() %&gt;% tidy()\nlogpreds_results\n\n# A tibble: 44 × 5\n   term                  estimate std.error  statistic p.value\n   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)            -19.5    5328.    -0.00365    0.997 \n 2 Age                     -0.717     0.441 -1.62       0.104 \n 3 Age_x_genderMale         0.261     0.154  1.70       0.0899\n 4 HomeState_Alabama       18.3    5328.     0.00343    0.997 \n 5 HomeState_Arizona       18.3    5328.     0.00343    0.997 \n 6 HomeState_Arkansas      39.3   12002.     0.00328    0.997 \n 7 HomeState_California    16.7    5328.     0.00313    0.998 \n 8 HomeState_Colorado      -0.646 12002.    -0.0000538  1.00  \n 9 HomeState_Connecticut   -1.03  12002.    -0.0000857  1.00  \n10 HomeState_Florida       -0.441  6231.    -0.0000708  1.00  \n# ℹ 34 more rows\n\n\n\ntestpreds &lt;- bind_cols(\n  top3 = modeltest$top3,\n  predict(logreg_fit, modeltest),\n  predict(logreg_fit, modeltest, type = 'prob')\n)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n\nnames(testpreds) &lt;- c('top3', 'logpreds', 'log0', 'log1')\n\nmodelperf &lt;- bind_rows(\n  acc_log = accuracy(testpreds, top3, logpreds, event_level = 'second'),\n  prec_log = precision(testpreds, top3, logpreds, event_level = 'second'),\n  rec_log = recall(testpreds, top3, logpreds, event_level = 'second'),\n  spec_log = specificity(testpreds, top3, logpreds, event_level = 'second')\n)\nnames(modelperf) &lt;- c('metric', 'estimator', 'log_reg')\nmodelperf\n\n# A tibble: 4 × 3\n  metric      estimator log_reg\n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;\n1 accuracy    binary      0.696\n2 precision   binary      0.364\n3 recall      binary      0.286\n4 specificity binary      0.833\n\n\n\nThe logistic regression model is pretty terrible, but there are a few things to glean here. First off, originally I had run this model without the interaction between Age and Gender, and the p-values for all predictors were well above any rejection threshold. However, including the interaction drastically changes the results for Age and Gender. Age alone is still not significant against any alpha level with a p-value of 0.1042617, but against an alpha level of 0.1 the interaction of Age and Gender is significant with a p-value of 0.0899354. Similarly Gender alone is significant at an alpha level of 0.1 with a p-value of 0.0690929. one could interpret these results as two finalists of the same age but different gender have different odds of making it to the top 3. An alpha level of 0.1 may not be the most stringent or typical value, but this example was not intended to be the most rigorous so I feel comfortable taking the liberty. The location variables are all basically worthless, though surprisingly taking them out causes the model to guess every contestant as Negative, that they would not make it to the top 3. That likely means some feature selection would be necessary if there was any intention of taking this model any further.\nPrediction performance for the model is what truly reveals the quality. The accuracy at first glance seems not totally terrible; with a value of 0.6964286, it’s better than flipping a coin at least. However, it is important to remember this is whether a finalist makes it to the top 3 or not. One season has more than 20 finalists, so in that instance there is a 98.5% chance of randomly choosing someone who is not in the top 3. In fact, among the finalists in our dataset only 25.2631579% of contestants make the top 3, meaning if the model were to guess that nobody made the top 3 it would have an accuracy value of 74.7368421%. This is also made clear by the poor Recall value of 0.2857143, essentially saying that out of the actual positive examples, few are predicted correctly.\nDespite the poor predictive performance this model suggests a relationship may exist, and warrants some more exploration. I will try a few more models to see if I get different results.\nThe scatterplot from earlier makes me think there may be some possibility of groupings based on the predictor variables. If you look at the bottom part of the scatter plot, where the top three would be, there seem to be a possible difference in Age and Gender amongst who was in the top 3. this leads me to think a KNN model might be possible, though this will largely depend on if those differences amongst the top 3 are also apparent for those not in the top 3.\n\n\nK-Nearest Neighbors\n\nknnmodel &lt;- nearest_neighbor() %&gt;% \n  set_engine('kknn') %&gt;% \n  set_mode('classification')\n\n\nknnrecipe &lt;- simprecipe %&gt;% \n  step_normalize(Age)\n  \n\nset.seed(42)\n\nknn_wflow &lt;-\n  workflow() %&gt;% \n  add_model(knnmodel) %&gt;% \n  add_recipe(knnrecipe)\n\nknn_fit &lt;- knn_wflow %&gt;% fit(data = modeltrain)\nknnpreds_results &lt;- knn_fit %&gt;% extract_fit_parsnip()\nknnpreds_results\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(5,     data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.3283582\nBest kernel: optimal\nBest k: 5\n\n\n\ntestpreds &lt;- bind_cols(testpreds,\n  predict(knn_fit, modeltest),\n  predict(knn_fit, modeltest, type = 'prob')\n)\n\nnames(testpreds) &lt;- c('top3', 'logpreds', 'log0', 'log1', 'knnpreds', 'knn0', 'knn1')\n\nmodelperf &lt;- bind_cols(modelperf,\n                       bind_rows(\n                         accuracy(testpreds, top3, knnpreds, event_level = 'second'),\n                         precision(testpreds, top3, knnpreds, event_level = 'second'),\n                         recall(testpreds, top3, knnpreds, event_level = 'second'),\n                         specificity(testpreds, top3, knnpreds, event_level = 'second')\n                         )[,3]\n                       )\nnames(modelperf) &lt;- c('metric', 'estimator', 'log_reg', 'KNN')\nmodelperf\n\n# A tibble: 4 × 4\n  metric      estimator log_reg   KNN\n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 accuracy    binary      0.696 0.661\n2 precision   binary      0.364 0.308\n3 recall      binary      0.286 0.286\n4 specificity binary      0.833 0.786\n\n\n\nThe KNN results look remarkably similar to the logistic regression results, though it does in fact perform worse. The recall is, somewhat surprisingly, the same at 0.2857143, but all other model performance metrics are worse. From this we can tell that the model guessed the same proportion of contestants who actually did make the top 3, but incorrectly guessed contestants made the top 3 more times. The KNN model doesn’t give us information about model predictor importance or significance, so there’s less to say about this one, but ultimately it offers nothing to us that the logistic regression does not.\nFor the last model I want to try something with variable selection/ reduction. I think the abundance of location variables could have a negative impact on my results, so reducing variables should help that.\n\n\nMARS\n\nmarsmodel &lt;- mars(prod_degree = 2, prune_method = \"backward\") %&gt;% \n  set_engine('earth') %&gt;% \n  set_mode('classification') %&gt;% \n  translate()\n\n\nset.seed(42)\n\nmars_wflow &lt;-\n  workflow() %&gt;% \n  add_model(marsmodel) %&gt;% \n  add_recipe(simprecipe)  #MARS does not require much preprocessing beyond what was already done\n\nmars_fit &lt;- mars_wflow %&gt;% fit(data = modeltrain)\nmarspreds_results &lt;- mars_fit %&gt;% extract_fit_parsnip()\nmarspreds_results\n\nparsnip model object\n\nGLM (family binomial, link logit):\n nulldev  df       dev  df   devratio     AIC iters converged\n 151.794 133   146.219 132     0.0367   150.2    14         1\n\nEarth selected 2 of 45 terms, and 2 of 43 predictors\nTermination condition: GRSq -10 at 45 terms\nImportance: HomeState_North.Carolina, gender_Male, Age-unused, ...\nNumber of terms at each degree of interaction: 1 0 1\nEarth GCV 0.1907479    RSS 24.24242    GRSq 0.007605869    RSq 0.04456328\n\nmars_fit %&gt;% vip()\n\n\n\n\n\n\n\n\n\ntestpreds &lt;- bind_cols(testpreds,\n  predict(mars_fit, modeltest),\n  predict(mars_fit, modeltest, type = 'prob')\n)\n\nnames(testpreds) &lt;- c('top3', 'logpreds', 'log0', 'log1', 'knnpreds', 'knn0', 'knn1', 'marspreds', 'mars0', 'mars1')\n\nmodelperf &lt;- bind_cols(modelperf,\n                       bind_rows(\n                         accuracy(testpreds, top3, marspreds, event_level = 'second'),\n                         precision(testpreds, top3, marspreds, event_level = 'second'),\n                         recall(testpreds, top3, marspreds, event_level = 'second'),\n                         specificity(testpreds, top3, marspreds, event_level = 'second')\n                         )[,3]\n                       )\nnames(modelperf) &lt;- c('metric', 'estimator', 'log_reg', 'KNN', 'MARS')\nmodelperf\n\n# A tibble: 4 × 5\n  metric      estimator log_reg   KNN   MARS\n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 accuracy    binary      0.696 0.661 0.75  \n2 precision   binary      0.364 0.308 0.5   \n3 recall      binary      0.286 0.286 0.0714\n4 specificity binary      0.833 0.786 0.976 \n\n\n\nThe MARS models have the advantage of variable selection, and my thinking is if it removed several of the unimportant location variables but kept some that were informative we have seen a difference in model performance. However, the variable selection only chose 2 variables, HomeState_North.Carolina (dummy variable) and gender_Male (dummy variable, but for a binary variable, so essentially just gender). The Gender variable being selected is unsurprising considering what we saw earlier with the logistic regression, but the North Carolina variable is somewhat surprising. Though, looking back at the location bar chart from earlier the frequency that finalists that come from North Carolina place high is noteworthy, so it does make some sense.\nOur predictive power is ultimately worse than both of the previous models. Specificity, precision, and accuracy all went up, but at a huge cost to recall, which had a value of 0.0714286. This is because the Model only predicted 2 contestants to be in the top 3, one of which it admittedly got correct, but this was too conservative to be of any actual value. Given the variables selected, it likely assumes any finalist who is Male from North Carolina will make the top 3.\nUltimately the last two models were too complex to appropriately predict simple data like this one, which is likely why the simpler model performed the best. Still, the predictors were just not enough to make a worthwhile prediction, So including other predictors for future work could be useful. Possibly something like finalists who sing songs by certain artists, or even just lumping more of the location responses together so there is not so much noise caused by the large number of dummy variables. That said, while there may be some underlying relationship between age and gender, it is subtle enough to argue that the American Idol contestants likely stand a fair chance, regardless of their background."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "The CDC data set I have chosen for this exercise is the Tobacco Usage Behavioral Risk Factor data set. The data comes from a survey administered by the CDC as part of the State Tobacco Activities Tracking and Evaluation (STATE) system. The data set contains information about the year the survey was administered, respondent location (State or territory), demographics (namely Race, Age, Education, and Gender), and it asks questions about a person’s tobacco usage status (current, former, never used), their cessation status (whether they have quit or attempted to quit in the last year), and the frequency that they use tobacco for those that do (daily, some days). The usage questions are asked for three different types of Tobacco products, Cigarettes, Smokeless Tobacco, and E-cigarettes.\n\npacman::p_load(here, tidyverse, skimr, plotly, synthpop, patchwork)\n\n\nrawdata &lt;- read_csv('cdc-data-raw.csv')\n\nRows: 43341 Columns: 31\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (25): YEAR, LocationAbbr, LocationDesc, TopicType, TopicDesc, MeasureDes...\ndbl  (6): Data_Value, Data_Value_Std_Err, Low_Confidence_Limit, High_Confide...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nstr(rawdata) # getting an idea for data structure\n\nspc_tbl_ [43,341 × 31] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ YEAR                      : chr [1:43341] \"2017\" \"2018\" \"2017\" \"2016\" ...\n $ LocationAbbr              : chr [1:43341] \"GU\" \"US\" \"US\" \"GU\" ...\n $ LocationDesc              : chr [1:43341] \"Guam\" \"National Median (States and DC)\" \"National Median (States and DC)\" \"Guam\" ...\n $ TopicType                 : chr [1:43341] \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" ...\n $ TopicDesc                 : chr [1:43341] \"Cigarette Use (Adults)\" \"Cigarette Use (Adults)\" \"Cigarette Use (Adults)\" \"Smokeless Tobacco Use (Adults)\" ...\n $ MeasureDesc               : chr [1:43341] \"Current Smoking\" \"Smoking Status\" \"Smoking Status\" \"Current Use\" ...\n $ DataSource                : chr [1:43341] \"BRFSS\" \"BRFSS\" \"BRFSS\" \"BRFSS\" ...\n $ Response                  : chr [1:43341] NA \"Current\" \"Never\" NA ...\n $ Data_Value_Unit           : chr [1:43341] \"%\" \"%\" \"%\" \"%\" ...\n $ Data_Value_Type           : chr [1:43341] \"Percentage\" \"Percentage\" \"Percentage\" \"Percentage\" ...\n $ Data_Value                : num [1:43341] 30 16.1 58.2 0.5 21.9 21.6 2.8 3.6 29.2 14.8 ...\n $ Data_Value_Footnote_Symbol: chr [1:43341] NA NA NA NA ...\n $ Data_Value_Footnote       : chr [1:43341] NA NA NA NA ...\n $ Data_Value_Std_Err        : num [1:43341] 2.3 NA NA 0.4 4.9 0.8 0.3 0.4 2.4 0.6 ...\n $ Low_Confidence_Limit      : num [1:43341] 25.4 NA NA 0 12.4 20 2.2 2.7 24.5 13.6 ...\n $ High_Confidence_Limit     : num [1:43341] 34.6 NA NA 1.2 31.4 23.2 3.4 4.5 33.9 16 ...\n $ Sample_Size               : num [1:43341] 692 NA NA 123 209 ...\n $ Gender                    : chr [1:43341] \"Male\" \"Overall\" \"Overall\" \"Overall\" ...\n $ Race                      : chr [1:43341] \"All Races\" \"All Races\" \"All Races\" \"Hispanic\" ...\n $ Age                       : chr [1:43341] \"All Ages\" \"All Ages\" \"All Ages\" \"All Ages\" ...\n $ Education                 : chr [1:43341] \"All Grades\" \"All Grades\" \"All Grades\" \"All Grades\" ...\n $ GeoLocation               : chr [1:43341] \"(13.444304, 144.793731)\" NA NA \"(13.444304, 144.793731)\" ...\n $ TopicTypeId               : chr [1:43341] \"BEH\" \"BEH\" \"BEH\" \"BEH\" ...\n $ TopicId                   : chr [1:43341] \"100BEH\" \"100BEH\" \"100BEH\" \"150BEH\" ...\n $ MeasureId                 : chr [1:43341] \"110CSA\" \"165SSA\" \"165SSA\" \"177SCU\" ...\n $ StratificationID1         : chr [1:43341] \"2GEN\" \"1GEN\" \"1GEN\" \"1GEN\" ...\n $ StratificationID2         : chr [1:43341] \"8AGE\" \"8AGE\" \"8AGE\" \"8AGE\" ...\n $ StratificationID3         : chr [1:43341] \"6RAC\" \"6RAC\" \"6RAC\" \"4RAC\" ...\n $ StratificationID4         : chr [1:43341] \"6EDU\" \"6EDU\" \"6EDU\" \"6EDU\" ...\n $ SubMeasureID              : chr [1:43341] \"BRF21\" \"BRF27\" \"BRF28\" \"BRF69\" ...\n $ DisplayOrder              : num [1:43341] 21 27 28 69 22 21 77 71 26 27 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   YEAR = col_character(),\n  ..   LocationAbbr = col_character(),\n  ..   LocationDesc = col_character(),\n  ..   TopicType = col_character(),\n  ..   TopicDesc = col_character(),\n  ..   MeasureDesc = col_character(),\n  ..   DataSource = col_character(),\n  ..   Response = col_character(),\n  ..   Data_Value_Unit = col_character(),\n  ..   Data_Value_Type = col_character(),\n  ..   Data_Value = col_double(),\n  ..   Data_Value_Footnote_Symbol = col_character(),\n  ..   Data_Value_Footnote = col_character(),\n  ..   Data_Value_Std_Err = col_double(),\n  ..   Low_Confidence_Limit = col_double(),\n  ..   High_Confidence_Limit = col_double(),\n  ..   Sample_Size = col_double(),\n  ..   Gender = col_character(),\n  ..   Race = col_character(),\n  ..   Age = col_character(),\n  ..   Education = col_character(),\n  ..   GeoLocation = col_character(),\n  ..   TopicTypeId = col_character(),\n  ..   TopicId = col_character(),\n  ..   MeasureId = col_character(),\n  ..   StratificationID1 = col_character(),\n  ..   StratificationID2 = col_character(),\n  ..   StratificationID3 = col_character(),\n  ..   StratificationID4 = col_character(),\n  ..   SubMeasureID = col_character(),\n  ..   DisplayOrder = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(rawdata) # summary statistics\n\n     YEAR           LocationAbbr       LocationDesc        TopicType        \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  TopicDesc         MeasureDesc         DataSource          Response        \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n Data_Value_Unit    Data_Value_Type      Data_Value   \n Length:43341       Length:43341       Min.   : 0.00  \n Class :character   Class :character   1st Qu.: 5.10  \n Mode  :character   Mode  :character   Median :17.10  \n                                       Mean   :25.36  \n                                       3rd Qu.:38.90  \n                                       Max.   :99.90  \n                                       NA's   :2117   \n Data_Value_Footnote_Symbol Data_Value_Footnote Data_Value_Std_Err\n Length:43341               Length:43341        Min.   : 0.000    \n Class :character           Class :character    1st Qu.: 0.600    \n Mode  :character           Mode  :character    Median : 1.100    \n                                                Mean   : 1.711    \n                                                3rd Qu.: 2.200    \n                                                Max.   :16.600    \n                                                NA's   :2195      \n Low_Confidence_Limit High_Confidence_Limit  Sample_Size       Gender         \n Min.   : 0.00        Min.   :  0.00        Min.   :   50   Length:43341      \n 1st Qu.: 3.20        1st Qu.:  6.90        1st Qu.:  480   Class :character  \n Median :14.30        Median : 19.80        Median : 1798   Mode  :character  \n Mean   :22.01        Mean   : 28.68        Mean   : 3050                     \n 3rd Qu.:30.50        3rd Qu.: 47.40        3rd Qu.: 4203                     \n Max.   :99.80        Max.   :100.00        Max.   :40726                     \n NA's   :2195         NA's   :2195          NA's   :2195                      \n     Race               Age             Education         GeoLocation       \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n TopicTypeId          TopicId           MeasureId         StratificationID1 \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n StratificationID2  StratificationID3  StratificationID4  SubMeasureID      \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  DisplayOrder  \n Min.   : 5.00  \n 1st Qu.:24.00  \n Median :51.00  \n Mean   :47.08  \n 3rd Qu.:71.00  \n Max.   :81.00  \n                \n\nskim(rawdata) # primarily to get completion rates\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n43341\n\n\nNumber of columns\n31\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n25\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nYEAR\n0\n1.00\n4\n9\n0\n17\n0\n\n\nLocationAbbr\n0\n1.00\n2\n2\n0\n54\n0\n\n\nLocationDesc\n0\n1.00\n4\n31\n0\n54\n0\n\n\nTopicType\n0\n1.00\n25\n25\n0\n1\n0\n\n\nTopicDesc\n0\n1.00\n18\n30\n0\n4\n0\n\n\nMeasureDesc\n0\n1.00\n11\n59\n0\n10\n0\n\n\nDataSource\n0\n1.00\n5\n5\n0\n1\n0\n\n\nResponse\n28323\n0.35\n5\n11\n0\n6\n0\n\n\nData_Value_Unit\n0\n1.00\n1\n1\n0\n1\n0\n\n\nData_Value_Type\n0\n1.00\n10\n10\n0\n1\n0\n\n\nData_Value_Footnote_Symbol\n41224\n0.05\n1\n1\n0\n1\n0\n\n\nData_Value_Footnote\n41224\n0.05\n71\n71\n0\n1\n0\n\n\nGender\n0\n1.00\n4\n7\n0\n3\n0\n\n\nRace\n0\n1.00\n5\n29\n0\n6\n0\n\n\nAge\n0\n1.00\n8\n18\n0\n8\n0\n\n\nEducation\n0\n1.00\n10\n12\n0\n4\n0\n\n\nGeoLocation\n78\n1.00\n23\n41\n0\n53\n0\n\n\nTopicTypeId\n0\n1.00\n3\n3\n0\n1\n0\n\n\nTopicId\n0\n1.00\n6\n6\n0\n4\n0\n\n\nMeasureId\n0\n1.00\n6\n6\n0\n14\n0\n\n\nStratificationID1\n0\n1.00\n4\n4\n0\n3\n0\n\n\nStratificationID2\n0\n1.00\n4\n4\n0\n8\n0\n\n\nStratificationID3\n0\n1.00\n4\n4\n0\n6\n0\n\n\nStratificationID4\n0\n1.00\n4\n4\n0\n4\n0\n\n\nSubMeasureID\n0\n1.00\n5\n5\n0\n53\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nData_Value\n2117\n0.95\n25.36\n25.00\n0\n5.1\n17.1\n38.9\n99.9\n▇▃▂▁▁\n\n\nData_Value_Std_Err\n2195\n0.95\n1.71\n1.72\n0\n0.6\n1.1\n2.2\n16.6\n▇▁▁▁▁\n\n\nLow_Confidence_Limit\n2195\n0.95\n22.01\n24.14\n0\n3.2\n14.3\n30.5\n99.8\n▇▂▂▁▁\n\n\nHigh_Confidence_Limit\n2195\n0.95\n28.68\n26.21\n0\n6.9\n19.8\n47.4\n100.0\n▇▃▂▂▁\n\n\nSample_Size\n2195\n0.95\n3050.17\n3798.87\n50\n480.0\n1798.0\n4203.0\n40726.0\n▇▁▁▁▁\n\n\nDisplayOrder\n0\n1.00\n47.08\n24.49\n5\n24.0\n51.0\n71.0\n81.0\n▂▇▁▂▇\n\n\n\n\n\n\nhead(rawdata, 20) # getting first 20 rows\n\n# A tibble: 20 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2017     GU           Guam         Tobacco … Cigarett… Current Sm… BRFSS     \n 2 2018     US           National Me… Tobacco … Cigarett… Smoking St… BRFSS     \n 3 2017     US           National Me… Tobacco … Cigarett… Smoking St… BRFSS     \n 4 2016     GU           Guam         Tobacco … Smokeles… Current Use BRFSS     \n 5 2014     GU           Guam         Tobacco … Cigarett… Current Sm… BRFSS     \n 6 2012     IN           Indiana      Tobacco … Cigarett… Current Sm… BRFSS     \n 7 2018-20… DE           Delaware     Tobacco … Smokeles… Current Us… BRFSS     \n 8 2011     MO           Missouri     Tobacco … Smokeles… Current Use BRFSS     \n 9 2013     ME           Maine        Tobacco … Cigarett… Smoking Fr… BRFSS     \n10 2017     WA           Washington   Tobacco … Cigarett… Smoking St… BRFSS     \n11 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n12 2011     MN           Minnesota    Tobacco … Smokeles… User Status BRFSS     \n13 2018     VA           Virginia     Tobacco … Cigarett… Smoking Fr… BRFSS     \n14 2014     MN           Minnesota    Tobacco … Cigarett… Smoking St… BRFSS     \n15 2017-20… WA           Washington   Tobacco … Cigarett… Current Sm… BRFSS     \n16 2017     UT           Utah         Tobacco … Smokeles… Current Use BRFSS     \n17 2012     CA           California   Tobacco … Smokeles… Frequency … BRFSS     \n18 2015     FL           Florida      Tobacco … Smokeles… Current Use BRFSS     \n19 2015     RI           Rhode Island Tobacco … Smokeles… Current Use BRFSS     \n20 2019     AZ           Arizona      Tobacco … Smokeles… Current Use BRFSS     \n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, StratificationID1 &lt;chr&gt;,\n#   StratificationID2 &lt;chr&gt;, StratificationID3 &lt;chr&gt;, …\n\n\nThe data set is not the most analyst friendly format. Generally the ideal format would be variables in the columns, observations in the rows. This data set has variables across several rows, and it is mostly aggregated. The questions are broken up by the different response options, which are spread across rows even though the values are proportions and therefore the samples sizes listed for the different response options for a single question come from the same audience. Aggregations grouped by each of the demographics, locations, and timeframes are also in the rows, similar to a pivot table, so getting things like summary statistics will require filtering these aggregated values. Some survey question variables have missing values, but with the data in this format it’s difficult to make much of them, but we will come back to that. There are a few variables that only serve to give information about the data set itself, like Data_Value_Unit, which only contains one value, “Percentage”, to inform that the Value column is a percentage. Variable like this (Zero or Near Zero Variance) can be removed, since they give no information about the individual observations.\nVariables to be removed: Zero Variance variables: Data_Value_Unit, Data_Value_Type, DataSource, TopicType, TopicTypeId NZV: Data_Value_Footnote_Symbol, Data_Value_Footnote\n\nrawdata %&gt;% filter(is.na(Data_Value)) #looking at nulls to determine why they are there\n\n# A tibble: 2,117 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n 2 2016     IA           Iowa         Tobacco … Smokeles… Frequency … BRFSS     \n 3 2014     ID           Idaho        Tobacco … Smokeles… Frequency … BRFSS     \n 4 2019     MI           Michigan     Tobacco … Smokeles… Frequency … BRFSS     \n 5 2016-20… MA           Massachuset… Tobacco … Smokeles… Current Us… BRFSS     \n 6 2013-20… DC           District of… Tobacco … Cigarett… Current Sm… BRFSS     \n 7 2013     ME           Maine        Tobacco … Cigarett… Current Sm… BRFSS     \n 8 2011     AZ           Arizona      Tobacco … Smokeles… Frequency … BRFSS     \n 9 2018     AL           Alabama      Tobacco … Smokeles… Frequency … BRFSS     \n10 2018     NC           North Carol… Tobacco … Smokeles… Frequency … BRFSS     \n# ℹ 2,107 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\nMissing values from the Data_Value column occur when the sample size is not large enough to report results. Some of these missing values may have the opportunity for imputation, and filtering these could cause some confusion in the actual structure of the data (complete and consistent combinations of categorical values) so we won’t filter anything out just yet. note this is evidenced by the Footnote column with the following message: “Data in these cells have been suppressed because of a small sample size.”\n\nunique(paste(rawdata$LocationDesc,rawdata$LocationAbbr)) # paste concatenates columns, unique to get distinct values.\n\n [1] \"Guam GU\"                            \"National Median (States and DC) US\"\n [3] \"Indiana IN\"                         \"Delaware DE\"                       \n [5] \"Missouri MO\"                        \"Maine ME\"                          \n [7] \"Washington WA\"                      \"Illinois IL\"                       \n [9] \"Minnesota MN\"                       \"Virginia VA\"                       \n[11] \"Utah UT\"                            \"California CA\"                     \n[13] \"Florida FL\"                         \"Rhode Island RI\"                   \n[15] \"Arizona AZ\"                         \"District of Columbia DC\"           \n[17] \"Kansas KS\"                          \"Nevada NV\"                         \n[19] \"Alabama AL\"                         \"West Virginia WV\"                  \n[21] \"Wisconsin WI\"                       \"Oklahoma OK\"                       \n[23] \"New York NY\"                        \"Iowa IA\"                           \n[25] \"Colorado CO\"                        \"Idaho ID\"                          \n[27] \"Alaska AK\"                          \"North Dakota ND\"                   \n[29] \"North Carolina NC\"                  \"New Hampshire NH\"                  \n[31] \"Vermont VT\"                         \"Texas TX\"                          \n[33] \"Nebraska NE\"                        \"Hawaii HI\"                         \n[35] \"Wyoming WY\"                         \"South Dakota SD\"                   \n[37] \"Louisiana LA\"                       \"Kentucky KY\"                       \n[39] \"Puerto Rico PR\"                     \"Arkansas AR\"                       \n[41] \"Maryland MD\"                        \"Ohio OH\"                           \n[43] \"South Carolina SC\"                  \"Massachusetts MA\"                  \n[45] \"Michigan MI\"                        \"Oregon OR\"                         \n[47] \"Montana MT\"                         \"Tennessee TN\"                      \n[49] \"Connecticut CT\"                     \"Mississippi MS\"                    \n[51] \"New Mexico NM\"                      \"Pennsylvania PA\"                   \n[53] \"New Jersey NJ\"                      \"Georgia GA\"                        \n\n\nTo understand the scope of the data set and check for things like inconsistencies, mismatched abbreviations, etc we look at all the unique values for LocationDesc and LocationAbbr. For simplicity sake I will remove the Abbreviated column in the future.\n\nunique(rawdata$YEAR) # seeing unique values of year\n\n [1] \"2017\"      \"2018\"      \"2016\"      \"2014\"      \"2012\"      \"2018-2019\"\n [7] \"2011\"      \"2013\"      \"2014-2015\" \"2017-2018\" \"2015\"      \"2019\"     \n[13] \"2016-2017\" \"2013-2014\" \"2015-2016\" \"2011-2012\" \"2012-2013\"\n\n\n\nrawdata %&gt;% filter(nchar(YEAR) &gt; 4) # confirming what the two-year values are\n\n# A tibble: 4,670 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2018-20… DE           Delaware     Tobacco … Smokeles… Current Us… BRFSS     \n 2 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n 3 2017-20… WA           Washington   Tobacco … Cigarett… Current Sm… BRFSS     \n 4 2018-20… DC           District of… Tobacco … Smokeles… Current Us… BRFSS     \n 5 2016-20… ID           Idaho        Tobacco … Cigarett… Current Sm… BRFSS     \n 6 2017-20… CO           Colorado     Tobacco … Smokeles… Current Us… BRFSS     \n 7 2018-20… TX           Texas        Tobacco … Cigarett… Current Sm… BRFSS     \n 8 2018-20… KS           Kansas       Tobacco … Cigarett… Current Sm… BRFSS     \n 9 2014-20… KS           Kansas       Tobacco … Smokeles… Current Us… BRFSS     \n10 2013-20… VT           Vermont      Tobacco … Cigarett… Current Sm… BRFSS     \n# ℹ 4,660 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\nAgain, understanding the scope of the data. Two things I’m noticing, first being that the year column is a character variable, and not a number. Second being that there are some two-year aggregations that are not necessary since we have every year between 2011 and 2019, these can likely be filtered out.\n\n# filtering down to one Locationa and a smaller year set, again to diagnose the two-year observations\nrawdata %&gt;% filter(LocationAbbr == 'IL', YEAR %in% c('2014-2015', '2014', '2015')) %&gt;% arrange(DisplayOrder)\n\n# A tibble: 152 × 31\n   YEAR  LocationAbbr LocationDesc TopicType    TopicDesc MeasureDesc DataSource\n   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 2 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 3 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 4 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 5 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 6 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 7 2014  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n 8 2015  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n 9 2015  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n10 2014  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n# ℹ 142 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\n\nrawdata %&gt;% filter(LocationAbbr == 'IL', YEAR %in% c('2014-2015', '2014', '2015')) %&gt;% \n  group_by(YEAR, Race, Gender, MeasureDesc) %&gt;% # groups picked to confirm aggregation hierarchy\n  summarize(\n    Samp = sum(Sample_Size, na.rm = TRUE) # sums sample size across the above groups\n  )\n\n`summarise()` has grouped output by 'YEAR', 'Race', 'Gender'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 78 × 5\n# Groups:   YEAR, Race, Gender [21]\n   YEAR  Race             Gender  MeasureDesc                               Samp\n   &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;                                    &lt;dbl&gt;\n 1 2014  African American Overall Current Smoking                            488\n 2 2014  African American Overall Current Use                                489\n 3 2014  All Races        Female  Current Smoking                           3459\n 4 2014  All Races        Female  Current Use                               3454\n 5 2014  All Races        Female  Frequency of Use                             0\n 6 2014  All Races        Female  Percent of Former Smokers Among Ever Sm…  1056\n 7 2014  All Races        Female  Quit Attempt in Past Year Among Every D…   225\n 8 2014  All Races        Female  Smoking Frequency                          686\n 9 2014  All Races        Female  Smoking Status                            8241\n10 2014  All Races        Female  User Status                               5486\n# ℹ 68 more rows\n\n\nThis was primarily to confirm that the two-year rows are in fact aggregations of the data we have in the single-year rows, which we can see that they are. Filtering to a specific location and timeframe also gives us an idea of data structure.\nNote: Values like Current Use and Current Smoking are intentionally different. ‘Use’ Corresponds to smokeless tobacco use, ‘Smoking’ refers to cigarrette/ non-electronic usage.\nThis is enough exploration to give me an idea of what I would want my final data set to look like after cleaning and processing.\nGoal Data Set Mapping:\n|Year|Location|Age|Race|Gender|Education|Value for Measure Desc combined with Values for Response, Surveyed then Freq\nThis would ultimately be a wider data set than we have now, and we will get there in steps.\n\nd1 &lt;- rawdata %&gt;% filter(nchar(YEAR) == 4) %&gt;% \n  mutate(\n  QA = paste(TopicDesc, MeasureDesc,Response), # concatenating all of the question-response related columns\n  Data_Value = as.numeric(Data_Value)/100, # transforming percentages into proper proportions\n  Year = as.numeric(YEAR) # Year variable was previously a string\n) %&gt;% select(Year, LocationDesc, Age, Race, Gender, Education, QA, Data_Value, Sample_Size) # reordering some but mostly getting rid of the original columns that were concatenated\n\nhead(d1)\n\n# A tibble: 6 × 9\n   Year LocationDesc   Age   Race  Gender Education QA    Data_Value Sample_Size\n  &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1  2017 Guam           All … All … Male   All Grad… Ciga…      0.3           692\n2  2018 National Medi… All … All … Overa… All Grad… Ciga…      0.161          NA\n3  2017 National Medi… All … All … Overa… All Grad… Ciga…      0.582          NA\n4  2016 Guam           All … Hisp… Overa… All Grad… Smok…      0.005         123\n5  2014 Guam           All … White Overa… All Grad… Ciga…      0.219         209\n6  2012 Indiana        All … All … Female All Grad… Ciga…      0.216        5165\n\n\nCombining Topic, Measures, and response effectively gives us a unique identifier for each answer choice for each question. Since the data set is already aggregated, we can use these combined question identifiers as variables and line them up with our categorical variables. This gets us one step closer to the rows as observations, or aggregations of observations in this case. Note that we essentially have two numeric values, the sample size and the observed proportion of interest. The sample size is not the same for each question or demographic, so we cant really solve for one without solving for both. To make this easier, I will split these up into two data sets then bring them back together at the end.\n\nd2 &lt;- d1 %&gt;% select(-Sample_Size) %&gt;% # removing sample size so the pivot over Data_Value works correctly\n  pivot_wider(names_from = QA, values_from = Data_Value) \n# transposes unique values of question-response identifier (QA) into columns, \n#essentially grouping by year, Location, and Demographics\n\nhead(d2)\n\n# A tibble: 6 × 25\n   Year LocationDesc         Age   Race  Gender Education Cigarette Use (Adult…¹\n  &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;dbl&gt;\n1  2017 Guam                 All … All … Male   All Grad…                  0.3  \n2  2018 National Median (St… All … All … Overa… All Grad…                 NA    \n3  2017 National Median (St… All … All … Overa… All Grad…                 NA    \n4  2016 Guam                 All … Hisp… Overa… All Grad…                  0.283\n5  2014 Guam                 All … White Overa… All Grad…                  0.219\n6  2012 Indiana              All … All … Female All Grad…                  0.216\n# ℹ abbreviated name: ¹​`Cigarette Use (Adults) Current Smoking NA`\n# ℹ 18 more variables: `Cigarette Use (Adults) Smoking Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Never` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) Current Use NA` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Frequency Some Days` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) User Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Former` &lt;dbl&gt;, …\n\n\n\nd3 &lt;- d1 %&gt;% select(-Data_Value) %&gt;% # removing Date_Value so the pivot over Sample_Size works correctly\n  pivot_wider(names_from = QA, values_from = Sample_Size)\n# transposes unique values of question-response identifier (QA) into columns, \n#essentially grouping by year, Location, and Demographics\n\nhead(d3)\n\n# A tibble: 6 × 25\n   Year LocationDesc         Age   Race  Gender Education Cigarette Use (Adult…¹\n  &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;dbl&gt;\n1  2017 Guam                 All … All … Male   All Grad…                    692\n2  2018 National Median (St… All … All … Overa… All Grad…                     NA\n3  2017 National Median (St… All … All … Overa… All Grad…                     NA\n4  2016 Guam                 All … Hisp… Overa… All Grad…                    123\n5  2014 Guam                 All … White Overa… All Grad…                    209\n6  2012 Indiana              All … All … Female All Grad…                   5165\n# ℹ abbreviated name: ¹​`Cigarette Use (Adults) Current Smoking NA`\n# ℹ 18 more variables: `Cigarette Use (Adults) Smoking Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Never` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) Current Use NA` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Frequency Some Days` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) User Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Former` &lt;dbl&gt;, …\n\n\n\nsum(d2[,1:6] != d3[,1:6]) # counts number of mismatches between categorical columns in the split dataset \n\n[1] 0\n\n\nThe pivot allows us to get each question-response identifier into a variable and line them up with categorical variables that are the same. Our d2 data frame is now our response frequency data set, and d3 is now our sample size data set. The last chunk is just checking that my categorical variables maintained the same structure after pivoting, which they should.\n\nd2 &lt;- d2 %&gt;%  select(c(names(d2[,1:6]),sort(names(d2[,7:25])))) %&gt;% \n  arrange(Year, LocationDesc, Age, Race)\n# Reorder the columns so like questions are next to each other, then reorders rows so like observations are next to each other\nd3 &lt;- d3 %&gt;%  select(c(names(d3[,1:6]),sort(names(d3[,7:25])))) %&gt;% \n  arrange(Year, LocationDesc, Age, Race)\n\nnames(d3)\n\n [1] \"Year\"                                                                             \n [2] \"LocationDesc\"                                                                     \n [3] \"Age\"                                                                              \n [4] \"Race\"                                                                             \n [5] \"Gender\"                                                                           \n [6] \"Education\"                                                                        \n [7] \"Cessation (Adults) Percent of Former Smokers Among Ever Smokers NA\"               \n [8] \"Cessation (Adults) Quit Attempt in Past Year Among Every Day Cigarette Smokers NA\"\n [9] \"Cigarette Use (Adults) Current Smoking NA\"                                        \n[10] \"Cigarette Use (Adults) Smoking Frequency Every Day\"                               \n[11] \"Cigarette Use (Adults) Smoking Frequency Some Days\"                               \n[12] \"Cigarette Use (Adults) Smoking Status Current\"                                    \n[13] \"Cigarette Use (Adults) Smoking Status Former\"                                     \n[14] \"Cigarette Use (Adults) Smoking Status Never\"                                      \n[15] \"E-Cigarette Use (Adults) Current Use NA\"                                          \n[16] \"E-Cigarette Use (Adults) Frequency of Use Every Day\"                              \n[17] \"E-Cigarette Use (Adults) Frequency of Use Some Days\"                              \n[18] \"E-Cigarette Use (Adults) User Status Current\"                                     \n[19] \"E-Cigarette Use (Adults) User Status Former\"                                      \n[20] \"E-Cigarette Use (Adults) User Status Never\"                                       \n[21] \"Smokeless Tobacco Use (Adults) Current Use NA\"                                    \n[22] \"Smokeless Tobacco Use (Adults) Frequency of Use Every Day\"                        \n[23] \"Smokeless Tobacco Use (Adults) Frequency of Use Some Days\"                        \n[24] \"Smokeless Tobacco Use (Adults) User Status Current\"                               \n[25] \"Smokeless Tobacco Use (Adults) User Status Not Current\"                           \n\n\nSince we lead off the question-response identifiers with the topic and Measure, we can get sort the variables alphabetically to get like questions next to each other. We do this for each data frame.\n\n# Manually renaming columns for conciseness\nnames(d3) &lt;- c(names(d3[,1:6]), \n               'QuitPctFrmr',\n               'QuitAttmpt',\n               \n               'CigCurrSmker',\n               'CigFreqDaily',\n               'CigFreqSome',\n               'CigStatCurr',\n               'CigStatFrmr',\n               'CigStatNvr',\n               \n               'EcigCurrUse',\n               'EcigFreqDaily',\n               'EcigFreqSome',\n               'EcigStatCurr',\n               'EcigStatFrmr',\n               'EcigStatNvr',\n               \n               'TobCurrUse',\n               'TobFreqDaily',\n               'TobFreqSome',\n               'TobStatCurr',\n               'TobStatNonCurr'\n               )\n\nredict &lt;- cbind(names(d3), names(d2)) # creates pseduo-dictionary\n\nnames(d2) &lt;- c(names(d3)) # copies new name convention from d3 to d2\n\n# concatenates the type of value identifier onto the columns, makes it easier to interpret\nnames(d2) &lt;- c(names(d2[,1:6]),paste0(names(d2[,7:25]), 'RespFreq')) \nnames(d3) &lt;- c(names(d3[,1:6]),paste0(names(d3[,7:25]), 'SrvCnt'))\n\nThere are shorter ways to make the column names usable, but I want column names that are also more concise. Here I have renamed each question-response identifier individually, to make them much easier to undrstand while I work with them. the redict part also creates a pseudo-data dictionary so I can remember what each renamed column corresponds to, if i forget. I rename the columns for d2 the lazy way, by copying the naming conventions from d3 since the structre is the same. Finally I add ‘RespFreq’ to the column names of my response frequency dataset, and ‘SrvCnt’ to the Survey Count sample size for my Sample Size dataset.\n\nsum(d2[,1:6] != d3[,1:6]) # one last check to make sure nothing got shuffled incorrectly\n\n[1] 0\n\nd4 &lt;- merge(d2,d3) # merging the two datasets into a master\n\nAgain, one last check to make sure everything is in the correct order, then merging the data sets into one master data set.\n\n# filter to narrow scope and see if transformations worked correctly and see what can be removed.\nd4 %&gt;% filter(Year == 2016, LocationDesc == 'Wyoming')\n\n   Year LocationDesc                Age                          Race  Gender\n1  2016      Wyoming     18 to 24 Years                     All Races Overall\n2  2016      Wyoming     18 to 44 Years                     All Races  Female\n3  2016      Wyoming     25 to 44 Years                     All Races Overall\n4  2016      Wyoming     45 to 64 Years                     All Races Overall\n5  2016      Wyoming 65 Years and Older                     All Races Overall\n6  2016      Wyoming   Age 20 and Older                     All Races Overall\n7  2016      Wyoming   Age 20 and Older                     All Races Overall\n8  2016      Wyoming   Age 20 and Older                     All Races Overall\n9  2016      Wyoming   Age 25 and Older                     All Races Overall\n10 2016      Wyoming   Age 25 and Older                     All Races Overall\n11 2016      Wyoming   Age 25 and Older                     All Races Overall\n12 2016      Wyoming           All Ages              African American Overall\n13 2016      Wyoming           All Ages                     All Races  Female\n14 2016      Wyoming           All Ages                     All Races    Male\n15 2016      Wyoming           All Ages                     All Races Overall\n16 2016      Wyoming           All Ages American Indian/Alaska Native Overall\n17 2016      Wyoming           All Ages        Asian/Pacific Islander Overall\n18 2016      Wyoming           All Ages                      Hispanic Overall\n19 2016      Wyoming           All Ages                         White Overall\n      Education QuitPctFrmrRespFreq QuitAttmptRespFreq CigCurrSmkerRespFreq\n1    All Grades                  NA                 NA                0.225\n2    All Grades                  NA                 NA                0.239\n3    All Grades                  NA                 NA                0.240\n4    All Grades                  NA                 NA                0.181\n5    All Grades                  NA                 NA                0.095\n6  &lt; 12th Grade                  NA                 NA                0.402\n7  &gt; 12th Grade                  NA                 NA                0.122\n8    12th Grade                  NA                 NA                0.267\n9  &lt; 12th Grade                  NA                 NA                0.386\n10 &gt; 12th Grade                  NA                 NA                0.122\n11   12th Grade                  NA                 NA                0.270\n12   All Grades                  NA                 NA                   NA\n13   All Grades               0.542              0.518                0.191\n14   All Grades               0.611              0.393                0.188\n15   All Grades               0.580              0.457                0.189\n16   All Grades                  NA                 NA                   NA\n17   All Grades                  NA                 NA                   NA\n18   All Grades                  NA                 NA                0.201\n19   All Grades                  NA                 NA                0.181\n   CigFreqDailyRespFreq CigFreqSomeRespFreq CigStatCurrRespFreq\n1                    NA                  NA                  NA\n2                    NA                  NA                  NA\n3                    NA                  NA                  NA\n4                    NA                  NA                  NA\n5                    NA                  NA                  NA\n6                    NA                  NA                  NA\n7                    NA                  NA                  NA\n8                    NA                  NA                  NA\n9                    NA                  NA                  NA\n10                   NA                  NA                  NA\n11                   NA                  NA                  NA\n12                   NA                  NA                  NA\n13                0.796               0.204               0.191\n14                0.745               0.255               0.188\n15                0.771               0.229               0.189\n16                   NA                  NA                  NA\n17                   NA                  NA                  NA\n18                   NA                  NA                  NA\n19                   NA                  NA                  NA\n   CigStatFrmrRespFreq CigStatNvrRespFreq EcigCurrUseRespFreq\n1                   NA                 NA               0.130\n2                   NA                 NA               0.075\n3                   NA                 NA               0.077\n4                   NA                 NA               0.033\n5                   NA                 NA               0.010\n6                   NA                 NA               0.100\n7                   NA                 NA               0.039\n8                   NA                 NA               0.066\n9                   NA                 NA               0.036\n10                  NA                 NA               0.042\n11                  NA                 NA               0.055\n12                  NA                 NA                  NA\n13               0.227              0.582               0.050\n14               0.295              0.518               0.061\n15               0.261              0.549               0.055\n16                  NA                 NA                  NA\n17                  NA                 NA                  NA\n18                  NA                 NA               0.033\n19                  NA                 NA               0.055\n   EcigFreqDailyRespFreq EcigFreqSomeRespFreq EcigStatCurrRespFreq\n1                     NA                   NA                   NA\n2                     NA                   NA                   NA\n3                     NA                   NA                   NA\n4                     NA                   NA                   NA\n5                     NA                   NA                   NA\n6                     NA                   NA                   NA\n7                     NA                   NA                   NA\n8                     NA                   NA                   NA\n9                     NA                   NA                   NA\n10                    NA                   NA                   NA\n11                    NA                   NA                   NA\n12                    NA                   NA                   NA\n13                 0.290                0.710                0.050\n14                 0.419                0.581                0.061\n15                 0.362                0.638                0.055\n16                    NA                   NA                   NA\n17                    NA                   NA                   NA\n18                    NA                   NA                   NA\n19                    NA                   NA                   NA\n   EcigStatFrmrRespFreq EcigStatNvrRespFreq TobCurrUseRespFreq\n1                    NA                  NA              0.175\n2                    NA                  NA              0.044\n3                    NA                  NA              0.139\n4                    NA                  NA              0.063\n5                    NA                  NA              0.038\n6                    NA                  NA              0.084\n7                    NA                  NA              0.079\n8                    NA                  NA              0.137\n9                    NA                  NA              0.086\n10                   NA                  NA              0.068\n11                   NA                  NA              0.127\n12                   NA                  NA                 NA\n13                0.157               0.793              0.023\n14                0.215               0.725              0.170\n15                0.186               0.758              0.098\n16                   NA                  NA                 NA\n17                   NA                  NA                 NA\n18                   NA                  NA              0.061\n19                   NA                  NA              0.097\n   TobFreqDailyRespFreq TobFreqSomeRespFreq TobStatCurrRespFreq\n1                    NA                  NA                  NA\n2                    NA                  NA                  NA\n3                    NA                  NA                  NA\n4                    NA                  NA                  NA\n5                    NA                  NA                  NA\n6                    NA                  NA                  NA\n7                    NA                  NA                  NA\n8                    NA                  NA                  NA\n9                    NA                  NA                  NA\n10                   NA                  NA                  NA\n11                   NA                  NA                  NA\n12                   NA                  NA                  NA\n13                   NA                  NA               0.023\n14                0.693               0.307               0.170\n15                0.672               0.328               0.098\n16                   NA                  NA                  NA\n17                   NA                  NA                  NA\n18                   NA                  NA                  NA\n19                   NA                  NA                  NA\n   TobStatNonCurrRespFreq QuitPctFrmrSrvCnt QuitAttmptSrvCnt CigCurrSmkerSrvCnt\n1                      NA                NA               NA                140\n2                      NA                NA               NA                469\n3                      NA                NA               NA                758\n4                      NA                NA               NA               1707\n5                      NA                NA               NA               1788\n6                      NA                NA               NA                211\n7                      NA                NA               NA               2896\n8                      NA                NA               NA               1231\n9                      NA                NA               NA                203\n10                     NA                NA               NA               2840\n11                     NA                NA               NA               1200\n12                     NA                NA               NA                 NA\n13                  0.977              1019              258               2492\n14                  0.830               951              217               1901\n15                  0.902              1970              475               4393\n16                     NA                NA               NA                 NA\n17                     NA                NA               NA                 NA\n18                     NA                NA               NA                213\n19                     NA                NA               NA               3961\n   CigFreqDailySrvCnt CigFreqSomeSrvCnt CigStatCurrSrvCnt CigStatFrmrSrvCnt\n1                  NA                NA                NA                NA\n2                  NA                NA                NA                NA\n3                  NA                NA                NA                NA\n4                  NA                NA                NA                NA\n5                  NA                NA                NA                NA\n6                  NA                NA                NA                NA\n7                  NA                NA                NA                NA\n8                  NA                NA                NA                NA\n9                  NA                NA                NA                NA\n10                 NA                NA                NA                NA\n11                 NA                NA                NA                NA\n12                 NA                NA                NA                NA\n13                339               339              2492              2492\n14                285               285              1901              1901\n15                624               624              4393              4393\n16                 NA                NA                NA                NA\n17                 NA                NA                NA                NA\n18                 NA                NA                NA                NA\n19                 NA                NA                NA                NA\n   CigStatNvrSrvCnt EcigCurrUseSrvCnt EcigFreqDailySrvCnt EcigFreqSomeSrvCnt\n1                NA               139                  NA                 NA\n2                NA               470                  NA                 NA\n3                NA               759                  NA                 NA\n4                NA              1713                  NA                 NA\n5                NA              1799                  NA                 NA\n6                NA               212                  NA                 NA\n7                NA              2902                  NA                 NA\n8                NA              1239                  NA                 NA\n9                NA               204                  NA                 NA\n10               NA              2847                  NA                 NA\n11               NA              1208                  NA                 NA\n12               NA                NA                  NA                 NA\n13             2492              2500                  76                 76\n14             1901              1910                  65                 65\n15             4393              4410                 141                141\n16               NA                NA                  NA                 NA\n17               NA                NA                  NA                 NA\n18               NA               213                  NA                 NA\n19               NA              3978                  NA                 NA\n   EcigStatCurrSrvCnt EcigStatFrmrSrvCnt EcigStatNvrSrvCnt TobCurrUseSrvCnt\n1                  NA                 NA                NA              140\n2                  NA                 NA                NA              470\n3                  NA                 NA                NA              756\n4                  NA                 NA                NA             1715\n5                  NA                 NA                NA             1799\n6                  NA                 NA                NA              213\n7                  NA                 NA                NA             2901\n8                  NA                 NA                NA             1239\n9                  NA                 NA                NA              205\n10                 NA                 NA                NA             2845\n11                 NA                 NA                NA             1208\n12                 NA                 NA                NA               NA\n13               2500               2500              2500             2503\n14               1910               1910              1910             1907\n15               4410               4410              4410             4410\n16                 NA                 NA                NA               NA\n17                 NA                 NA                NA               NA\n18                 NA                 NA                NA              214\n19                 NA                 NA                NA             3978\n   TobFreqDailySrvCnt TobFreqSomeSrvCnt TobStatCurrSrvCnt TobStatNonCurrSrvCnt\n1                  NA                NA                NA                   NA\n2                  NA                NA                NA                   NA\n3                  NA                NA                NA                   NA\n4                  NA                NA                NA                   NA\n5                  NA                NA                NA                   NA\n6                  NA                NA                NA                   NA\n7                  NA                NA                NA                   NA\n8                  NA                NA                NA                   NA\n9                  NA                NA                NA                   NA\n10                 NA                NA                NA                   NA\n11                 NA                NA                NA                   NA\n12                 NA                NA                NA                   NA\n13                 NA                NA              2503                 2503\n14                252               252              1907                 1907\n15                278               278              4410                 4410\n16                 NA                NA                NA                   NA\n17                 NA                NA                NA                   NA\n18                 NA                NA                NA                   NA\n19                 NA                NA                NA                   NA\n\n\nFiltering down to one Year and Location, so I can see how some of the aggregated values roll up and how they combine with other demographic variables. This gives me some insight into which questions were missing at what grains, and also allows me to check my merge to make sure it looks right. We finally have the data in a shape and layout that makes sense, and we see that its still kind of a mess. The intention was to maintain the separated demographic columns, and get each survey question into a column or columns. We accomplished that, however we can see that the demographics don’t stack in most cases. For example, I can see aggregated sample sizes for each Gender and the frequency for each question’s response, but I cannot see the age, education, or race makeup for each gender, or any combination of the demographic variables really. This was likely a deliberate choice for sample size constraints, nonetheless it complicates the next steps of this exercise. For a more in depth exercise, I would probably consider imputing the missing sample sizes or rates to get stratification by gender, age and race combined. However for the sake of this exercise, I will break each of the demographics into separate data sets since they have different questions considered. Once done, I will focus in on the Gender data set, since it looks like the gender demographic is the most complete across all questions.\nThe aggregated values by demographic aren’t necessary in the final version of any of these data sets, but we may need them to get the response volumes of some of the demographic values if there few responses. So, we can filter against all of the other demographic variables to only included their roll-up or overall value. First we will check for what those values are for each variable.\n\nunique(d4$Age) #'All Ages', also note some of the age bins are overlapping.\n\n[1] \"18 to 24 Years\"     \"18 to 44 Years\"     \"25 to 44 Years\"    \n[4] \"45 to 64 Years\"     \"65 Years and Older\" \"Age 20 and Older\"  \n[7] \"Age 25 and Older\"   \"All Ages\"          \n\nunique(d4$Gender) #'Overall'\n\n[1] \"Overall\" \"Female\"  \"Male\"   \n\nunique(d4$Race) #'All Races'\n\n[1] \"All Races\"                     \"African American\"             \n[3] \"American Indian/Alaska Native\" \"Asian/Pacific Islander\"       \n[5] \"Hispanic\"                      \"White\"                        \n\nunique(d4$Education) #'All Grades'\n\n[1] \"All Grades\"   \"&lt; 12th Grade\" \"&gt; 12th Grade\" \"12th Grade\"  \n\n\n\n# Age dataset, looking at rollup for all other variables except age, \n# then selecting relevant columns and filtering any last missing values\nAgeSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age != 'All Ages'\n              ,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\n# Gender Dataset. This time keeping the rollup value as well as the others,\n# but still filtering all other variables to their rollup\n# Also filtering out the national Median rows since its not the same grain as the others\n# finally removing last missing values. Will select relevant columns later.\nGndSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age == 'All Ages'\n              #,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  filter(LocationDesc != 'National Median (States and DC)',\n         !is.na(CigCurrSmkerRespFreq)\n         )\n\n# Race dataset, looking at rollup for all other variables except Race, \n# then selecting relevant columns and filtering any last missing values\nRaceSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age == 'All Ages'\n              ,Gender == 'Overall'\n              #,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\n# Education dataset, looking at rollup for all other variables except Education and Age,\n#since age actually is startified with education (interestingly, not vice-versa)\n# then selecting relevant columns and filtering any last missing values\nEdSrv &lt;- d4 %&gt;% filter(1==1\n              #,Age == 'All Ages'\n              ,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education != 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\nNow to focus in on the gender data set. I did not select specific columns earlier because the treatment was a bit different than the other data sets. This data set in particular has a lot more opportunity for imputing and other transformations, but for the simplicity of this exercise we will select only a few interesting and complete columns. Also one last note, the Gender variables has three values, Male, Female, and Overall. Since the Response frequency is the true variable of interest, I’m going to leave the “Overall” values in. Under other circumstances I would remove these and calculate overall values only when they apply, that way there aren’t “repeated” observations in the data set. However, the response frequency variables are pre-processed and rounded, so it would only unnecessarily complicate things to try and calculate response counts to get a proper aggregation.\n\n#Selecting relevant columns for the remainder of the exercise\nGndSrv &lt;- GndSrv %&gt;% \n  select(\n    Year\n    ,LocationDesc\n    ,Gender\n    ,QuitAttmptRespFreq\n    ,CigStatCurrRespFreq\n    ,CigStatFrmrRespFreq\n    ,CigStatNvrRespFreq\n    )\n\nI have decided to focus in on the questions for Cigarette usage, namely status and if they have attempted to quit in the last year. I also kept Year and Location since those variables were the least problematic, and of course Gender since this is the Gender-specific dataset.\n\nGndSrv %&gt;% group_by(Gender) %&gt;% #grouped by general to see potential differences in mean.\n  summarize( # getting mean values for each of my questions\n    AvgQuitAttempt = mean(QuitAttmptRespFreq)\n    ,AvgCurentSmokers = mean(CigStatCurrRespFreq)\n    ,AvgFormerSmokers = mean(CigStatFrmrRespFreq)\n    ,AvgNeverSmoked = mean(CigStatNvrRespFreq)\n  )\n\n# A tibble: 3 × 5\n  Gender  AvgQuitAttempt AvgCurentSmokers AvgFormerSmokers AvgNeverSmoked\n  &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1 Female           0.526            0.163            0.215          0.622\n2 Male             0.510            0.203            0.283          0.514\n3 Overall          0.518            0.183            0.248          0.569\n\n\nLet’s start by getting some quick descriptive statistics. Notice that this table will get the average Rate for each of the questions across all states and timeframes. The number of combinations we have between categorical variables, especially since the Location variable has so many categories, complicates our ability to get a closer look. Faceting and framing will help with this, as we will see in a moment.\n\nbox1o &lt;- GndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_boxplot() +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\nbox1o\n\n\n\n\n\n\n\nhist1o &lt;- GndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\nhist1o\n\n\n\n\n\n\n\n\nthe boxplots should show the distribution of the Percent of Current Smokers across the different locations. I have filter to 2011, the earliest year in the data set, and 2019, the latest year. Here I wanted to see if there would be a large difference from the beginning of the timeframe to the end, and for the most part we can see that there is. It appears as though we have a lower rate of “Current” Smokers from beginning to end. the distribution is not particularly clean, but it also looks to be shifting left while keeping some outliers on the higher end to make for a bit of a right skew.\n\nGndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=QuitAttmptRespFreq)) + # Quit Attempt Rate observed\n  geom_boxplot() +\n  labs(x = 'Percent of Quit Attempts\\nAmong Current Smokers') +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\n\n\n\n\n\n\n\nGndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=QuitAttmptRespFreq)) + # Quit Attempt Rate observed\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Quit Attempts\\nAmong Current Smokers', y = NULL) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\n\n\n\n\n\n\n\n\nGoing into the quit rate, it’s a bit of a different story here. There is maybe a little bit of a left shift in the Males and Overall, but it is far less pronounced than it was for the percent of current smokers. This makes sense, since it is the quit rate for current smokers, meaning we would expect it to either stay the same or potentially increase over time. This begs the question if the decrease in current smokers is coming from more people quitting or fewer people starting. The distribution is also all over the place, so there is a wider variance. This will be apparent in the next visual.\n\naniplot &lt;- GndSrv %&gt;% filter(Gender != 'Overall') %&gt;% # wanting Male and Female only\n  ggplot(aes(x=CigStatCurrRespFreq, y = QuitAttmptRespFreq)) + #plotting current smokers against quit attempt rate\n  geom_point(aes(frame = Year, color = Gender, ids = LocationDesc)) + #frame for animation, ids for hover text, color for comparison\n  labs(x = 'Percent of Current Smokers', y = 'Percent of Quit Attempts\\nAmong Current Smokers')\n\nWarning in geom_point(aes(frame = Year, color = Gender, ids = LocationDesc)):\nIgnoring unknown aesthetics: frame and ids\n\nggplotly(aniplot) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animaton, also slows down the slider when i click play\n\n\n\n\n\nThis was a pretty telling visual, so a few observations. First, there tend to be a higher rate of men who are current smokers over women. Second, the distribution is slowly but surely shifting left over time, it was not simply a fluke between 2011 and 2019 earlier. Finally, the wide variance in the percent of quit attempts is apparent, with the amount of vertical movement each point had in the visual. Still the distribution did not look to have any large shift up or down over time, which tells me the rate of people quitting is roughly the same, so more like the decrease in current smokers is happening because fewer people ever start smoking.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in later chunks\n  ggplot(aes(x = CigStatCurrRespFreq, y = fct_reorder(LocationDesc, CigStatCurrRespFreq), frame = Year)) + #fct_reorder orders location by Current smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of Current Smokers', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nMostly wanting to see if any outliers are more apparent here. Guam looks to have the highest rate of people who smoke, but they also have wide swings, likely due to a smaller sample size. Worth noting that the ranking of rates by state does not change significantly over time either.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in other chunks\n  ggplot(aes(x = CigStatFrmrRespFreq, y = fct_reorder(LocationDesc, CigStatFrmrRespFreq), frame = Year)) + #fct_reorder orders location by former smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of Former Smokers', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nThe rate of Former smokers does not look like it shares the trend. Again, if we saw an increase in the rates of people quitting we might expect the rate of Former Smokers to increase over time, but we don’t really see that much here.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in other chunks\n  ggplot(aes(x = CigStatNvrRespFreq, y = fct_reorder(LocationDesc, CigStatNvrRespFreq), frame = Year)) + #fct_reorder orders location by Never smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of People Who Have Never Smoked', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nHere we can see that increase in rate of people who have never smoked, though admittedly it still seems less pronounced than it was for the decrease in Current smokers. Still, it’s in line with the hypothesis that people are more likely are more likely to have never started smoking than they were in the past, and that is why we are seeing such a decrease in the rate of Current Smokers. Not to end on a darker note, but this is also likely a result of those who were Current Smokers in the earlier years dying off at a higher rate than non-smokers. This may suggest that younger people are much less likely to start smoking as well."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#exploring-the-original-data-set",
    "href": "cdcdata-exercise/cdcdata-exercise.html#exploring-the-original-data-set",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "The CDC data set I have chosen for this exercise is the Tobacco Usage Behavioral Risk Factor data set. The data comes from a survey administered by the CDC as part of the State Tobacco Activities Tracking and Evaluation (STATE) system. The data set contains information about the year the survey was administered, respondent location (State or territory), demographics (namely Race, Age, Education, and Gender), and it asks questions about a person’s tobacco usage status (current, former, never used), their cessation status (whether they have quit or attempted to quit in the last year), and the frequency that they use tobacco for those that do (daily, some days). The usage questions are asked for three different types of Tobacco products, Cigarettes, Smokeless Tobacco, and E-cigarettes.\n\npacman::p_load(here, tidyverse, skimr, plotly, synthpop, patchwork)\n\n\nrawdata &lt;- read_csv('cdc-data-raw.csv')\n\nRows: 43341 Columns: 31\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (25): YEAR, LocationAbbr, LocationDesc, TopicType, TopicDesc, MeasureDes...\ndbl  (6): Data_Value, Data_Value_Std_Err, Low_Confidence_Limit, High_Confide...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nstr(rawdata) # getting an idea for data structure\n\nspc_tbl_ [43,341 × 31] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ YEAR                      : chr [1:43341] \"2017\" \"2018\" \"2017\" \"2016\" ...\n $ LocationAbbr              : chr [1:43341] \"GU\" \"US\" \"US\" \"GU\" ...\n $ LocationDesc              : chr [1:43341] \"Guam\" \"National Median (States and DC)\" \"National Median (States and DC)\" \"Guam\" ...\n $ TopicType                 : chr [1:43341] \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" ...\n $ TopicDesc                 : chr [1:43341] \"Cigarette Use (Adults)\" \"Cigarette Use (Adults)\" \"Cigarette Use (Adults)\" \"Smokeless Tobacco Use (Adults)\" ...\n $ MeasureDesc               : chr [1:43341] \"Current Smoking\" \"Smoking Status\" \"Smoking Status\" \"Current Use\" ...\n $ DataSource                : chr [1:43341] \"BRFSS\" \"BRFSS\" \"BRFSS\" \"BRFSS\" ...\n $ Response                  : chr [1:43341] NA \"Current\" \"Never\" NA ...\n $ Data_Value_Unit           : chr [1:43341] \"%\" \"%\" \"%\" \"%\" ...\n $ Data_Value_Type           : chr [1:43341] \"Percentage\" \"Percentage\" \"Percentage\" \"Percentage\" ...\n $ Data_Value                : num [1:43341] 30 16.1 58.2 0.5 21.9 21.6 2.8 3.6 29.2 14.8 ...\n $ Data_Value_Footnote_Symbol: chr [1:43341] NA NA NA NA ...\n $ Data_Value_Footnote       : chr [1:43341] NA NA NA NA ...\n $ Data_Value_Std_Err        : num [1:43341] 2.3 NA NA 0.4 4.9 0.8 0.3 0.4 2.4 0.6 ...\n $ Low_Confidence_Limit      : num [1:43341] 25.4 NA NA 0 12.4 20 2.2 2.7 24.5 13.6 ...\n $ High_Confidence_Limit     : num [1:43341] 34.6 NA NA 1.2 31.4 23.2 3.4 4.5 33.9 16 ...\n $ Sample_Size               : num [1:43341] 692 NA NA 123 209 ...\n $ Gender                    : chr [1:43341] \"Male\" \"Overall\" \"Overall\" \"Overall\" ...\n $ Race                      : chr [1:43341] \"All Races\" \"All Races\" \"All Races\" \"Hispanic\" ...\n $ Age                       : chr [1:43341] \"All Ages\" \"All Ages\" \"All Ages\" \"All Ages\" ...\n $ Education                 : chr [1:43341] \"All Grades\" \"All Grades\" \"All Grades\" \"All Grades\" ...\n $ GeoLocation               : chr [1:43341] \"(13.444304, 144.793731)\" NA NA \"(13.444304, 144.793731)\" ...\n $ TopicTypeId               : chr [1:43341] \"BEH\" \"BEH\" \"BEH\" \"BEH\" ...\n $ TopicId                   : chr [1:43341] \"100BEH\" \"100BEH\" \"100BEH\" \"150BEH\" ...\n $ MeasureId                 : chr [1:43341] \"110CSA\" \"165SSA\" \"165SSA\" \"177SCU\" ...\n $ StratificationID1         : chr [1:43341] \"2GEN\" \"1GEN\" \"1GEN\" \"1GEN\" ...\n $ StratificationID2         : chr [1:43341] \"8AGE\" \"8AGE\" \"8AGE\" \"8AGE\" ...\n $ StratificationID3         : chr [1:43341] \"6RAC\" \"6RAC\" \"6RAC\" \"4RAC\" ...\n $ StratificationID4         : chr [1:43341] \"6EDU\" \"6EDU\" \"6EDU\" \"6EDU\" ...\n $ SubMeasureID              : chr [1:43341] \"BRF21\" \"BRF27\" \"BRF28\" \"BRF69\" ...\n $ DisplayOrder              : num [1:43341] 21 27 28 69 22 21 77 71 26 27 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   YEAR = col_character(),\n  ..   LocationAbbr = col_character(),\n  ..   LocationDesc = col_character(),\n  ..   TopicType = col_character(),\n  ..   TopicDesc = col_character(),\n  ..   MeasureDesc = col_character(),\n  ..   DataSource = col_character(),\n  ..   Response = col_character(),\n  ..   Data_Value_Unit = col_character(),\n  ..   Data_Value_Type = col_character(),\n  ..   Data_Value = col_double(),\n  ..   Data_Value_Footnote_Symbol = col_character(),\n  ..   Data_Value_Footnote = col_character(),\n  ..   Data_Value_Std_Err = col_double(),\n  ..   Low_Confidence_Limit = col_double(),\n  ..   High_Confidence_Limit = col_double(),\n  ..   Sample_Size = col_double(),\n  ..   Gender = col_character(),\n  ..   Race = col_character(),\n  ..   Age = col_character(),\n  ..   Education = col_character(),\n  ..   GeoLocation = col_character(),\n  ..   TopicTypeId = col_character(),\n  ..   TopicId = col_character(),\n  ..   MeasureId = col_character(),\n  ..   StratificationID1 = col_character(),\n  ..   StratificationID2 = col_character(),\n  ..   StratificationID3 = col_character(),\n  ..   StratificationID4 = col_character(),\n  ..   SubMeasureID = col_character(),\n  ..   DisplayOrder = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(rawdata) # summary statistics\n\n     YEAR           LocationAbbr       LocationDesc        TopicType        \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  TopicDesc         MeasureDesc         DataSource          Response        \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n Data_Value_Unit    Data_Value_Type      Data_Value   \n Length:43341       Length:43341       Min.   : 0.00  \n Class :character   Class :character   1st Qu.: 5.10  \n Mode  :character   Mode  :character   Median :17.10  \n                                       Mean   :25.36  \n                                       3rd Qu.:38.90  \n                                       Max.   :99.90  \n                                       NA's   :2117   \n Data_Value_Footnote_Symbol Data_Value_Footnote Data_Value_Std_Err\n Length:43341               Length:43341        Min.   : 0.000    \n Class :character           Class :character    1st Qu.: 0.600    \n Mode  :character           Mode  :character    Median : 1.100    \n                                                Mean   : 1.711    \n                                                3rd Qu.: 2.200    \n                                                Max.   :16.600    \n                                                NA's   :2195      \n Low_Confidence_Limit High_Confidence_Limit  Sample_Size       Gender         \n Min.   : 0.00        Min.   :  0.00        Min.   :   50   Length:43341      \n 1st Qu.: 3.20        1st Qu.:  6.90        1st Qu.:  480   Class :character  \n Median :14.30        Median : 19.80        Median : 1798   Mode  :character  \n Mean   :22.01        Mean   : 28.68        Mean   : 3050                     \n 3rd Qu.:30.50        3rd Qu.: 47.40        3rd Qu.: 4203                     \n Max.   :99.80        Max.   :100.00        Max.   :40726                     \n NA's   :2195         NA's   :2195          NA's   :2195                      \n     Race               Age             Education         GeoLocation       \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n TopicTypeId          TopicId           MeasureId         StratificationID1 \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n StratificationID2  StratificationID3  StratificationID4  SubMeasureID      \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  DisplayOrder  \n Min.   : 5.00  \n 1st Qu.:24.00  \n Median :51.00  \n Mean   :47.08  \n 3rd Qu.:71.00  \n Max.   :81.00  \n                \n\nskim(rawdata) # primarily to get completion rates\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n43341\n\n\nNumber of columns\n31\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n25\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nYEAR\n0\n1.00\n4\n9\n0\n17\n0\n\n\nLocationAbbr\n0\n1.00\n2\n2\n0\n54\n0\n\n\nLocationDesc\n0\n1.00\n4\n31\n0\n54\n0\n\n\nTopicType\n0\n1.00\n25\n25\n0\n1\n0\n\n\nTopicDesc\n0\n1.00\n18\n30\n0\n4\n0\n\n\nMeasureDesc\n0\n1.00\n11\n59\n0\n10\n0\n\n\nDataSource\n0\n1.00\n5\n5\n0\n1\n0\n\n\nResponse\n28323\n0.35\n5\n11\n0\n6\n0\n\n\nData_Value_Unit\n0\n1.00\n1\n1\n0\n1\n0\n\n\nData_Value_Type\n0\n1.00\n10\n10\n0\n1\n0\n\n\nData_Value_Footnote_Symbol\n41224\n0.05\n1\n1\n0\n1\n0\n\n\nData_Value_Footnote\n41224\n0.05\n71\n71\n0\n1\n0\n\n\nGender\n0\n1.00\n4\n7\n0\n3\n0\n\n\nRace\n0\n1.00\n5\n29\n0\n6\n0\n\n\nAge\n0\n1.00\n8\n18\n0\n8\n0\n\n\nEducation\n0\n1.00\n10\n12\n0\n4\n0\n\n\nGeoLocation\n78\n1.00\n23\n41\n0\n53\n0\n\n\nTopicTypeId\n0\n1.00\n3\n3\n0\n1\n0\n\n\nTopicId\n0\n1.00\n6\n6\n0\n4\n0\n\n\nMeasureId\n0\n1.00\n6\n6\n0\n14\n0\n\n\nStratificationID1\n0\n1.00\n4\n4\n0\n3\n0\n\n\nStratificationID2\n0\n1.00\n4\n4\n0\n8\n0\n\n\nStratificationID3\n0\n1.00\n4\n4\n0\n6\n0\n\n\nStratificationID4\n0\n1.00\n4\n4\n0\n4\n0\n\n\nSubMeasureID\n0\n1.00\n5\n5\n0\n53\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nData_Value\n2117\n0.95\n25.36\n25.00\n0\n5.1\n17.1\n38.9\n99.9\n▇▃▂▁▁\n\n\nData_Value_Std_Err\n2195\n0.95\n1.71\n1.72\n0\n0.6\n1.1\n2.2\n16.6\n▇▁▁▁▁\n\n\nLow_Confidence_Limit\n2195\n0.95\n22.01\n24.14\n0\n3.2\n14.3\n30.5\n99.8\n▇▂▂▁▁\n\n\nHigh_Confidence_Limit\n2195\n0.95\n28.68\n26.21\n0\n6.9\n19.8\n47.4\n100.0\n▇▃▂▂▁\n\n\nSample_Size\n2195\n0.95\n3050.17\n3798.87\n50\n480.0\n1798.0\n4203.0\n40726.0\n▇▁▁▁▁\n\n\nDisplayOrder\n0\n1.00\n47.08\n24.49\n5\n24.0\n51.0\n71.0\n81.0\n▂▇▁▂▇\n\n\n\n\n\n\nhead(rawdata, 20) # getting first 20 rows\n\n# A tibble: 20 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2017     GU           Guam         Tobacco … Cigarett… Current Sm… BRFSS     \n 2 2018     US           National Me… Tobacco … Cigarett… Smoking St… BRFSS     \n 3 2017     US           National Me… Tobacco … Cigarett… Smoking St… BRFSS     \n 4 2016     GU           Guam         Tobacco … Smokeles… Current Use BRFSS     \n 5 2014     GU           Guam         Tobacco … Cigarett… Current Sm… BRFSS     \n 6 2012     IN           Indiana      Tobacco … Cigarett… Current Sm… BRFSS     \n 7 2018-20… DE           Delaware     Tobacco … Smokeles… Current Us… BRFSS     \n 8 2011     MO           Missouri     Tobacco … Smokeles… Current Use BRFSS     \n 9 2013     ME           Maine        Tobacco … Cigarett… Smoking Fr… BRFSS     \n10 2017     WA           Washington   Tobacco … Cigarett… Smoking St… BRFSS     \n11 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n12 2011     MN           Minnesota    Tobacco … Smokeles… User Status BRFSS     \n13 2018     VA           Virginia     Tobacco … Cigarett… Smoking Fr… BRFSS     \n14 2014     MN           Minnesota    Tobacco … Cigarett… Smoking St… BRFSS     \n15 2017-20… WA           Washington   Tobacco … Cigarett… Current Sm… BRFSS     \n16 2017     UT           Utah         Tobacco … Smokeles… Current Use BRFSS     \n17 2012     CA           California   Tobacco … Smokeles… Frequency … BRFSS     \n18 2015     FL           Florida      Tobacco … Smokeles… Current Use BRFSS     \n19 2015     RI           Rhode Island Tobacco … Smokeles… Current Use BRFSS     \n20 2019     AZ           Arizona      Tobacco … Smokeles… Current Use BRFSS     \n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, StratificationID1 &lt;chr&gt;,\n#   StratificationID2 &lt;chr&gt;, StratificationID3 &lt;chr&gt;, …\n\n\nThe data set is not the most analyst friendly format. Generally the ideal format would be variables in the columns, observations in the rows. This data set has variables across several rows, and it is mostly aggregated. The questions are broken up by the different response options, which are spread across rows even though the values are proportions and therefore the samples sizes listed for the different response options for a single question come from the same audience. Aggregations grouped by each of the demographics, locations, and timeframes are also in the rows, similar to a pivot table, so getting things like summary statistics will require filtering these aggregated values. Some survey question variables have missing values, but with the data in this format it’s difficult to make much of them, but we will come back to that. There are a few variables that only serve to give information about the data set itself, like Data_Value_Unit, which only contains one value, “Percentage”, to inform that the Value column is a percentage. Variable like this (Zero or Near Zero Variance) can be removed, since they give no information about the individual observations.\nVariables to be removed: Zero Variance variables: Data_Value_Unit, Data_Value_Type, DataSource, TopicType, TopicTypeId NZV: Data_Value_Footnote_Symbol, Data_Value_Footnote\n\nrawdata %&gt;% filter(is.na(Data_Value)) #looking at nulls to determine why they are there\n\n# A tibble: 2,117 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n 2 2016     IA           Iowa         Tobacco … Smokeles… Frequency … BRFSS     \n 3 2014     ID           Idaho        Tobacco … Smokeles… Frequency … BRFSS     \n 4 2019     MI           Michigan     Tobacco … Smokeles… Frequency … BRFSS     \n 5 2016-20… MA           Massachuset… Tobacco … Smokeles… Current Us… BRFSS     \n 6 2013-20… DC           District of… Tobacco … Cigarett… Current Sm… BRFSS     \n 7 2013     ME           Maine        Tobacco … Cigarett… Current Sm… BRFSS     \n 8 2011     AZ           Arizona      Tobacco … Smokeles… Frequency … BRFSS     \n 9 2018     AL           Alabama      Tobacco … Smokeles… Frequency … BRFSS     \n10 2018     NC           North Carol… Tobacco … Smokeles… Frequency … BRFSS     \n# ℹ 2,107 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\nMissing values from the Data_Value column occur when the sample size is not large enough to report results. Some of these missing values may have the opportunity for imputation, and filtering these could cause some confusion in the actual structure of the data (complete and consistent combinations of categorical values) so we won’t filter anything out just yet. note this is evidenced by the Footnote column with the following message: “Data in these cells have been suppressed because of a small sample size.”\n\nunique(paste(rawdata$LocationDesc,rawdata$LocationAbbr)) # paste concatenates columns, unique to get distinct values.\n\n [1] \"Guam GU\"                            \"National Median (States and DC) US\"\n [3] \"Indiana IN\"                         \"Delaware DE\"                       \n [5] \"Missouri MO\"                        \"Maine ME\"                          \n [7] \"Washington WA\"                      \"Illinois IL\"                       \n [9] \"Minnesota MN\"                       \"Virginia VA\"                       \n[11] \"Utah UT\"                            \"California CA\"                     \n[13] \"Florida FL\"                         \"Rhode Island RI\"                   \n[15] \"Arizona AZ\"                         \"District of Columbia DC\"           \n[17] \"Kansas KS\"                          \"Nevada NV\"                         \n[19] \"Alabama AL\"                         \"West Virginia WV\"                  \n[21] \"Wisconsin WI\"                       \"Oklahoma OK\"                       \n[23] \"New York NY\"                        \"Iowa IA\"                           \n[25] \"Colorado CO\"                        \"Idaho ID\"                          \n[27] \"Alaska AK\"                          \"North Dakota ND\"                   \n[29] \"North Carolina NC\"                  \"New Hampshire NH\"                  \n[31] \"Vermont VT\"                         \"Texas TX\"                          \n[33] \"Nebraska NE\"                        \"Hawaii HI\"                         \n[35] \"Wyoming WY\"                         \"South Dakota SD\"                   \n[37] \"Louisiana LA\"                       \"Kentucky KY\"                       \n[39] \"Puerto Rico PR\"                     \"Arkansas AR\"                       \n[41] \"Maryland MD\"                        \"Ohio OH\"                           \n[43] \"South Carolina SC\"                  \"Massachusetts MA\"                  \n[45] \"Michigan MI\"                        \"Oregon OR\"                         \n[47] \"Montana MT\"                         \"Tennessee TN\"                      \n[49] \"Connecticut CT\"                     \"Mississippi MS\"                    \n[51] \"New Mexico NM\"                      \"Pennsylvania PA\"                   \n[53] \"New Jersey NJ\"                      \"Georgia GA\"                        \n\n\nTo understand the scope of the data set and check for things like inconsistencies, mismatched abbreviations, etc we look at all the unique values for LocationDesc and LocationAbbr. For simplicity sake I will remove the Abbreviated column in the future.\n\nunique(rawdata$YEAR) # seeing unique values of year\n\n [1] \"2017\"      \"2018\"      \"2016\"      \"2014\"      \"2012\"      \"2018-2019\"\n [7] \"2011\"      \"2013\"      \"2014-2015\" \"2017-2018\" \"2015\"      \"2019\"     \n[13] \"2016-2017\" \"2013-2014\" \"2015-2016\" \"2011-2012\" \"2012-2013\"\n\n\n\nrawdata %&gt;% filter(nchar(YEAR) &gt; 4) # confirming what the two-year values are\n\n# A tibble: 4,670 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2018-20… DE           Delaware     Tobacco … Smokeles… Current Us… BRFSS     \n 2 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n 3 2017-20… WA           Washington   Tobacco … Cigarett… Current Sm… BRFSS     \n 4 2018-20… DC           District of… Tobacco … Smokeles… Current Us… BRFSS     \n 5 2016-20… ID           Idaho        Tobacco … Cigarett… Current Sm… BRFSS     \n 6 2017-20… CO           Colorado     Tobacco … Smokeles… Current Us… BRFSS     \n 7 2018-20… TX           Texas        Tobacco … Cigarett… Current Sm… BRFSS     \n 8 2018-20… KS           Kansas       Tobacco … Cigarett… Current Sm… BRFSS     \n 9 2014-20… KS           Kansas       Tobacco … Smokeles… Current Us… BRFSS     \n10 2013-20… VT           Vermont      Tobacco … Cigarett… Current Sm… BRFSS     \n# ℹ 4,660 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\nAgain, understanding the scope of the data. Two things I’m noticing, first being that the year column is a character variable, and not a number. Second being that there are some two-year aggregations that are not necessary since we have every year between 2011 and 2019, these can likely be filtered out.\n\n# filtering down to one Locationa and a smaller year set, again to diagnose the two-year observations\nrawdata %&gt;% filter(LocationAbbr == 'IL', YEAR %in% c('2014-2015', '2014', '2015')) %&gt;% arrange(DisplayOrder)\n\n# A tibble: 152 × 31\n   YEAR  LocationAbbr LocationDesc TopicType    TopicDesc MeasureDesc DataSource\n   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 2 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 3 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 4 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 5 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 6 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 7 2014  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n 8 2015  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n 9 2015  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n10 2014  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n# ℹ 142 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\n\nrawdata %&gt;% filter(LocationAbbr == 'IL', YEAR %in% c('2014-2015', '2014', '2015')) %&gt;% \n  group_by(YEAR, Race, Gender, MeasureDesc) %&gt;% # groups picked to confirm aggregation hierarchy\n  summarize(\n    Samp = sum(Sample_Size, na.rm = TRUE) # sums sample size across the above groups\n  )\n\n`summarise()` has grouped output by 'YEAR', 'Race', 'Gender'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 78 × 5\n# Groups:   YEAR, Race, Gender [21]\n   YEAR  Race             Gender  MeasureDesc                               Samp\n   &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;                                    &lt;dbl&gt;\n 1 2014  African American Overall Current Smoking                            488\n 2 2014  African American Overall Current Use                                489\n 3 2014  All Races        Female  Current Smoking                           3459\n 4 2014  All Races        Female  Current Use                               3454\n 5 2014  All Races        Female  Frequency of Use                             0\n 6 2014  All Races        Female  Percent of Former Smokers Among Ever Sm…  1056\n 7 2014  All Races        Female  Quit Attempt in Past Year Among Every D…   225\n 8 2014  All Races        Female  Smoking Frequency                          686\n 9 2014  All Races        Female  Smoking Status                            8241\n10 2014  All Races        Female  User Status                               5486\n# ℹ 68 more rows\n\n\nThis was primarily to confirm that the two-year rows are in fact aggregations of the data we have in the single-year rows, which we can see that they are. Filtering to a specific location and timeframe also gives us an idea of data structure.\nNote: Values like Current Use and Current Smoking are intentionally different. ‘Use’ Corresponds to smokeless tobacco use, ‘Smoking’ refers to cigarrette/ non-electronic usage.\nThis is enough exploration to give me an idea of what I would want my final data set to look like after cleaning and processing.\nGoal Data Set Mapping:\n|Year|Location|Age|Race|Gender|Education|Value for Measure Desc combined with Values for Response, Surveyed then Freq\nThis would ultimately be a wider data set than we have now, and we will get there in steps.\n\nd1 &lt;- rawdata %&gt;% filter(nchar(YEAR) == 4) %&gt;% \n  mutate(\n  QA = paste(TopicDesc, MeasureDesc,Response), # concatenating all of the question-response related columns\n  Data_Value = as.numeric(Data_Value)/100, # transforming percentages into proper proportions\n  Year = as.numeric(YEAR) # Year variable was previously a string\n) %&gt;% select(Year, LocationDesc, Age, Race, Gender, Education, QA, Data_Value, Sample_Size) # reordering some but mostly getting rid of the original columns that were concatenated\n\nhead(d1)\n\n# A tibble: 6 × 9\n   Year LocationDesc   Age   Race  Gender Education QA    Data_Value Sample_Size\n  &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1  2017 Guam           All … All … Male   All Grad… Ciga…      0.3           692\n2  2018 National Medi… All … All … Overa… All Grad… Ciga…      0.161          NA\n3  2017 National Medi… All … All … Overa… All Grad… Ciga…      0.582          NA\n4  2016 Guam           All … Hisp… Overa… All Grad… Smok…      0.005         123\n5  2014 Guam           All … White Overa… All Grad… Ciga…      0.219         209\n6  2012 Indiana        All … All … Female All Grad… Ciga…      0.216        5165\n\n\nCombining Topic, Measures, and response effectively gives us a unique identifier for each answer choice for each question. Since the data set is already aggregated, we can use these combined question identifiers as variables and line them up with our categorical variables. This gets us one step closer to the rows as observations, or aggregations of observations in this case. Note that we essentially have two numeric values, the sample size and the observed proportion of interest. The sample size is not the same for each question or demographic, so we cant really solve for one without solving for both. To make this easier, I will split these up into two data sets then bring them back together at the end.\n\nd2 &lt;- d1 %&gt;% select(-Sample_Size) %&gt;% # removing sample size so the pivot over Data_Value works correctly\n  pivot_wider(names_from = QA, values_from = Data_Value) \n# transposes unique values of question-response identifier (QA) into columns, \n#essentially grouping by year, Location, and Demographics\n\nhead(d2)\n\n# A tibble: 6 × 25\n   Year LocationDesc         Age   Race  Gender Education Cigarette Use (Adult…¹\n  &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;dbl&gt;\n1  2017 Guam                 All … All … Male   All Grad…                  0.3  \n2  2018 National Median (St… All … All … Overa… All Grad…                 NA    \n3  2017 National Median (St… All … All … Overa… All Grad…                 NA    \n4  2016 Guam                 All … Hisp… Overa… All Grad…                  0.283\n5  2014 Guam                 All … White Overa… All Grad…                  0.219\n6  2012 Indiana              All … All … Female All Grad…                  0.216\n# ℹ abbreviated name: ¹​`Cigarette Use (Adults) Current Smoking NA`\n# ℹ 18 more variables: `Cigarette Use (Adults) Smoking Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Never` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) Current Use NA` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Frequency Some Days` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) User Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Former` &lt;dbl&gt;, …\n\n\n\nd3 &lt;- d1 %&gt;% select(-Data_Value) %&gt;% # removing Date_Value so the pivot over Sample_Size works correctly\n  pivot_wider(names_from = QA, values_from = Sample_Size)\n# transposes unique values of question-response identifier (QA) into columns, \n#essentially grouping by year, Location, and Demographics\n\nhead(d3)\n\n# A tibble: 6 × 25\n   Year LocationDesc         Age   Race  Gender Education Cigarette Use (Adult…¹\n  &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;dbl&gt;\n1  2017 Guam                 All … All … Male   All Grad…                    692\n2  2018 National Median (St… All … All … Overa… All Grad…                     NA\n3  2017 National Median (St… All … All … Overa… All Grad…                     NA\n4  2016 Guam                 All … Hisp… Overa… All Grad…                    123\n5  2014 Guam                 All … White Overa… All Grad…                    209\n6  2012 Indiana              All … All … Female All Grad…                   5165\n# ℹ abbreviated name: ¹​`Cigarette Use (Adults) Current Smoking NA`\n# ℹ 18 more variables: `Cigarette Use (Adults) Smoking Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Never` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) Current Use NA` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Frequency Some Days` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) User Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Former` &lt;dbl&gt;, …\n\n\n\nsum(d2[,1:6] != d3[,1:6]) # counts number of mismatches between categorical columns in the split dataset \n\n[1] 0\n\n\nThe pivot allows us to get each question-response identifier into a variable and line them up with categorical variables that are the same. Our d2 data frame is now our response frequency data set, and d3 is now our sample size data set. The last chunk is just checking that my categorical variables maintained the same structure after pivoting, which they should.\n\nd2 &lt;- d2 %&gt;%  select(c(names(d2[,1:6]),sort(names(d2[,7:25])))) %&gt;% \n  arrange(Year, LocationDesc, Age, Race)\n# Reorder the columns so like questions are next to each other, then reorders rows so like observations are next to each other\nd3 &lt;- d3 %&gt;%  select(c(names(d3[,1:6]),sort(names(d3[,7:25])))) %&gt;% \n  arrange(Year, LocationDesc, Age, Race)\n\nnames(d3)\n\n [1] \"Year\"                                                                             \n [2] \"LocationDesc\"                                                                     \n [3] \"Age\"                                                                              \n [4] \"Race\"                                                                             \n [5] \"Gender\"                                                                           \n [6] \"Education\"                                                                        \n [7] \"Cessation (Adults) Percent of Former Smokers Among Ever Smokers NA\"               \n [8] \"Cessation (Adults) Quit Attempt in Past Year Among Every Day Cigarette Smokers NA\"\n [9] \"Cigarette Use (Adults) Current Smoking NA\"                                        \n[10] \"Cigarette Use (Adults) Smoking Frequency Every Day\"                               \n[11] \"Cigarette Use (Adults) Smoking Frequency Some Days\"                               \n[12] \"Cigarette Use (Adults) Smoking Status Current\"                                    \n[13] \"Cigarette Use (Adults) Smoking Status Former\"                                     \n[14] \"Cigarette Use (Adults) Smoking Status Never\"                                      \n[15] \"E-Cigarette Use (Adults) Current Use NA\"                                          \n[16] \"E-Cigarette Use (Adults) Frequency of Use Every Day\"                              \n[17] \"E-Cigarette Use (Adults) Frequency of Use Some Days\"                              \n[18] \"E-Cigarette Use (Adults) User Status Current\"                                     \n[19] \"E-Cigarette Use (Adults) User Status Former\"                                      \n[20] \"E-Cigarette Use (Adults) User Status Never\"                                       \n[21] \"Smokeless Tobacco Use (Adults) Current Use NA\"                                    \n[22] \"Smokeless Tobacco Use (Adults) Frequency of Use Every Day\"                        \n[23] \"Smokeless Tobacco Use (Adults) Frequency of Use Some Days\"                        \n[24] \"Smokeless Tobacco Use (Adults) User Status Current\"                               \n[25] \"Smokeless Tobacco Use (Adults) User Status Not Current\"                           \n\n\nSince we lead off the question-response identifiers with the topic and Measure, we can get sort the variables alphabetically to get like questions next to each other. We do this for each data frame.\n\n# Manually renaming columns for conciseness\nnames(d3) &lt;- c(names(d3[,1:6]), \n               'QuitPctFrmr',\n               'QuitAttmpt',\n               \n               'CigCurrSmker',\n               'CigFreqDaily',\n               'CigFreqSome',\n               'CigStatCurr',\n               'CigStatFrmr',\n               'CigStatNvr',\n               \n               'EcigCurrUse',\n               'EcigFreqDaily',\n               'EcigFreqSome',\n               'EcigStatCurr',\n               'EcigStatFrmr',\n               'EcigStatNvr',\n               \n               'TobCurrUse',\n               'TobFreqDaily',\n               'TobFreqSome',\n               'TobStatCurr',\n               'TobStatNonCurr'\n               )\n\nredict &lt;- cbind(names(d3), names(d2)) # creates pseduo-dictionary\n\nnames(d2) &lt;- c(names(d3)) # copies new name convention from d3 to d2\n\n# concatenates the type of value identifier onto the columns, makes it easier to interpret\nnames(d2) &lt;- c(names(d2[,1:6]),paste0(names(d2[,7:25]), 'RespFreq')) \nnames(d3) &lt;- c(names(d3[,1:6]),paste0(names(d3[,7:25]), 'SrvCnt'))\n\nThere are shorter ways to make the column names usable, but I want column names that are also more concise. Here I have renamed each question-response identifier individually, to make them much easier to undrstand while I work with them. the redict part also creates a pseudo-data dictionary so I can remember what each renamed column corresponds to, if i forget. I rename the columns for d2 the lazy way, by copying the naming conventions from d3 since the structre is the same. Finally I add ‘RespFreq’ to the column names of my response frequency dataset, and ‘SrvCnt’ to the Survey Count sample size for my Sample Size dataset.\n\nsum(d2[,1:6] != d3[,1:6]) # one last check to make sure nothing got shuffled incorrectly\n\n[1] 0\n\nd4 &lt;- merge(d2,d3) # merging the two datasets into a master\n\nAgain, one last check to make sure everything is in the correct order, then merging the data sets into one master data set.\n\n# filter to narrow scope and see if transformations worked correctly and see what can be removed.\nd4 %&gt;% filter(Year == 2016, LocationDesc == 'Wyoming')\n\n   Year LocationDesc                Age                          Race  Gender\n1  2016      Wyoming     18 to 24 Years                     All Races Overall\n2  2016      Wyoming     18 to 44 Years                     All Races  Female\n3  2016      Wyoming     25 to 44 Years                     All Races Overall\n4  2016      Wyoming     45 to 64 Years                     All Races Overall\n5  2016      Wyoming 65 Years and Older                     All Races Overall\n6  2016      Wyoming   Age 20 and Older                     All Races Overall\n7  2016      Wyoming   Age 20 and Older                     All Races Overall\n8  2016      Wyoming   Age 20 and Older                     All Races Overall\n9  2016      Wyoming   Age 25 and Older                     All Races Overall\n10 2016      Wyoming   Age 25 and Older                     All Races Overall\n11 2016      Wyoming   Age 25 and Older                     All Races Overall\n12 2016      Wyoming           All Ages              African American Overall\n13 2016      Wyoming           All Ages                     All Races  Female\n14 2016      Wyoming           All Ages                     All Races    Male\n15 2016      Wyoming           All Ages                     All Races Overall\n16 2016      Wyoming           All Ages American Indian/Alaska Native Overall\n17 2016      Wyoming           All Ages        Asian/Pacific Islander Overall\n18 2016      Wyoming           All Ages                      Hispanic Overall\n19 2016      Wyoming           All Ages                         White Overall\n      Education QuitPctFrmrRespFreq QuitAttmptRespFreq CigCurrSmkerRespFreq\n1    All Grades                  NA                 NA                0.225\n2    All Grades                  NA                 NA                0.239\n3    All Grades                  NA                 NA                0.240\n4    All Grades                  NA                 NA                0.181\n5    All Grades                  NA                 NA                0.095\n6  &lt; 12th Grade                  NA                 NA                0.402\n7  &gt; 12th Grade                  NA                 NA                0.122\n8    12th Grade                  NA                 NA                0.267\n9  &lt; 12th Grade                  NA                 NA                0.386\n10 &gt; 12th Grade                  NA                 NA                0.122\n11   12th Grade                  NA                 NA                0.270\n12   All Grades                  NA                 NA                   NA\n13   All Grades               0.542              0.518                0.191\n14   All Grades               0.611              0.393                0.188\n15   All Grades               0.580              0.457                0.189\n16   All Grades                  NA                 NA                   NA\n17   All Grades                  NA                 NA                   NA\n18   All Grades                  NA                 NA                0.201\n19   All Grades                  NA                 NA                0.181\n   CigFreqDailyRespFreq CigFreqSomeRespFreq CigStatCurrRespFreq\n1                    NA                  NA                  NA\n2                    NA                  NA                  NA\n3                    NA                  NA                  NA\n4                    NA                  NA                  NA\n5                    NA                  NA                  NA\n6                    NA                  NA                  NA\n7                    NA                  NA                  NA\n8                    NA                  NA                  NA\n9                    NA                  NA                  NA\n10                   NA                  NA                  NA\n11                   NA                  NA                  NA\n12                   NA                  NA                  NA\n13                0.796               0.204               0.191\n14                0.745               0.255               0.188\n15                0.771               0.229               0.189\n16                   NA                  NA                  NA\n17                   NA                  NA                  NA\n18                   NA                  NA                  NA\n19                   NA                  NA                  NA\n   CigStatFrmrRespFreq CigStatNvrRespFreq EcigCurrUseRespFreq\n1                   NA                 NA               0.130\n2                   NA                 NA               0.075\n3                   NA                 NA               0.077\n4                   NA                 NA               0.033\n5                   NA                 NA               0.010\n6                   NA                 NA               0.100\n7                   NA                 NA               0.039\n8                   NA                 NA               0.066\n9                   NA                 NA               0.036\n10                  NA                 NA               0.042\n11                  NA                 NA               0.055\n12                  NA                 NA                  NA\n13               0.227              0.582               0.050\n14               0.295              0.518               0.061\n15               0.261              0.549               0.055\n16                  NA                 NA                  NA\n17                  NA                 NA                  NA\n18                  NA                 NA               0.033\n19                  NA                 NA               0.055\n   EcigFreqDailyRespFreq EcigFreqSomeRespFreq EcigStatCurrRespFreq\n1                     NA                   NA                   NA\n2                     NA                   NA                   NA\n3                     NA                   NA                   NA\n4                     NA                   NA                   NA\n5                     NA                   NA                   NA\n6                     NA                   NA                   NA\n7                     NA                   NA                   NA\n8                     NA                   NA                   NA\n9                     NA                   NA                   NA\n10                    NA                   NA                   NA\n11                    NA                   NA                   NA\n12                    NA                   NA                   NA\n13                 0.290                0.710                0.050\n14                 0.419                0.581                0.061\n15                 0.362                0.638                0.055\n16                    NA                   NA                   NA\n17                    NA                   NA                   NA\n18                    NA                   NA                   NA\n19                    NA                   NA                   NA\n   EcigStatFrmrRespFreq EcigStatNvrRespFreq TobCurrUseRespFreq\n1                    NA                  NA              0.175\n2                    NA                  NA              0.044\n3                    NA                  NA              0.139\n4                    NA                  NA              0.063\n5                    NA                  NA              0.038\n6                    NA                  NA              0.084\n7                    NA                  NA              0.079\n8                    NA                  NA              0.137\n9                    NA                  NA              0.086\n10                   NA                  NA              0.068\n11                   NA                  NA              0.127\n12                   NA                  NA                 NA\n13                0.157               0.793              0.023\n14                0.215               0.725              0.170\n15                0.186               0.758              0.098\n16                   NA                  NA                 NA\n17                   NA                  NA                 NA\n18                   NA                  NA              0.061\n19                   NA                  NA              0.097\n   TobFreqDailyRespFreq TobFreqSomeRespFreq TobStatCurrRespFreq\n1                    NA                  NA                  NA\n2                    NA                  NA                  NA\n3                    NA                  NA                  NA\n4                    NA                  NA                  NA\n5                    NA                  NA                  NA\n6                    NA                  NA                  NA\n7                    NA                  NA                  NA\n8                    NA                  NA                  NA\n9                    NA                  NA                  NA\n10                   NA                  NA                  NA\n11                   NA                  NA                  NA\n12                   NA                  NA                  NA\n13                   NA                  NA               0.023\n14                0.693               0.307               0.170\n15                0.672               0.328               0.098\n16                   NA                  NA                  NA\n17                   NA                  NA                  NA\n18                   NA                  NA                  NA\n19                   NA                  NA                  NA\n   TobStatNonCurrRespFreq QuitPctFrmrSrvCnt QuitAttmptSrvCnt CigCurrSmkerSrvCnt\n1                      NA                NA               NA                140\n2                      NA                NA               NA                469\n3                      NA                NA               NA                758\n4                      NA                NA               NA               1707\n5                      NA                NA               NA               1788\n6                      NA                NA               NA                211\n7                      NA                NA               NA               2896\n8                      NA                NA               NA               1231\n9                      NA                NA               NA                203\n10                     NA                NA               NA               2840\n11                     NA                NA               NA               1200\n12                     NA                NA               NA                 NA\n13                  0.977              1019              258               2492\n14                  0.830               951              217               1901\n15                  0.902              1970              475               4393\n16                     NA                NA               NA                 NA\n17                     NA                NA               NA                 NA\n18                     NA                NA               NA                213\n19                     NA                NA               NA               3961\n   CigFreqDailySrvCnt CigFreqSomeSrvCnt CigStatCurrSrvCnt CigStatFrmrSrvCnt\n1                  NA                NA                NA                NA\n2                  NA                NA                NA                NA\n3                  NA                NA                NA                NA\n4                  NA                NA                NA                NA\n5                  NA                NA                NA                NA\n6                  NA                NA                NA                NA\n7                  NA                NA                NA                NA\n8                  NA                NA                NA                NA\n9                  NA                NA                NA                NA\n10                 NA                NA                NA                NA\n11                 NA                NA                NA                NA\n12                 NA                NA                NA                NA\n13                339               339              2492              2492\n14                285               285              1901              1901\n15                624               624              4393              4393\n16                 NA                NA                NA                NA\n17                 NA                NA                NA                NA\n18                 NA                NA                NA                NA\n19                 NA                NA                NA                NA\n   CigStatNvrSrvCnt EcigCurrUseSrvCnt EcigFreqDailySrvCnt EcigFreqSomeSrvCnt\n1                NA               139                  NA                 NA\n2                NA               470                  NA                 NA\n3                NA               759                  NA                 NA\n4                NA              1713                  NA                 NA\n5                NA              1799                  NA                 NA\n6                NA               212                  NA                 NA\n7                NA              2902                  NA                 NA\n8                NA              1239                  NA                 NA\n9                NA               204                  NA                 NA\n10               NA              2847                  NA                 NA\n11               NA              1208                  NA                 NA\n12               NA                NA                  NA                 NA\n13             2492              2500                  76                 76\n14             1901              1910                  65                 65\n15             4393              4410                 141                141\n16               NA                NA                  NA                 NA\n17               NA                NA                  NA                 NA\n18               NA               213                  NA                 NA\n19               NA              3978                  NA                 NA\n   EcigStatCurrSrvCnt EcigStatFrmrSrvCnt EcigStatNvrSrvCnt TobCurrUseSrvCnt\n1                  NA                 NA                NA              140\n2                  NA                 NA                NA              470\n3                  NA                 NA                NA              756\n4                  NA                 NA                NA             1715\n5                  NA                 NA                NA             1799\n6                  NA                 NA                NA              213\n7                  NA                 NA                NA             2901\n8                  NA                 NA                NA             1239\n9                  NA                 NA                NA              205\n10                 NA                 NA                NA             2845\n11                 NA                 NA                NA             1208\n12                 NA                 NA                NA               NA\n13               2500               2500              2500             2503\n14               1910               1910              1910             1907\n15               4410               4410              4410             4410\n16                 NA                 NA                NA               NA\n17                 NA                 NA                NA               NA\n18                 NA                 NA                NA              214\n19                 NA                 NA                NA             3978\n   TobFreqDailySrvCnt TobFreqSomeSrvCnt TobStatCurrSrvCnt TobStatNonCurrSrvCnt\n1                  NA                NA                NA                   NA\n2                  NA                NA                NA                   NA\n3                  NA                NA                NA                   NA\n4                  NA                NA                NA                   NA\n5                  NA                NA                NA                   NA\n6                  NA                NA                NA                   NA\n7                  NA                NA                NA                   NA\n8                  NA                NA                NA                   NA\n9                  NA                NA                NA                   NA\n10                 NA                NA                NA                   NA\n11                 NA                NA                NA                   NA\n12                 NA                NA                NA                   NA\n13                 NA                NA              2503                 2503\n14                252               252              1907                 1907\n15                278               278              4410                 4410\n16                 NA                NA                NA                   NA\n17                 NA                NA                NA                   NA\n18                 NA                NA                NA                   NA\n19                 NA                NA                NA                   NA\n\n\nFiltering down to one Year and Location, so I can see how some of the aggregated values roll up and how they combine with other demographic variables. This gives me some insight into which questions were missing at what grains, and also allows me to check my merge to make sure it looks right. We finally have the data in a shape and layout that makes sense, and we see that its still kind of a mess. The intention was to maintain the separated demographic columns, and get each survey question into a column or columns. We accomplished that, however we can see that the demographics don’t stack in most cases. For example, I can see aggregated sample sizes for each Gender and the frequency for each question’s response, but I cannot see the age, education, or race makeup for each gender, or any combination of the demographic variables really. This was likely a deliberate choice for sample size constraints, nonetheless it complicates the next steps of this exercise. For a more in depth exercise, I would probably consider imputing the missing sample sizes or rates to get stratification by gender, age and race combined. However for the sake of this exercise, I will break each of the demographics into separate data sets since they have different questions considered. Once done, I will focus in on the Gender data set, since it looks like the gender demographic is the most complete across all questions.\nThe aggregated values by demographic aren’t necessary in the final version of any of these data sets, but we may need them to get the response volumes of some of the demographic values if there few responses. So, we can filter against all of the other demographic variables to only included their roll-up or overall value. First we will check for what those values are for each variable.\n\nunique(d4$Age) #'All Ages', also note some of the age bins are overlapping.\n\n[1] \"18 to 24 Years\"     \"18 to 44 Years\"     \"25 to 44 Years\"    \n[4] \"45 to 64 Years\"     \"65 Years and Older\" \"Age 20 and Older\"  \n[7] \"Age 25 and Older\"   \"All Ages\"          \n\nunique(d4$Gender) #'Overall'\n\n[1] \"Overall\" \"Female\"  \"Male\"   \n\nunique(d4$Race) #'All Races'\n\n[1] \"All Races\"                     \"African American\"             \n[3] \"American Indian/Alaska Native\" \"Asian/Pacific Islander\"       \n[5] \"Hispanic\"                      \"White\"                        \n\nunique(d4$Education) #'All Grades'\n\n[1] \"All Grades\"   \"&lt; 12th Grade\" \"&gt; 12th Grade\" \"12th Grade\"  \n\n\n\n# Age dataset, looking at rollup for all other variables except age, \n# then selecting relevant columns and filtering any last missing values\nAgeSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age != 'All Ages'\n              ,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\n# Gender Dataset. This time keeping the rollup value as well as the others,\n# but still filtering all other variables to their rollup\n# Also filtering out the national Median rows since its not the same grain as the others\n# finally removing last missing values. Will select relevant columns later.\nGndSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age == 'All Ages'\n              #,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  filter(LocationDesc != 'National Median (States and DC)',\n         !is.na(CigCurrSmkerRespFreq)\n         )\n\n# Race dataset, looking at rollup for all other variables except Race, \n# then selecting relevant columns and filtering any last missing values\nRaceSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age == 'All Ages'\n              ,Gender == 'Overall'\n              #,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\n# Education dataset, looking at rollup for all other variables except Education and Age,\n#since age actually is startified with education (interestingly, not vice-versa)\n# then selecting relevant columns and filtering any last missing values\nEdSrv &lt;- d4 %&gt;% filter(1==1\n              #,Age == 'All Ages'\n              ,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education != 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\nNow to focus in on the gender data set. I did not select specific columns earlier because the treatment was a bit different than the other data sets. This data set in particular has a lot more opportunity for imputing and other transformations, but for the simplicity of this exercise we will select only a few interesting and complete columns. Also one last note, the Gender variables has three values, Male, Female, and Overall. Since the Response frequency is the true variable of interest, I’m going to leave the “Overall” values in. Under other circumstances I would remove these and calculate overall values only when they apply, that way there aren’t “repeated” observations in the data set. However, the response frequency variables are pre-processed and rounded, so it would only unnecessarily complicate things to try and calculate response counts to get a proper aggregation.\n\n#Selecting relevant columns for the remainder of the exercise\nGndSrv &lt;- GndSrv %&gt;% \n  select(\n    Year\n    ,LocationDesc\n    ,Gender\n    ,QuitAttmptRespFreq\n    ,CigStatCurrRespFreq\n    ,CigStatFrmrRespFreq\n    ,CigStatNvrRespFreq\n    )\n\nI have decided to focus in on the questions for Cigarette usage, namely status and if they have attempted to quit in the last year. I also kept Year and Location since those variables were the least problematic, and of course Gender since this is the Gender-specific dataset.\n\nGndSrv %&gt;% group_by(Gender) %&gt;% #grouped by general to see potential differences in mean.\n  summarize( # getting mean values for each of my questions\n    AvgQuitAttempt = mean(QuitAttmptRespFreq)\n    ,AvgCurentSmokers = mean(CigStatCurrRespFreq)\n    ,AvgFormerSmokers = mean(CigStatFrmrRespFreq)\n    ,AvgNeverSmoked = mean(CigStatNvrRespFreq)\n  )\n\n# A tibble: 3 × 5\n  Gender  AvgQuitAttempt AvgCurentSmokers AvgFormerSmokers AvgNeverSmoked\n  &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1 Female           0.526            0.163            0.215          0.622\n2 Male             0.510            0.203            0.283          0.514\n3 Overall          0.518            0.183            0.248          0.569\n\n\nLet’s start by getting some quick descriptive statistics. Notice that this table will get the average Rate for each of the questions across all states and timeframes. The number of combinations we have between categorical variables, especially since the Location variable has so many categories, complicates our ability to get a closer look. Faceting and framing will help with this, as we will see in a moment.\n\nbox1o &lt;- GndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_boxplot() +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\nbox1o\n\n\n\n\n\n\n\nhist1o &lt;- GndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\nhist1o\n\n\n\n\n\n\n\n\nthe boxplots should show the distribution of the Percent of Current Smokers across the different locations. I have filter to 2011, the earliest year in the data set, and 2019, the latest year. Here I wanted to see if there would be a large difference from the beginning of the timeframe to the end, and for the most part we can see that there is. It appears as though we have a lower rate of “Current” Smokers from beginning to end. the distribution is not particularly clean, but it also looks to be shifting left while keeping some outliers on the higher end to make for a bit of a right skew.\n\nGndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=QuitAttmptRespFreq)) + # Quit Attempt Rate observed\n  geom_boxplot() +\n  labs(x = 'Percent of Quit Attempts\\nAmong Current Smokers') +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\n\n\n\n\n\n\n\nGndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=QuitAttmptRespFreq)) + # Quit Attempt Rate observed\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Quit Attempts\\nAmong Current Smokers', y = NULL) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\n\n\n\n\n\n\n\n\nGoing into the quit rate, it’s a bit of a different story here. There is maybe a little bit of a left shift in the Males and Overall, but it is far less pronounced than it was for the percent of current smokers. This makes sense, since it is the quit rate for current smokers, meaning we would expect it to either stay the same or potentially increase over time. This begs the question if the decrease in current smokers is coming from more people quitting or fewer people starting. The distribution is also all over the place, so there is a wider variance. This will be apparent in the next visual.\n\naniplot &lt;- GndSrv %&gt;% filter(Gender != 'Overall') %&gt;% # wanting Male and Female only\n  ggplot(aes(x=CigStatCurrRespFreq, y = QuitAttmptRespFreq)) + #plotting current smokers against quit attempt rate\n  geom_point(aes(frame = Year, color = Gender, ids = LocationDesc)) + #frame for animation, ids for hover text, color for comparison\n  labs(x = 'Percent of Current Smokers', y = 'Percent of Quit Attempts\\nAmong Current Smokers')\n\nWarning in geom_point(aes(frame = Year, color = Gender, ids = LocationDesc)):\nIgnoring unknown aesthetics: frame and ids\n\nggplotly(aniplot) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animaton, also slows down the slider when i click play\n\n\n\n\n\nThis was a pretty telling visual, so a few observations. First, there tend to be a higher rate of men who are current smokers over women. Second, the distribution is slowly but surely shifting left over time, it was not simply a fluke between 2011 and 2019 earlier. Finally, the wide variance in the percent of quit attempts is apparent, with the amount of vertical movement each point had in the visual. Still the distribution did not look to have any large shift up or down over time, which tells me the rate of people quitting is roughly the same, so more like the decrease in current smokers is happening because fewer people ever start smoking.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in later chunks\n  ggplot(aes(x = CigStatCurrRespFreq, y = fct_reorder(LocationDesc, CigStatCurrRespFreq), frame = Year)) + #fct_reorder orders location by Current smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of Current Smokers', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nMostly wanting to see if any outliers are more apparent here. Guam looks to have the highest rate of people who smoke, but they also have wide swings, likely due to a smaller sample size. Worth noting that the ranking of rates by state does not change significantly over time either.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in other chunks\n  ggplot(aes(x = CigStatFrmrRespFreq, y = fct_reorder(LocationDesc, CigStatFrmrRespFreq), frame = Year)) + #fct_reorder orders location by former smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of Former Smokers', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nThe rate of Former smokers does not look like it shares the trend. Again, if we saw an increase in the rates of people quitting we might expect the rate of Former Smokers to increase over time, but we don’t really see that much here.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in other chunks\n  ggplot(aes(x = CigStatNvrRespFreq, y = fct_reorder(LocationDesc, CigStatNvrRespFreq), frame = Year)) + #fct_reorder orders location by Never smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of People Who Have Never Smoked', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nHere we can see that increase in rate of people who have never smoked, though admittedly it still seems less pronounced than it was for the decrease in Current smokers. Still, it’s in line with the hypothesis that people are more likely are more likely to have never started smoking than they were in the past, and that is why we are seeing such a decrease in the rate of Current Smokers. Not to end on a darker note, but this is also likely a result of those who were Current Smokers in the earlier years dying off at a higher rate than non-smokers. This may suggest that younger people are much less likely to start smoking as well."
  },
  {
    "objectID": "case-study-3/case-study-3-write-up.html",
    "href": "case-study-3/case-study-3-write-up.html",
    "title": "DA 6813 Case Study 3 Dow Jones",
    "section": "",
    "text": "1 Executive Summary\nThis case study focuses on predicting weekly stock price changes for the Dow Jones Index to optimize investment strategies. Using historical stock performance data, including prices, trading volumes, and financial indicators, the analysis aimed to forecast the percent_change_next_weeks_price for each stock. Three models—Linear Regression (LM), Decision Trees (DT), and Support Vector Regression (SVR)—were developed and evaluated using Root Mean Square Error (RMSE) as the primary metric. Data preprocessing steps included handling missing values with KNN imputation and normalizing numeric features to ensure comparability, especially for SVR. The findings showed that SVR consistently outperformed LM and DT, delivering the lowest RMSE across various stocks. This highlights its superior capability in capturing complex, non-linear relationships within the data. The insights gained from this analysis provide actionable recommendations for selecting stocks with high growth potential and managing investment risks. Future enhancements, such as incorporating ensemble methods and additional financial indicators, could further improve predictive accuracy and decision-making.\n\n\n2 Problem Statement\nThe Dow Jones Index case study addresses a critical business challenge: predicting weekly stock performance to optimize investment strategies. In a highly volatile market, businesses and investors rely on accurate forecasts to maximize returns and mitigate risks. This study aims to develop predictive models that forecast the percentage change in stock prices for the following week, helping identify stocks with the highest growth potential. Using historical data, including stock prices, trading volumes, and financial indicators, the analysis seeks to improve decision-making by leveraging models such as Linear Regression, Decision Trees, and Support Vector Regression. These models will be evaluated for their accuracy and ability to assess risk using methods like the Capital Asset Pricing Model (CAPM). Addressing this problem will enable businesses to allocate resources more effectively, capitalize on profitable opportunities, and enhance their competitive edge in the financial market. Notably, the study will also examine broader market risks, such as the unusual losses experienced by all Dow Jones stocks in the week ending May 27, 2011, providing a comprehensive view of potential challenges.\n\n\n3 Additional Sources\nSupport Vector Machines (SVMs) have been extensively applied in financial markets for stock price prediction due to their ability to handle complex, non-linear relationships. For instance, a study by Kim (2003) demonstrated that SVMs could outperform traditional models in forecasting stock price indices, highlighting their robustness in capturing market dynamics.\nThe Capital Asset Pricing Model (CAPM) is a fundamental tool in finance for assessing the risk and expected return of an investment. It establishes a linear relationship between the expected return of an asset and its systematic risk, measured by beta. This model aids investors in determining whether a stock is fairly priced relative to its risk. For a comprehensive understanding of CAPM, Investopedia provides an in-depth explanation of its components and applications.\nIntegrating SVMs for stock price prediction with CAPM for risk assessment can offer a holistic approach to investment decision-making, combining advanced predictive analytics with established financial theories.\n\nKim, K. (2003).\nA study on the application of Support Vector Machines (SVM) for stock price index prediction.\nhttps://www.researchgate.net/publication/220379019\nWall Street Prep - Capital Asset Pricing Model (CAPM).\nComprehensive guide on using CAPM for risk assessment in financial markets.\nhttps://www.wallstreetprep.com/knowledge/capm-capital-asset-pricing-model\nInvestopedia - Capital Asset Pricing Model (CAPM).\nIn-depth explanation of CAPM, its components, and applications.\nhttps://www.investopedia.com/terms/c/capm.asp\n\n\n\n4 Methodology\nTo predict weekly stock price changes (percent_change_next_weeks_price), we employed three models: Linear Regression (LM), Decision Trees (DT), and Support Vector Regression (SVR). Before modeling, we performed comprehensive data preprocessing. Missing values in key variables were addressed using KNN imputation to preserve data integrity. Numeric features were scaled and normalized to ensure comparability, particularly important for SVR, which relies on distance measures in feature space. Lagged variables were created to capture potential temporal dependencies; however, lagged plots indicated no significant relationship between the lagged variables and the target, so they were excluded from the final models. The data was then split into training (Q1) and testing (Q2) sets, maintaining temporal integrity to simulate real-world forecasting scenarios.\nEach model comes with specific assumptions. Linear Regression assumes a linear relationship between predictors and the target, independence of errors, homoscedasticity (constant error variance), and normally distributed errors. It also requires minimal multicollinearity among predictors. Decision Trees, being non-parametric, do not assume a specific relationship between predictors and the target but are sensitive to small data changes. SVR, which emerged as the best-performing model, makes no assumptions about the data’s underlying distribution but relies on normalized inputs to maximize the margin around the true values. Model performance was evaluated using Root Mean Square Error (RMSE), as it effectively measures prediction accuracy without the limitations of Mean Absolute Percentage Error (MAPE), which is problematic when actual values approach zero.\nTo tailor predictions to individual stocks, we applied these modeling techniques separately to each stock in the dataset. A loop was used to iterate through each stock, training and testing the models on its specific data. This approach ensured that the models accounted for the unique patterns and behaviors of each stock. SVR consistently achieved the lowest RMSE across stocks, making it the most reliable model for predicting weekly stock changes. Future work could explore ensemble methods or incorporate additional financial indicators to improve performance further.\n\n\n5 Data\nThe dataset for this analysis consists of weekly stock performance metrics from the Dow Jones Index, including variables such as opening, closing, high, and low prices, trading volumes, and percentage changes in prices. After importing the data, we conducted a thorough inspection to understand its structure and address missing values. Missing data, particularly in numeric variables like percent_change_volume_over_last_wk and previous_weeks_volume, was imputed using KNN imputation to maintain data integrity and preserve patterns. Numeric features were normalized and scaled to ensure comparability, which was especially crucial for models like SVR that depend on distance-based calculations.\nTo explore potential temporal dependencies, we initially created lagged variables, assuming that previous week changes might have predictive power for the target variable, percent_change_next_weeks_price. However, as shown in the lag plot below, there was no significant correlation between the lagged variable (percent_change_price of the previous week) and the target variable. The scatterplot shows a nearly flat trend line, indicating that the past week’s percentage change provides no meaningful predictive value for the current week’s change. Consequently, we excluded the lagged variables from our final models to avoid introducing unnecessary noise. After preprocessing, the data was split into training (Q1) and testing (Q2) sets, maintaining temporal integrity to simulate realistic forecasting scenarios. This preprocessing ensured a solid foundation for our modeling efforts.\nBelow, the plot illustrates the lack of correlation between the lagged variable and the target, confirming its irrelevance to our predictive modeling:\n\n\n\n\n\n\n\n\n\n\n\n6 Findings\nOur analysis assessed the performance of Linear Regression (LM), Decision Trees (DT), and Support Vector Regression (SVR) for predicting weekly stock price changes, with Root Mean Square Error (RMSE) as the evaluation metric. Preprocessing steps, including KNN imputation and normalization, ensured the models were optimized for performance. Among the models, SVR consistently delivered the lowest RMSE, averaging 0.211, compared to 18.3 for LM and 0.250 for DT, as shown in the bar chart below. This highlights SVR’s superior ability to capture complex, non-linear relationships, making it the most reliable model for stock forecasting.\nFor individual stocks such as AA, AXP, and BA, SVR demonstrated robust performance, with RMSE values of 0.234, 0.167, and 0.250, respectively. In contrast, LM’s RMSE for these stocks was significantly higher at 18.8, 16.8, and 3.21, while DT showed moderate accuracy with RMSE values of 0.335, 0.234, and 0.274. The bar chart below compares RMSE values across all three models, clearly illustrating SVR’s consistent accuracy.\nThe second visual, a line graph, provides an analysis of stock performance relative to the S&P 500. This graph plots the Beta coefficient (x-axis), which measures each stock’s volatility compared to the market, against performance (y-axis). Stocks on the left represent lower risk (Beta &lt; 1), while those on the right show higher risk (Beta &gt; 1). This risk-performance relationship helps contextualize each stock’s predictive accuracy and market behavior. SVR’s ability to accurately forecast stock prices across both low- and high-risk stocks demonstrates its adaptability, even under varying market conditions.\n\n6.0.1 Visuals:\n\nBar Chart: RMSE Comparison Across Models\nHighlights the predictive accuracy of LM, DT, and SVR for individual stocks.\nLine Graph: Stock Performance Relative to S&P 500\nShows the relationship between risk (Beta coefficient) and stock performance, providing insights into market dynamics and risk management.\nBar Graph: Comparison of Actual vs SVR Prediction by Stock\n\nThese findings affirm SVR as the most effective model for predicting weekly stock price changes, offering both accuracy and adaptability. Future efforts could explore ensemble methods or integrate additional risk indicators for enhanced forecasting.\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7 Conclusion\nIn conclusion, the study demonstrated that Support Vector Regression (SVR) is the most effective model for predicting weekly stock price changes, consistently outperforming Linear Regression and Decision Trees in terms of RMSE. This result underscores the importance of selecting models capable of capturing complex, non-linear relationships in financial data. The preprocessing steps, including normalization and KNN imputation, ensured the models operated on high-quality data, contributing to the reliability of the findings. While the SVR model showed strong predictive performance, future improvements could involve exploring ensemble techniques and integrating additional indicators such as market sentiment or macroeconomic variables. These enhancements could further refine the models’ predictive capabilities and provide deeper insights into stock market behavior, aiding investors in making informed, data-driven decisions."
  },
  {
    "objectID": "case-study-1/case-study-1-write-up.html",
    "href": "case-study-1/case-study-1-write-up.html",
    "title": "DA 6813 Case Study 1",
    "section": "",
    "text": "1 Executive Summary\n\nThis study aimed to develop a predictive model to determine whether clients of a Portuguese banking institution would subscribe to a term deposit following a marketing campaign. The analysis focused on a dataset containing client demographics, financial indicators, and marketing campaign details. Key variables such as age, contact type, month of the campaign, number of previous contacts, employment rate, and previous client contacts were found to influence the likelihood of subscription.\nDue to the unbalanced nature of our response variable, a balanced sample was taken so the model would not favor prediction for those who did not subscribe versus those that did. Then, a logistic regression model was built using backward stepwise selection based on the Akaike Information Criterion (AIC). Initially, the model had high specificity (86.36%), meaning it effectively identified clients that did not subscribe. However, despite balancing train and test sets, it underperformed in identifying clients who did subscribe, as reflected by its lower sensitivity (56.99%).\nTo further improve the model’s overall performance, an optimal decision threshold was identified, balancing sensitivity and specificity. After optimization, the model achieved a balanced accuracy of 73.48%, with both sensitivity and specificity improving to 73.12% and 73.86%, respectively. This adjustment enhanced the model’s ability to accurately classify both subscribing and non-subscribing clients.\nThe results provide actionable insights for the bank, enabling it to more effectively target clients and optimize marketing resources. The findings suggest that further improvements, such as testing alternative models or incorporating more detailed features, could yield even higher predictive performance.\n\n\n2 Problem Statement\n\nThe task of this case study is to develop a predictive model to classify whether clients of a Portuguese banking institution will subscribe to a term deposit following a direct marketing campaign. The campaigns were based on phone calls, and often, multiple contacts were made to the same client to assess if they would subscribe (‘yes’) or not (‘no’). The goal is to accurately predict the likelihood of a client subscribing to a term deposit based on various input variables, including demographic, social, and economic indicators, as well as information from previous marketing campaigns.\nKey variables influencing the prediction include the client’s age, job type, marital status, education, and financial status (e.g., default history, housing loan, and personal loan). Additionally, variables related to the marketing campaign, such as the type of contact, day, and month of the last contact, and previous campaign outcomes, are also critical in determining the client’s response. Social and economic context attributes, such as the employment variation rate and consumer confidence index, are included to enhance the model’s predictive power.\nThe primary objective is to build a classification model that can accurately predict whether a client will subscribe to a term deposit. This will allow the bank to target its marketing efforts more effectively, optimizing resource allocation, and improving conversion rates.\n\n\n3 Methodology\n\nThe analysis began with data preparation, where the dataset of 21 variables, both categorical and numeric, was cleaned and transformed. Categorical variables were converted to factors for proper handling in models, and the target variable, y, was transformed into a binary indicator, where 1 indicated a client subscribed to a term deposit and 0 indicated otherwise. The pdays variable, which represented the number of days since a previous contact, was recoded into a binary indicator named pcontact to simplify the analysis.\nExploratory data analysis (EDA) was conducted to explore the distribution and relationships within the data. Box plots were generated to visualize the distribution of numeric variables based on the target outcome, and bar plots were used to explore the frequency distribution of categorical variables. Additionally, a correlation matrix was constructed to examine the relationships among numeric variables and to identify potential multicollinearity issues.\nThe dataset was then split into a training set (80%) and a test set (20%) to ensure model evaluation was conducted on unseen data. To address class imbalance, as there were significantly more clients who did not subscribe to the term deposit, resampling techniques were applied to create a balanced dataset for training. This was done by narrowing down to all responses where a bank customer subscribed and an equal number of those who did not subscribe before performing the train and test split.\nLogistic regression was selected as the primary modeling technique. A stepwise backward selection method, based on the Akaike Information Criterion (AIC), was used to remove insignificant variables and select the most parsimonious model. To mitigate multicollinearity, variables with high variance inflation factor (VIF) values were removed.\nThe model was evaluated using metrics such as accuracy, sensitivity, and specificity, and the results were summarized in a confusion matrix. Given that class imbalance persisted, even after balancing the data, an optimal cutoff threshold for classifying clients was determined by balancing sensitivity and specificity. This threshold optimization helped improve the performance of the logistic regression model by identifying the point where sensitivity and specificity converged.\n\n\n4 Data\n\nThe dataset contains 4,119 rows and 21 variables. The variables are split between 11 categorical (character type) and 10 numeric variables. In preparation for analysis, categorical variables were converted into factors to ensure proper handling in models. The target variable, y, was converted into a binary outcome where “yes” was mapped to 1, indicating that the client subscribed to a term deposit, and “no” was mapped to 0.\n\n\n\nData summary\n\n\nName\ndf3\n\n\nNumber of rows\n4119\n\n\nNumber of columns\n21\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n10\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\njob\n0\n1\nFALSE\n12\nadm: 1012, blu: 884, tec: 691, ser: 393\n\n\nmarital\n0\n1\nFALSE\n4\nmar: 2509, sin: 1153, div: 446, unk: 11\n\n\neducation\n0\n1\nFALSE\n8\nuni: 1264, hig: 921, bas: 574, pro: 535\n\n\ndefault\n0\n1\nFALSE\n3\nno: 3315, unk: 803, yes: 1\n\n\nhousing\n0\n1\nFALSE\n3\nyes: 2175, no: 1839, unk: 105\n\n\nloan\n0\n1\nFALSE\n3\nno: 3349, yes: 665, unk: 105\n\n\ncontact\n0\n1\nFALSE\n2\ncel: 2652, tel: 1467\n\n\nmonth\n0\n1\nFALSE\n10\nmay: 1378, jul: 711, aug: 636, jun: 530\n\n\nday_of_week\n0\n1\nFALSE\n5\nthu: 860, mon: 855, tue: 841, wed: 795\n\n\npoutcome\n0\n1\nFALSE\n3\nnon: 3523, fai: 454, suc: 142\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nage\n0\n1\n40.11\n10.31\n18.00\n32.00\n38.00\n47.00\n88.00\n▅▇▅▁▁\n\n\nduration\n0\n1\n256.79\n254.70\n0.00\n103.00\n181.00\n317.00\n3643.00\n▇▁▁▁▁\n\n\ncampaign\n0\n1\n2.54\n2.57\n1.00\n1.00\n2.00\n3.00\n35.00\n▇▁▁▁▁\n\n\nprevious\n0\n1\n0.19\n0.54\n0.00\n0.00\n0.00\n0.00\n6.00\n▇▁▁▁▁\n\n\nemp.var.rate\n0\n1\n0.08\n1.56\n-3.40\n-1.80\n1.10\n1.40\n1.40\n▁▃▁▁▇\n\n\ncons.price.idx\n0\n1\n93.58\n0.58\n92.20\n93.08\n93.75\n93.99\n94.77\n▁▆▃▇▂\n\n\ncons.conf.idx\n0\n1\n-40.50\n4.59\n-50.80\n-42.70\n-41.80\n-36.40\n-26.90\n▅▇▁▇▁\n\n\neuribor3m\n0\n1\n3.62\n1.73\n0.64\n1.33\n4.86\n4.96\n5.04\n▅▁▁▁▇\n\n\nnr.employed\n0\n1\n5166.48\n73.67\n4963.60\n5099.10\n5191.00\n5228.10\n5228.10\n▁▁▃▁▇\n\n\ny\n0\n1\n0.11\n0.31\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\npcontact\n0\n1\n0.04\n0.19\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\n\n\n\nDuring the exploratory data analysis (EDA), various visualizations were utilized to examine the distributions and relationships within the dataset. Box plots were used to display the distribution of numeric variables such as age, campaign, emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, and nr.employed, stratified by whether or not the client subscribed to a deposit. These visualizations helped identify differences in the distributions between clients who subscribed and those who did not.\nBar plots were also used to explore the distribution of categorical variables like job, marital status, education, default, housing, loan, contact, month, day_of_week, and poutcome. These plots illustrated the relative frequencies of each category concerning the target outcome, shedding light on which factors might be more indicative of a client’s decision to subscribe.\nRegarding the pdays variable, a value of 999 indicated that a client had not been previously contacted, while any other number signified prior contact. To simplify the analysis, this variable was transformed into a binary indicator, where “1” denotes previous contact and “0” indicates no prior contact.\nFinally, to explore relationships among numeric variables, a correlation matrix was generated. This matrix was visualized using a heatmap to provide a clear representation of the strength and direction of correlations between variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 Findings\nThe logistic regression model was built using a backward stepwise elimination process based on the Akaike Information Criterion (AIC). During each step, insignificant variables such as job, day_of_week, education, marital status, and others were removed to improve model parsimony and fit. The final model included age, contact, month, campaign, nr.employed, and pcontact as the key predictors.\nTwo sets of results were generated to evaluate the performance of the model—one based on the initial logistic regression with a non-optimal threshold and the other with an optimized threshold for better classification performance.\nFor the non-optimal threshold, the confusion matrix showed an accuracy of 71.27%, with a specificity of 86.36% and a sensitivity of 56.99%. The model had higher specificity, meaning it was better at correctly identifying clients who did not subscribe, but it performed less well in identifying those who did.\nAfter threshold optimization, the model’s performance improved. The confusion matrix for the optimized model yielded an accuracy of 73.48%, with more balanced specificity (73.86%) and sensitivity (73.12%). This optimization process allowed for better classification of both subscribing and non-subscribing clients by adjusting the decision threshold to balance the trade-off between specificity. and sensitivity\nOverall, the findings indicate that the threshold adjustment improved the model’s ability to predict both positive and negative outcomes more evenly. This highlights the importance of optimizing decision thresholds, particularly in imbalanced datasets like this one. Interestingly, this issue was still encountered despite balancing the initial audience data based on the response value, suggesting that an imbalance in the predictors may also influence a binary classification model’s performance.\n\n\n\n\nAIC at Each Step\n\n\n\n\n\n\n\n\n\n\nStep\nDf\nDeviance\nResid. Df\nResid. Dev\nAIC\n\n\n\n\n\nNA\nNA\n674\n744.9079\n838.9079\n\n\n- job\n11\n10.1992377\n685\n755.1072\n827.1072\n\n\n- day_of_week\n4\n0.9350717\n689\n756.0423\n820.0423\n\n\n- education\n6\n5.8197873\n695\n761.8620\n813.8620\n\n\n- marital\n3\n1.8152111\n698\n763.6772\n809.6772\n\n\n- cons.price.idx\n1\n0.0286753\n699\n763.7059\n807.7059\n\n\n- default\n1\n0.0548683\n700\n763.7608\n805.7608\n\n\n- cons.conf.idx\n1\n0.2981325\n701\n764.0589\n804.0589\n\n\n- housing\n2\n2.4672400\n703\n766.5262\n802.5262\n\n\n- previous\n1\n1.0908448\n704\n767.6170\n801.6170\n\n\n- poutcome\n2\n3.4459295\n706\n771.0629\n801.0629\n\n\n\n\n\n\nConfusion Matrix Non-Optimal Value\n\n\n\n0\n1\n\n\n\n\n0\n76\n40\n\n\n1\n12\n53\n\n\n\n\n\n\nModel Results Non-Optimal Value\n\n\nMetric\nValue\n\n\n\n\nAccuracy\n0.7127\n\n\nSensitivity\n0.5699\n\n\nSpecificity\n0.8636\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix Optimal Value\n\n\n\n0\n1\n\n\n\n\n0\n65\n25\n\n\n1\n23\n68\n\n\n\n\n\n\nModel Results Optimal Value\n\n\nMetric\nValue\n\n\n\n\nAccuracy\n0.7348\n\n\nSensitivity\n0.7312\n\n\nSpecificity\n0.7386\n\n\n\n\n\nas.factor(y) ~ age + contact + month + campaign + nr.employed + \n    pcontact\n\n\n\n\n6 Conclusion\nThe analysis successfully developed a logistic regression model to predict whether clients would subscribe to a term deposit following a marketing campaign. Through data preparation, exploratory analysis, and model building, key predictors such as age, contact, month, campaign, employment status (nr.employed), and previous contact (pcontact) were identified as significant factors influencing client subscription decisions.\nThe initial model had a higher specificity but lower sensitivity, meaning it was more effective at identifying clients who did not subscribe but less so for those who did. After optimizing the decision threshold, the model achieved a more balanced performance, with improved accuracy and sensitivity. This highlights the value of threshold adjustment in classification problems, particularly when dealing with imbalanced datasets.\nIn conclusion, the model offers actionable insights for the bank to more effectively target potential customers, allowing for better allocation of marketing resources and improved conversion rates. Further enhancements, such as testing additional models or incorporating more complex feature engineering, could further improve predictive performance."
  },
  {
    "objectID": "applications-final-project/Final_project_sandbox.html#unbalanced-split",
    "href": "applications-final-project/Final_project_sandbox.html#unbalanced-split",
    "title": "Final Project Sandbox",
    "section": "Unbalanced Split",
    "text": "Unbalanced Split\n\n# making test and train sets\nset.seed(24601) \nunbal_part &lt;- sample(nrow(df2),0.8*nrow(df2),replace = F)\n\nunbal_train &lt;- df2[unbal_part,]\nunbal_test &lt;- df2[-unbal_part,]"
  },
  {
    "objectID": "applications-final-project/Final_project_sandbox.html#standardizing-numeric-variables",
    "href": "applications-final-project/Final_project_sandbox.html#standardizing-numeric-variables",
    "title": "Final Project Sandbox",
    "section": "Standardizing Numeric Variables",
    "text": "Standardizing Numeric Variables\n\nnum_means &lt;- unbal_train %&gt;% select_if(is.numeric) %&gt;% colMeans() %&gt;% t() %&gt;% as.data.frame()\n# rownames(num_means) &lt;- 'mean'\n\nnum_means &lt;- num_means %&gt;% \n  rows_append(\n    apply(unbal_train %&gt;% select_if(is.numeric), 2,sd) %&gt;% \n      t() %&gt;% as.data.frame()\n    )\n\nrownames(num_means) &lt;- c('means', 'sd')\n\n\nunbal_trn_norm &lt;- unbal_train %&gt;% mutate(\n    Administrative = (Administrative - num_means$Administrative[1])/num_means$Administrative[2],\n    Administrative_Duration = (Administrative_Duration - num_means$Administrative_Duration[1])/num_means$Administrative_Duration[2],\n    Informational = (Informational - num_means$Informational[1])/num_means$Informational[2],\n    Informational_Duration = (Informational_Duration - num_means$Informational_Duration[1])/num_means$Informational_Duration[2],\n    ProductRelated = (ProductRelated - num_means$ProductRelated[1])/num_means$ProductRelated[2],\n    ProductRelated_Duration = (ProductRelated_Duration - num_means$ProductRelated_Duration[1])/num_means$ProductRelated_Duration[2],\n    BounceRates = (BounceRates - num_means$BounceRates[1])/num_means$BounceRates[2],\n    ExitRates = (ExitRates - num_means$ExitRates[1])/num_means$ExitRates[2],\n    PageValues = (PageValues - num_means$PageValues[1])/num_means$PageValues[2],\n    SpecialDay = (SpecialDay - num_means$SpecialDay[1])/num_means$SpecialDay[2]\n)\n\nunbal_tst_norm &lt;- unbal_test %&gt;% mutate(\n    Administrative = (Administrative - num_means$Administrative[1])/num_means$Administrative[2],\n    Administrative_Duration = (Administrative_Duration - num_means$Administrative_Duration[1])/num_means$Administrative_Duration[2],\n    Informational = (Informational - num_means$Informational[1])/num_means$Informational[2],\n    Informational_Duration = (Informational_Duration - num_means$Informational_Duration[1])/num_means$Informational_Duration[2],\n    ProductRelated = (ProductRelated - num_means$ProductRelated[1])/num_means$ProductRelated[2],\n    ProductRelated_Duration = (ProductRelated_Duration - num_means$ProductRelated_Duration[1])/num_means$ProductRelated_Duration[2],\n    BounceRates = (BounceRates - num_means$BounceRates[1])/num_means$BounceRates[2],\n    ExitRates = (ExitRates - num_means$ExitRates[1])/num_means$ExitRates[2],\n    PageValues = (PageValues - num_means$PageValues[1])/num_means$PageValues[2],\n    SpecialDay = (SpecialDay - num_means$SpecialDay[1])/num_means$SpecialDay[2]\n)"
  },
  {
    "objectID": "applications-final-project/Final_project_sandbox.html#balancing-the-datasets",
    "href": "applications-final-project/Final_project_sandbox.html#balancing-the-datasets",
    "title": "Final Project Sandbox",
    "section": "Balancing the datasets",
    "text": "Balancing the datasets\n\nrev_part &lt;- df2 %&gt;% filter(Revenue == '1')\nnrev_part &lt;- df2 %&gt;% filter(Revenue == '0')\n\nset.seed(24601)\nsample_nrev_part = sample_n(nrev_part, nrow(rev_part))\nbal_df1 &lt;- rbind(rev_part,sample_nrev_part)\n\n# making test and train sets\nset.seed(24601)\nbal_part &lt;- sample(nrow(bal_df1),0.8*nrow(bal_df1),replace = F)\n\nbal_train1 &lt;- bal_df1[bal_part,]\nbal_test1 &lt;- bal_df1[-bal_part,]"
  },
  {
    "objectID": "applications-final-project/Final_project_sandbox.html#balanced-normalized",
    "href": "applications-final-project/Final_project_sandbox.html#balanced-normalized",
    "title": "Final Project Sandbox",
    "section": "Balanced, Normalized",
    "text": "Balanced, Normalized\nNormalizing the balanced data\n\nbal_num_means &lt;- bal_train1 %&gt;% select_if(is.numeric) %&gt;% colMeans() %&gt;% t() %&gt;% as.data.frame()\n\nbal_num_means &lt;- bal_num_means %&gt;% \n  rows_append(\n    apply(bal_train1 %&gt;% select_if(is.numeric), 2,sd) %&gt;% \n      t() %&gt;% as.data.frame()\n    )\n\nrownames(bal_num_means) &lt;- c('means', 'sd')\n\n\nbal_trn_norm &lt;- bal_train1 %&gt;% mutate(\n    Administrative = (Administrative - bal_num_means$Administrative[1])/bal_num_means$Administrative[2],\n    Administrative_Duration = (Administrative_Duration - bal_num_means$Administrative_Duration[1])/bal_num_means$Administrative_Duration[2],\n    Informational = (Informational - bal_num_means$Informational[1])/bal_num_means$Informational[2],\n    Informational_Duration = (Informational_Duration - bal_num_means$Informational_Duration[1])/bal_num_means$Informational_Duration[2],\n    ProductRelated = (ProductRelated - bal_num_means$ProductRelated[1])/bal_num_means$ProductRelated[2],\n    ProductRelated_Duration = (ProductRelated_Duration - bal_num_means$ProductRelated_Duration[1])/bal_num_means$ProductRelated_Duration[2],\n    BounceRates = (BounceRates - bal_num_means$BounceRates[1])/bal_num_means$BounceRates[2],\n    ExitRates = (ExitRates - bal_num_means$ExitRates[1])/bal_num_means$ExitRates[2],\n    PageValues = (PageValues - bal_num_means$PageValues[1])/bal_num_means$PageValues[2],\n    SpecialDay = (SpecialDay - bal_num_means$SpecialDay[1])/bal_num_means$SpecialDay[2]\n)\n\nbal_tst_norm &lt;- bal_test1 %&gt;% mutate(\n    Administrative = (Administrative - bal_num_means$Administrative[1])/bal_num_means$Administrative[2],\n    Administrative_Duration = (Administrative_Duration - bal_num_means$Administrative_Duration[1])/bal_num_means$Administrative_Duration[2],\n    Informational = (Informational - bal_num_means$Informational[1])/bal_num_means$Informational[2],\n    Informational_Duration = (Informational_Duration - bal_num_means$Informational_Duration[1])/bal_num_means$Informational_Duration[2],\n    ProductRelated = (ProductRelated - bal_num_means$ProductRelated[1])/bal_num_means$ProductRelated[2],\n    ProductRelated_Duration = (ProductRelated_Duration - bal_num_means$ProductRelated_Duration[1])/bal_num_means$ProductRelated_Duration[2],\n    BounceRates = (BounceRates - bal_num_means$BounceRates[1])/bal_num_means$BounceRates[2],\n    ExitRates = (ExitRates - bal_num_means$ExitRates[1])/bal_num_means$ExitRates[2],\n    PageValues = (PageValues - bal_num_means$PageValues[1])/bal_num_means$PageValues[2],\n    SpecialDay = (SpecialDay - bal_num_means$SpecialDay[1])/bal_num_means$SpecialDay[2]\n)"
  },
  {
    "objectID": "applications-final-project/Final_project_sandbox.html#logistic-models-interpretations",
    "href": "applications-final-project/Final_project_sandbox.html#logistic-models-interpretations",
    "title": "Final Project Sandbox",
    "section": "Logistic Models: Interpretations",
    "text": "Logistic Models: Interpretations\n\nLinearity Assumption\n\nlinlog3 &lt;- bal_train1 %&gt;% select_if(is.numeric) %&gt;% \n  mutate(\n    probabilities = predict(logfit3, type = 'response'),\n    logit = log(probabilities/(1-probabilities))\n)\n\n\nlogit_scatter &lt;- lapply(colnames(linlog3),\n       function(col) {\n        ggplot(linlog3,\n                aes(x = .data$logit, y = .data[[col]])) + \n           geom_point() + \n           geom_smooth(method = 'loess') + \n           ggtitle(col)\n       }\n)\n\nlogit_scatter[[1]] + geom_jitter() + logit_scatter[[3]] + geom_jitter() + logit_scatter[[5]]\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nlogit_scatter[[8]] + logit_scatter[[9]]\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n#ggplot(linlog3, aes(logit, predictor.value))+\n#  geom_point(size = 0.5, alpha = 0.5) +\n#  geom_smooth(method = \"loess\") + \n#  theme_bw() + \n#  facet_wrap(~predictors, scales = \"free_y\")\n\nAlmost all of the numeric variables do not have a linear relationship with the logsitic model output, with the exception of Page Value.\n\n\nOutliers/ Influential points\n\nplot(logfit1, which = 4)\n\n\n\n\n\n\n\n\n\nplot(logfit2, which = 4)\n\n\n\n\n\n\n\n\n\nplot(logfit3, which = 4)\n\n\n\n\n\n\n\n\n\nDescTools::HosmerLemeshowTest(fitted(logfit3), bal_train1$Revenue)$C\n\nWarning in Ops.factor(1, obs): '-' not meaningful for factors\nWarning in Ops.factor(1, obs): '-' not meaningful for factors\n\n\n\n    Hosmer-Lemeshow C statistic\n\ndata:  fitted(logfit3) and bal_train1$Revenue\nX-squared = 3052, df = 8, p-value &lt; 2.2e-16\n\n\n\nlog3data &lt;- broom::augment(logfit3) %&gt;% \n  mutate(index = 1:n())\n\n\nggplot(log3data, aes(index, .std.resid)) + \n  geom_point(aes(color = Revenue), alpha = .5) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nwhich(abs(log3data$.std.resid)&gt;3) %&gt;% length()\n\n[1] 15\n\n\n\n\nMulticollinearity\n\ncar::vif(logfit3)\n\n                   GVIF Df GVIF^(1/(2*Df))\nAdministrative 1.296233  1        1.138522\nInformational  1.189551  1        1.090665\nProductRelated 1.350909  1        1.162286\nExitRates      1.177768  1        1.085250\nPageValues     1.122590  1        1.059524\nMonth          1.796772  9        1.033091\nTrafficType    1.711386 11        1.024724\n\n\n\n\nLogistic Assumptions Conclusions\nThe logistic models violate several of the assumptions of a Logistic model, namely that there are several influential outliers and that few of the numeric variables have a linear relationship with the Revenue outcome in logit scale. There may still be a chance to rectify some of these issues, by removing outlier observations or transforming the data."
  },
  {
    "objectID": "applications-final-project/Final_project_sandbox.html#unbalanced-normalized",
    "href": "applications-final-project/Final_project_sandbox.html#unbalanced-normalized",
    "title": "Final Project Sandbox",
    "section": "Unbalanced, Normalized",
    "text": "Unbalanced, Normalized\n\n# decreased the scope of the tuning parameters to make the model run faster. Very slow otherwise.\n#form1 &lt;- Revenue ~ .\n#set.seed(24601)\n#svmtune1 &lt;- tune.svm(form1, data = unbal_train, kernel = 'linear', cost = seq(0.1, 1, by = 0.1))\n\n\n#svmtune1$best.parameters\n\nbest params so far: 0.05 and 0.5\n\n#svmfit1 &lt;- svm(form1, data = unbal_train, kernel = 'linear', cost = svmtune1$best.parameters$cost)\n#summary(svmfit1)\n\n\n#resp_preds$svm_unbal &lt;- predict(svmfit1, newdata = unbal_test, type = 'response')\n\n\n#caret::confusionMatrix(resp_preds$svm_unbal, resp_preds$actual, positive = '1')\n\nUnbalanced data leads to the SVM model performing pretty terribly. It is not bad in accuracy, but favors incidents where a revenue transaction is not performed. Since we do not have the ability to manipulate the probability threshold like was done for logistic regression, the only option to combat this is to use balanced data."
  },
  {
    "objectID": "applications-final-project/Final_project_sandbox.html#balanced-variables-selected",
    "href": "applications-final-project/Final_project_sandbox.html#balanced-variables-selected",
    "title": "Final Project Sandbox",
    "section": "Balanced, Variables Selected",
    "text": "Balanced, Variables Selected\n\nset.seed(24601)\nform1 &lt;- Revenue ~ PageValues + Month + TrafficType + VisitorType + Weekend + Browser + ProductRelated_Duration + Administrative_Duration + Informational_Duration + ExitRates\nbal_svmtune1 &lt;- tune.svm(form1, data = bal_train1, kernel = 'linear', cost = seq(0.1, 2, by = 0.1))\n\n\nbal_svmtune1$best.parameters\n\n   cost\n14  1.4\n\n\nbest cost parameter is 1.9\n\nbal_svmfit1 &lt;- svm(form1, data = bal_train1, kernel = 'linear', cost = bal_svmtune1$best.parameters$cost)\nsummary(bal_svmfit1)\n\n\nCall:\nsvm(formula = form1, data = bal_train1, kernel = \"linear\", cost = bal_svmtune1$best.parameters$cost)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  1.4 \n\nNumber of Support Vectors:  1323\n\n ( 656 667 )\n\n\nNumber of Classes:  2 \n\nLevels: \n 0 1\n\n\n\nresp_preds_bal$svm_bal &lt;- predict(bal_svmfit1, newdata = bal_test1, type = 'response')\n\n\ncaret::confusionMatrix(resp_preds_bal$svm_bal, resp_preds_bal$actual, positive = '1')\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 314  96\n         1  34 320\n                                          \n               Accuracy : 0.8298          \n                 95% CI : (0.8013, 0.8558)\n    No Information Rate : 0.5445          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.6619          \n                                          \n Mcnemar's Test P-Value : 8.793e-08       \n                                          \n            Sensitivity : 0.7692          \n            Specificity : 0.9023          \n         Pos Pred Value : 0.9040          \n         Neg Pred Value : 0.7659          \n             Prevalence : 0.5445          \n         Detection Rate : 0.4188          \n   Detection Prevalence : 0.4634          \n      Balanced Accuracy : 0.8358          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\nsvm_imp &lt;- abs(t(bal_svmfit1$coefs) %*% bal_svmfit1$SV)\n\n\nsvm_imp_df &lt;- data.frame(Variable = colnames(svm_imp), Importance = as.vector(svm_imp))\nsvm_imp_df2 &lt;- svm_imp_df[order(-svm_imp_df$Importance),]\nprint(svm_imp_df2)\n\n                       Variable  Importance\n1                    PageValues 3.377157743\n36                    Browser12 1.302517670\n30                     Browser6 0.686653193\n35                    Browser11 0.647766297\n31                     Browser7 0.603925955\n21                TrafficType20 0.551031944\n37                    Browser13 0.525159444\n10                     MonthNov 0.516928291\n23             VisitorTypeOther 0.512831197\n20                TrafficType13 0.379844904\n27                     Browser3 0.350682401\n19                TrafficType11 0.281648579\n38      ProductRelated_Duration 0.242261827\n4                      MonthMay 0.188485207\n41                    ExitRates 0.175849536\n17                 TrafficType8 0.159264800\n2                      MonthFeb 0.145464505\n11                     MonthDec 0.137083308\n15                 TrafficType5 0.128611868\n18                TrafficType10 0.116888117\n3                      MonthMar 0.104626971\n28                     Browser4 0.098697657\n34                    Browser10 0.092106531\n6                      MonthJul 0.088793246\n12                 TrafficType2 0.082727574\n5                     MonthJune 0.067890453\n40       Informational_Duration 0.063319798\n29                     Browser5 0.062568739\n26                     Browser2 0.059018415\n7                      MonthAug 0.053973689\n25                  WeekendTRUE 0.052142329\n13                 TrafficType3 0.048171876\n24 VisitorTypeReturning_Visitor 0.045816177\n32                     Browser8 0.040772502\n14                 TrafficType4 0.034647319\n8                      MonthSep 0.023999622\n39      Administrative_Duration 0.022263640\n22             TrafficTypeOther 0.020827229\n9                      MonthOct 0.007854841\n16                 TrafficType6 0.001861453\n33                     Browser9 0.000000000\n\n\n\nsvm_imp_df2 %&gt;% \n  #filter(Importance &gt;= 1.295415e-01) %&gt;% \nggplot(aes(y = reorder(Variable, Importance), x = Importance)) +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.y = element_text(size = 7)) +\n  labs(title = \"SVM Variable Importance\", x = \"Variable\", y = \"Importance\")\n\n\n\n\n\n\n\n\n\nsvm_imp_df2 %&gt;% #filter(Variable != 'PageValues') %&gt;% \nggplot(aes(y = reorder(Variable, Importance), x = Importance)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"SVM Variable Importance\", x = \"Variable\", y = \"Importance\") +\n  theme(axis.text.y = element_text(size = 7))"
  },
  {
    "objectID": "applications-final-project/Final_project_sandbox.html#balanced-random-forest",
    "href": "applications-final-project/Final_project_sandbox.html#balanced-random-forest",
    "title": "Final Project Sandbox",
    "section": "Balanced Random Forest",
    "text": "Balanced Random Forest\n\nset.seed(24601)\nrffit2 &lt;- rfsrc(Revenue ~ .,\n                data = bal_train1, \n                importance = TRUE, \n                ntree = 1000)\n\n\nrffit2\n\n                         Sample size: 3052\n           Frequency of class labels: 1560, 1492\n                     Number of trees: 1000\n           Forest terminal node size: 1\n       Average no. of terminal nodes: 336.597\nNo. of variables tried at each split: 5\n              Total no. of variables: 17\n       Resampling used to grow trees: swor\n    Resample size used to grow trees: 1929\n                            Analysis: RF-C\n                              Family: class\n                      Splitting rule: gini *random*\n       Number of random split points: 10\n                    Imbalanced ratio: 1.0456\n                   (OOB) Brier score: 0.10461445\n        (OOB) Normalized Brier score: 0.41845779\n                           (OOB) AUC: 0.93172862\n                      (OOB) Log-loss: 0.33351603\n                        (OOB) PR-AUC: 0.92340393\n                        (OOB) G-mean: 0.85373393\n   (OOB) Requested performance error: 0.14613368, 0.14102564, 0.15147453\n\nConfusion matrix:\n\n          predicted\n  observed    0    1 class.error\n         0 1340  220      0.1410\n         1  226 1266      0.1515\n\n      (OOB) Misclassification rate: 0.1461337\n\n\n\nrffit2$importance\n\n                                all             0             1\nAdministrative          0.038215684  0.1497145992  0.0537644080\nAdministrative_Duration 0.019980648  0.0932057019  0.0124982663\nInformational           0.052569678  0.2669840652  0.0101662283\nInformational_Duration  0.030322579  0.1399392395  0.0205328661\nProductRelated          0.040181630  0.1853833357  0.0272921326\nProductRelated_Duration 0.028040526  0.1484948573 -0.0009473904\nBounceRates             0.020972249  0.0233319190  0.0910223593\nExitRates               0.052599050  0.0181218789  0.2704981924\nPageValues              0.536484563  1.7631194137  1.1088294260\nSpecialDay              0.012478900 -0.0684100927  0.1401955675\nMonth                   0.069626223  0.1502199208  0.2260801556\nOperatingSystems        0.011544456  0.0418197204  0.0198041042\nBrowser                 0.023431681  0.0866016711  0.0384057513\nRegion                  0.010997212  0.0296571517  0.0295148563\nTrafficType             0.020831402  0.0737421070  0.0375312370\nVisitorType             0.017105228  0.1038871555 -0.0144841425\nWeekend                 0.000440454 -0.0006621456  0.0031154571\n\n\n\nrf_probs_bal &lt;- predict(rffit2, newdata = bal_test1)$predicted[,2]\nrf_preds_bal &lt;- as.factor(ifelse(rf_probs_bal &gt;= 0.5, 1, 0))\n\n\nresp_preds_bal$rf &lt;- rf_preds_bal\n\n\ncaret::confusionMatrix(resp_preds_bal$rf, resp_preds_bal$actual, positive = '1')\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 298  53\n         1  50 363\n                                          \n               Accuracy : 0.8652          \n                 95% CI : (0.8389, 0.8886)\n    No Information Rate : 0.5445          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.7284          \n                                          \n Mcnemar's Test P-Value : 0.8438          \n                                          \n            Sensitivity : 0.8726          \n            Specificity : 0.8563          \n         Pos Pred Value : 0.8789          \n         Neg Pred Value : 0.8490          \n             Prevalence : 0.5445          \n         Detection Rate : 0.4751          \n   Detection Prevalence : 0.5406          \n      Balanced Accuracy : 0.8645          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\ndata.frame(importance = rffit2$importance[,1]) %&gt;%\n  tibble::rownames_to_column(var = \"variable\") %&gt;%\n  ggplot(aes(x = reorder(variable,importance), y = importance)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip() +\n     labs(x = \"Variables\", y = \"Variable importance\", title = 'Random Forest Variable Importance: Balanced Response')\n\n\n\n\n\n\n\n\n\ndata.frame(importance = rffit2$importance[,1]) %&gt;%\n  tibble::rownames_to_column(var = \"variable\") %&gt;%\n  filter(variable != 'PageValues') %&gt;% \n  ggplot(aes(x = reorder(variable,importance), y = importance)) +\n    geom_bar(stat = \"identity\")+\n    coord_flip() +\n     labs(x = \"Variables\", y = \"Variable importance\", title = 'Random Forest Variable Importance: Balanced Response')\n\n\n\n\n\n\n\n\nThe problem with balanced:\n\nbal_train1 %&gt;% group_by(Month) %&gt;% summarize(Rev = sum(ifelse(Revenue == '0', 0, 1)), NonRev=n() - Rev)\n\n# A tibble: 10 × 3\n   Month   Rev NonRev\n   &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Feb       2     20\n 2 Mar     146    275\n 3 May     300    444\n 4 June     22     38\n 5 Jul      49     56\n 6 Aug      55     57\n 7 Sep      64     53\n 8 Oct      89     58\n 9 Nov     584    325\n10 Dec     181    234\n\n\nCategorical variable distributions change, and it’s difficult to tell if it is in an even way based on the original distribution.\n\nsandbox_probs &lt;- predict(rffit2, newdata = unbal_test)$predicted[,2]\n\nsandbox_test &lt;- data.frame(actuals = unbal_test$Revenue,\n                           preds = as.factor(ifelse(sandbox_probs &gt;= 0.5, 1, 0)))\n\n\ncaret::confusionMatrix(sandbox_test$preds, sandbox_test$actuals, positive = '1')\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1811   12\n         1  265  378\n                                          \n               Accuracy : 0.8877          \n                 95% CI : (0.8745, 0.8999)\n    No Information Rate : 0.8418          \n    P-Value [Acc &gt; NIR] : 4.517e-11       \n                                          \n                  Kappa : 0.6661          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.9692          \n            Specificity : 0.8724          \n         Pos Pred Value : 0.5879          \n         Neg Pred Value : 0.9934          \n             Prevalence : 0.1582          \n         Detection Rate : 0.1533          \n   Detection Prevalence : 0.2607          \n      Balanced Accuracy : 0.9208          \n                                          \n       'Positive' Class : 1"
  },
  {
    "objectID": "case-study-2/case-study-2-write-up.html",
    "href": "case-study-2/case-study-2-write-up.html",
    "title": "DA 6813 Case Study 2",
    "section": "",
    "text": "1 Executive Summary\nThis study aimed to develop a predictive model for the Bookbinders Book Club (BBBC) to determine which customers were likely to purchase The Art History of Florence following a direct mail campaign. The analysis was conducted on a dataset containing customer demographics, purchasing behavior, and preferences for different book genres. The key variables considered included gender, amount spent on BBBC books, frequency of purchases, and the number of specific genres purchased (e.g., children’s books, cookbooks, art books).\nFour different modeling techniques were evaluated: Linear Regression, LDA, Logit, and support vector machines (SVM). Given that the dependent variable was binary (purchase or no purchase), linear regression was found to be unsuitable as it attempts to predict a continuous outcome rather than a classification, leading to misleading results. Logistic regression and SVM were thus compared for their predictive performance on the unbalanced and balanced datasets.\nInitial logistic regression results on the unbalanced data showed moderate accuracy (65.61%) and balanced accuracy (71.84%) but low sensitivity (17.78%). However, balancing the dataset improved sensitivity and overall predictive performance. Similarly, the SVM model initially underperformed due to the unbalanced nature of the data but saw significant improvement after balancing, with a final accuracy of 73.77%, sensitivity of 65.69%, and specificity of 81.86%.\nThe logit model, while having a lower overall accuracy (65.61%) compared to the LDA model (88.91%), shows a much higher sensitivity in detecting the minority class (79.41% vs. 37.75% for LDA). However, the LDA model excels in specificity (93.89% vs. 64.27% for logit) and achieves a higher Kappa value, indicating better agreement overall. Despite this, the logit model has a higher balanced accuracy (71.84% vs. 65.82%), suggesting a better trade-off between detecting both classes. Ultimately, the logit model is more effective at identifying the minority class, while the LDA model performs better for the majority class and overall accuracy.\nA key insight was that improving sensitivity, even at the cost of specificity, was critical for maximizing revenue. By lowering the decision threshold in the logistic regression model, the sensitivity increased, resulting in a higher proportion of correctly identified purchasers. Despite the reduction in specificity, the model captured a larger number of likely buyers, ultimately leading to greater profit potential compared to a naïve approach.Capturing more buyers is crucial because the low cost of sending mailers is far outweighed by the potential revenue from correctly identifying additional purchasers. Increasing sensitivity ensures that more potential buyers receive offers, boosting the chances of converting them into sales. Even with some false positives, the higher number of actual buyers leads to greater overall profit compared to a more conservative approach that risks missing out on revenue opportunities.\nThe profitability analysis showed that while the logistic regression and SVM models performed better than random guessing, there is still room for improvement. Adjusting model parameters such as the decision threshold and considering alternative models, like LDA, could further enhance profitability by improving the balance between targeting the right customers and controlling campaign costs.\nThe logistic regression model with an optimized threshold provided a solid proof of concept by delivering the best balance of revenue and costs, demonstrating its potential for profitability.\n\n\n2 Problem Statement\nThe task of this case study is to develop a predictive model to classify whether customers of the Bookbinders Book Club (BBBC) will purchase The Art History of Florence following a direct mail marketing campaign. The campaign involved sending a specially produced brochure to selected customers in Pennsylvania, New York, and Ohio, aiming to assess the likelihood of each customer making a purchase (‘yes’) or not (‘no’).\nThe goal is to accurately (or rather accurate enough to maximize profit) predict their likelihood of purchasing ‘The Art History of Florence’ based on various input variables, including demographic factors (such as gender) and past purchasing behaviors, including the total amount spent on BBBC books, the frequency of past purchases, and preferences for different book genres (such as children’s books, cookbooks, do-it-yourself, and art books). These factors are believed to significantly influence the decision to purchase the featured book.\nThe primary objective is to build a classification model that can accurately predict customer purchases, enabling BBBC to target its marketing efforts more effectively. By identifying the most likely purchasers, BBBC can optimize resource allocation, reduce unnecessary mailer costs, and improve the overall conversion rate of its marketing campaigns.\n\n\n3 Additional Sources\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Balanced datasets are critical for support vector machines (SVMs) because they help avoid bias toward the majority class, improving classification accuracy. When trained on imbalanced data, SVMs may favor the dominant class, reducing true positive rates for the minority class and affecting performance metrics like balanced accuracy, MCC, and AUC. Balanced training sets ensure more robust, consistent predictions across both classes, enhancing model reliability in bioinformatics tasks like mutation classification.\nCitation: Wei Q, Dunbrack RL Jr. PLoS One. 2013;8(7) . doi: 10.1371/journal.pone.0067863. ======= Aldelemy, A., & Abd-Alhameed, R. A. (2023). Binary classification of customer’s online purchasing behavior using machine learning. Journal of Techniques, 5(2), 163–186. https://doi.org/10.51173/jt.v5i2.1226\nThis reference highlights the strong performance of logistic regression compared to other models, which supports our conclusion where logistic regression ultimately outperformed other methods\n\n\n\n\n\n\n\norigin/main\n\n\n\n\n\n\n\n\n\n4 Methodology\nThe analysis began with data preparation, where the dataset of 12 variables, both categorical and numeric, was cleaned and transformed. The categorical variable Gender was converted to a binary factor, and the target variable, Choice, which indicated whether a customer purchased The Art History of Florence, was transformed into a binary indicator (1 for purchase, 0 for no purchase). Variables representing different genres of books purchased, such as P_Child, P_Youth, P_Cook, P_DIY, and P_Art, were retained as numeric variables reflecting the number of books purchased in each category.\nExploratory data analysis (EDA) was conducted to examine the distribution and relationships within the data. Histograms and box plots were generated to visualize the distribution of numeric variables such as Amount_purchased, Frequency, and Last_purchase based on the outcome variable Choice. Bar plots were used to explore the frequency of categorical variables like Gender. A correlation matrix was constructed to identify relationships among numeric variables and to detect potential multicollinearity issues.\nThe dataset initially provided was two pre-split sets: one for training and one for testing. However, these datasets were later combined for exploratory data analysis (EDA), correlation analysis, and visualization. The training set contained 80% of the data, and the test set comprised 20%. The combination allowed for comprehensive analysis while ensuring that model evaluation was still conducted on unseen data.\nSeveral modeling techniques were explored, including logistic regression, linear discriminant analysis (LDA), and support vector machines (SVM). Logistic regression was selected as the primary technique due to its suitability for binary classification and its flexibility in optimizing sensitivity and specificity. A stepwise backward selection method, based on the Akaike Information Criterion (AIC), was used to remove insignificant variables and select the most relevant predictors. To address potential multicollinearity, variables with high variance inflation factor (VIF) values were removed.\nModel performance was evaluated using accuracy, sensitivity, and specificity, with results summarized in a confusion matrix. To further address class imbalance in the test set, the decision threshold for classifying customers was adjusted to optimize model performance. By iterating over different threshold values, an optimal cutoff was determined that balanced sensitivity and specificity, enhancing the model’s ability to predict both purchasers and non-purchasers effectively.\nFinally, a profitability analysis was conducted to evaluate the financial impact of the model. The cost of sending mailers and the revenue from book purchases were calculated to determine the overall profit for each modeling approach.\n\n\n5 Data\nThe dataset used for this analysis contains a total of 12 variables across both training and testing sets. These variables represent customer demographics, purchasing behavior, and preferences for various book genres at the Bookbinders Book Club (BBBC). The key target variable is Choice, which indicates whether a customer purchased The Art History of Florence. The data consists of both categorical and numeric variables.\n\n\n\nData summary\n\n\nName\ncombined\n\n\nNumber of rows\n3900\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nChoice\n0\n1\nFALSE\n2\n0: 3296, 1: 604\n\n\nGender\n0\n1\nFALSE\n2\n1: 2633, 0: 1267\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAmount_purchased\n0\n1\n197.59\n95.78\n15\n122\n200\n270\n474\n▅▇▇▃▁\n\n\nFrequency\n0\n1\n12.90\n8.09\n2\n8\n12\n16\n36\n▇▇▅▁▂\n\n\nLast_purchase\n0\n1\n3.12\n2.94\n1\n1\n2\n4\n12\n▇▁▁▁▁\n\n\nFirst_purchase\n0\n1\n22.74\n15.90\n2\n12\n18\n30\n96\n▇▃▂▁▁\n\n\nP_Child\n0\n1\n0.73\n1.03\n0\n0\n0\n1\n8\n▇▁▁▁▁\n\n\nP_Youth\n0\n1\n0.34\n0.63\n0\n0\n0\n1\n5\n▇▁▁▁▁\n\n\nP_Cook\n0\n1\n0.78\n1.05\n0\n0\n0\n1\n6\n▇▁▁▁▁\n\n\nP_DIY\n0\n1\n0.40\n0.70\n0\n0\n0\n1\n4\n▇▃▁▁▁\n\n\nP_Art\n0\n1\n0.37\n0.67\n0\n0\n0\n1\n5\n▇▁▁▁▁\n\n\n\n\n\nA check for missing values was performed (anyNA()), and no missing data was detected, so no further imputation or cleaning steps were necessary in that regard.\nThe variable Observation was removed not due to multicollinearity but because it served as a unique identifier for each record and did not provide any predictive value for the analysis.We then converted categorical variables (Choice and Gender) into factors, and combined the training and testing datasets for further analysis or visualization.\nDuring our exploratory data analysis (EDA), various visualizations were used to examine the distributions and relationships within the dataset. A correlation plot was created to assess the relationships among numeric variables such as Amount_purchased, Frequency, Last_purchase, First_purchase, and the number of different types of books purchased (e.g., P_Child, P_Youth, P_Cook, P_DIY, P_Art). This helped to identify any strong correlations or multicollinearity between the numeric features.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBar plots were used to explore the distribution of categorical variables such as Gender and Choice (purchase or non-purchase). For example, a bar plot was generated to visualize the relationship between gender and purchase behavior, displaying the frequency of purchases and non-purchases among males and females. These visualizations provided insights into the key factors that might influence the likelihood of a customer purchasing a book.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6 Findings\nUpon initial assesment of our SVM on balanced data, the sensitivity of the model was relatively low, but this wasn’t a significant issue given our objective. The model predicted that 160 out of 408 observations would likely purchase the book. To ensure the integrity of our model and data, we applied appropriate transformations and balanced the responses in both the training and test sets. However, since we have no knowledge of the distribution of responses in the actual mailing list audience, we cannot assume that it will be balanced. Therefore, it was important to validate our model’s performance on an unbalanced dataset to ensure it remained effective in real-world scenarios, where the distribution of purchasers and non-purchasers may differ.\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 2060  171\n         1   36   33\n                                          \n               Accuracy : 0.91            \n                 95% CI : (0.8976, 0.9214)\n    No Information Rate : 0.9113          \n    P-Value [Acc &gt; NIR] : 0.6049          \n                                          \n                  Kappa : 0.2062          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.16176         \n            Specificity : 0.98282         \n         Pos Pred Value : 0.47826         \n         Neg Pred Value : 0.92335         \n             Prevalence : 0.08870         \n         Detection Rate : 0.01435         \n   Detection Prevalence : 0.03000         \n      Balanced Accuracy : 0.57229         \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nAfter applying the SVM model to the original unbalanced test dataset, the sensitivity and specificity metrics remained consistent with those observed in the balanced dataset. This outcome is logical because the distribution between positive (purchasers) and negative (non-purchasers) cases only affects the overall prevalence, not the fundamental calculations of sensitivity and specificity. Each metric remained robust regardless of changes in class distribution because they were calculated independently within each class. As a result, we could apply these performance metrics to a hypothetical, unbalanced dataset of random customers.\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1660   70\n         1  436  134\n                                          \n               Accuracy : 0.78            \n                 95% CI : (0.7625, 0.7968)\n    No Information Rate : 0.9113          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.248           \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.65686         \n            Specificity : 0.79198         \n         Pos Pred Value : 0.23509         \n         Neg Pred Value : 0.95954         \n             Prevalence : 0.08870         \n         Detection Rate : 0.05826         \n   Detection Prevalence : 0.24783         \n      Balanced Accuracy : 0.72442         \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nHowever, our overall analysis revealed that the logistic regression outperformed the other models in terms of overall prediction accuracy, particularly after the decision threshold was optimized. By adjusting the threshold, the model’s sensitivity significantly improved, allowing it to correctly identify a larger number of customers who were likely to purchase the featured book. While the support vector machine (SVM) model initially demonstrated poor sensitivity due to the unbalanced nature of the dataset, its performance improved once the data was balanced. The SVM model showed strong specificity, meaning it effectively reduced false positives, but this came at the cost of lower sensitivity. Linear discriminant analysis (LDA) performed similarly to logistic regression but did not achieve the same level of sensitivity as the threshold-optimized logistic model.\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1347   42\n         1  749  162\n                                          \n               Accuracy : 0.6561          \n                 95% CI : (0.6363, 0.6755)\n    No Information Rate : 0.9113          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.1703          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.79412         \n            Specificity : 0.64265         \n         Pos Pred Value : 0.17783         \n         Neg Pred Value : 0.96976         \n             Prevalence : 0.08870         \n         Detection Rate : 0.07043         \n   Detection Prevalence : 0.39609         \n      Balanced Accuracy : 0.71839         \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nHow we gathered these findings were by first using data from the training and test sets, the proportion of people who are expected to purchase the book out of the 50,000 people in the mailing audience is calculated then storing this estimate in a column called ‘newcnt’. We then used the model to estimate how many of the individuals in both the purchasing and non-purchasing groups would be predicted to buy the book called ‘est_targets’.\nThe cost of sending mailers was calculated by multiplying the number of predicted buyers (“est_targets”) by $0.65 (the cost of each mailer) and called ‘mailercst’.\nFor those who are predicted to buy the book, the total cost of the books and overhead (calculated as $15 x 1.45) was estimated. These costs are only applied to those predicted to purchase the book (“newcnt” where Choice == 1), “purchcst” variable. The total revenue from book sales is calculated by multiplying the number of predicted buyers by $31.95 (the price of the book). ‘Revenue’ is only generated when Choice == 1, and ‘Profit’ is calculated by subtracting both the mailer and book purchase costs from the total revenue.\nWe see by summing up the values in the “profit” we get a total expected profit from the mailer campaign. We see the comparison results here:\n\n\n\n  \n\n\n\nOne of the primary challenges in the dataset was the inherent class imbalance, with significantly more non-purchasers than purchasers. To address this, the dataset was balanced by oversampling the minority class (purchasers), which improved the performance of both logistic regression and SVM models, particularly in terms of sensitivity. Even when tested on the original unbalanced dataset, the sensitivity and specificity metrics for both models remained consistent, indicating that the models were robust against changes in class distribution.\nThe analysis also highlighted a trade-off between sensitivity and specificity. Improving sensitivity was crucial for identifying a greater proportion of potential purchasers, which is the primary goal of the direct mail campaign. However, this improvement came at the cost of specificity, meaning that some mailers would be sent to non-purchasers, resulting in false positives. Despite this, the increased sensitivity was considered an acceptable trade-off, as the cost of sending mailers to non-purchasers is relatively low compared to the revenue generated from correctly identified purchasers.\nFinally, the profitability analysis showed that the logistic regression model with an optimized threshold provided the best balance between sensitivity and specificity, leading to the highest potential profit for the campaign. The SVM model, while strong in terms of specificity, identified fewer purchasers overall, limiting its potential revenue generation. This analysis demonstrated that balancing the dataset and fine-tuning the decision threshold were critical steps in maximizing the effectiveness and profitability of the direct mail campaign.\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\nIn a similar study on “Binary Classification of Customer’s Online Purchasing Behavior Using Machine Learning”, the strength of logistic regression compared to other models was also found: “A comparative study of ten classifiers is presented in [18]. Their accuracy indicator, i.e., the area under the curve (AUC), highlighted logistic regression as the best classifier. Naive Bayes, neural network, and support vector machine classifiers followed as runners-up, while decision tree-based classifiers tended to underperform.”\n\n\n7 Conclusion\nIn conclusion, the logistic regression model ultimately performed the best in predicting customer purchases for the mailer campaign.\nBy iterating through different decision thresholds, we identified the optimal threshold that maximized profit, adjusting the threshold down to 0.2 for the logistic model. While the naive method yielded higher revenue by reaching all potential buyers, using a predictive model like logistic regression or LDA with an optimized threshold helped balance mailer costs and capture more actual purchasers. This approach resulted in higher overall profitability by efficiently targeting customers most likely to buy the book, demonstrating that a carefully tuned predictive model provides a more cost-effective solution than the naive approach."
  },
  {
    "objectID": "case-study-4/case-study-4-write-up.html",
    "href": "case-study-4/case-study-4-write-up.html",
    "title": "DA 6813 Case Study 4 Customer Retention",
    "section": "",
    "text": "1 Executive Summary\nThis analysis focused on predicting customer acquisition and retention to optimize marketing strategies and resource allocation. Using the acquisitionRetention dataset, three models—Logistic Regression, Support Vector Machines (SVM), and Random Forest—were evaluated to classify prospects as “acquired” or “not acquired.” Random Forest emerged as the best-performing model, achieving the highest sensitivity (91.2%) while maintaining high accuracy (75%) in the acquisition task, demonstrating its reliability in identifying prospects likely to convert into customers.\nFor retention prediction, Random Forest was again employed to forecast customer duration, leveraging post-acquisition variables such as retention expenditures, purchase frequency, and cross-category buying behavior. Retention expenditures were identified as the most significant predictor, though a diminishing return effect was observed, suggesting the need to optimize spending.\nKey recommendations include exploring additional external factors, refining spending thresholds for retention efforts, and evaluating advanced ensemble techniques to further enhance predictive accuracy. These insights provide actionable guidance for improving customer acquisition and retention strategies, enabling more efficient resource allocation and sustained business growth.\n\n\n2 Problem Statement\nIn today’s competitive market, managing customer acquisition and retention is crucial for maintaining long-term business success. Companies face the dual challenge of predicting which current customers are at risk of ending their relationship and identifying new customers likely to join. Accurate predictions enable firms to allocate resources effectively, targeting at-risk customers with retention strategies and high-potential prospects with acquisition incentives. This study aims to develop predictive models using the acquisitionRetention dataset to forecast customer acquisition and retention, specifically focusing on the likelihood of acquisition and the duration of customer relationships. The analysis will involve building and evaluating models such as Random Forest, Decision Trees, and Logistic Regression to identify significant predictors and improve model performance. Insights gained from this analysis will help optimize marketing strategies, reduce campaign costs, and maximize the efficient use of firm resources.\n\n\n3 Additional Sources\nRandom forests are particularly well-suited for customer acquisition and retention analysis due to their ability to handle complex, non-linear relationships and interactions between variables. Unlike traditional linear models, random forests can automatically capture intricate patterns in customer behavior without requiring explicit specification of these relationships. This capability is crucial when predicting customer retention and acquisition, as it allows the model to identify subtle, high-order interactions between demographic, behavioral, and transactional features that influence customer decisions. Additionally, random forests provide variable importance measures, which help businesses understand the key drivers of customer churn and acquisition, enabling more targeted marketing strategies (Breiman, 2001). The robustness of random forests against overfitting, even in the presence of noisy data, further enhances their reliability in dynamic and uncertain market environments, making them an ideal choice for optimizing customer relationship management.\nBreiman, L. (2001) Random Forests. Machine Learning, 45, 5-32. http://dx.doi.org/10.1023/A:1010933404324\n\n\n4 Data Exploration and Preprocessing\nThe acquisitionRetention dataset is designed to address two key prediction tasks: (1) identifying which prospects are likely to be acquired and (2) forecasting the retention duration for acquired customers. To meet the requirements of these tasks, different subsets of variables are selected to ensure data integrity and prevent leakage.\nTo prepare the data for modeling, we centered and scaled numeric variables when applying SVM and Logistic Regression to ensure comparability and improve model performance. However, for Random Forest, the raw data was used, as this model is not sensitive to feature scaling.\n\n4.0.1 Variables Used to Predict Customer Acquisition\nFor the customer acquisition task, only variables available prior to acquisition were used to avoid data leakage. Post-acquisition variables, such as duration and profit, were excluded because they rely on outcomes that would not be known at the time of prediction. The following variables were used:\n\nacq_exp: Total dollars spent on acquiring a prospect.\nindustry: Indicates whether the prospect operates in the B2B sector.\nrevenue: Annual sales revenue of the prospect’s firm.\nemployees: Number of employees in the prospect’s firm.\n\nInitially, acq_exp_sq (the square of acquisition expenditure) was included to capture potential non-linear effects. However, this lead to issues with multicollinearity, diminishing our ability to understand the importance of the individual variables, relative to one another.\nThese selected variables reflect the financial and business characteristics of each prospect, along with the firm’s investment in acquisition efforts. They provide relevant and actionable insights into the likelihood of acquiring a customer while maintaining the model’s validity and interpretability.\n\n\n\n5 Methodology\nTo conduct the analysis, we evaluated three models—Logistic Regression, Support Vector Machines (SVM), and Random Forest—to classify prospects as “acquired” or “not acquired,” using pre-acquisition variables such as acq_exp, industry, revenue, and employees to avoid data leakage. The dataset was split into training and testing sets, and numeric variables were normalized for SVM and Logistic Regression, while Random Forest was applied to the raw data due to its robustness to scaling. Each model’s performance was assessed using accuracy, precision, recall, and AUC-ROC. Logistic Regression’s assumption of linearity and independence among predictors, as well as SVM’s reliance on normalized inputs and computational demands, limited their suitability for this task. Random Forest outperformed the other models, excelling in handling non-linear relationships, multicollinearity, and interactions, making it the preferred acquisition model. The Random Forest model was then applied to the full dataset to identify acquired customers, creating a subset for those predicted to be acquired. For retention modeling, Random Forest was used again, leveraging its ability to capture complex relationships and interactions among behavioral and post-acquisition variables such as freq, crossbuy, and ret_exp. Again, variables in the dataset that were square terms of other variables were excluded, to ensure proper measures of variable importance, and because Random Forest captures non-linear relationships between variables. Model performance for retention was evaluated using Root Mean Squared Error (RMSE). This approach ensured that the methods were tailored to the task requirements, addressing model assumptions and limitations while maintaining interpretability.\n\n\n\n6 Findings\nThe analysis of customer acquisition and retention utilized Logistic Regression, Support Vector Machines (SVM), and Random Forest models to classify prospects as “acquired” or “not acquired” and to predict retention duration for acquired customers.\n\n6.0.0.1 Customer Acquisition Findings\nRandom Forest demonstrated superior performance in predicting acquisition outcomes compared to Logistic Regression and SVM. While all models achieved comparable accuracy of 74%–75%, Random Forest excelled with the highest sensitivity (91.2%) while maintaining high accuracy (75%), indicating its robustness in identifying acquired customers. SVM and Logistic Regression performed similarly, with slightly lower sensitivity scores of 89.7%, and similar measures of accuracy (74% - 75%). Random Forest’s ability to handle multicollinearity and non-linear relationships, combined with its interpretability via variable importance measures, solidified its position as the optimal model for acquisition predictions.\nIt’s important to note the method of applying the Random Forest model to the full dataset does result in perfect prediction on the training data, because the Random Forest Model is built on Decision Trees it always “knows the answer” for records that it was trained on. This means that the duration dataset will contain mostly correct values for acquisition, which may not be the case when applied in practice and the acqusition information is not known ahead of time.\n\n\n6.0.0.2 Customer Retention Findings\nThe retention duration prediction was conducted using a Random Forest regression model. Key variables influencing retention included ret_exp (retention expenditures), freq (purchase frequency), and crossbuy (number of product categories purchased). Cross-buy behavior emerged as the most important predictor, with Retention Expenditure as a close second, falling short only due to a diminishing return effect revealed by the Partial Dependence Plot. After a certain expenditure level, further increases had minimal impact on retention duration. Additional Partial Dependence Plots were observed for two other variables: frequency and Acquisition Expenditure. While these plots do seem to indicate a non-linear relationship between the variables and duration, their impact was much less pronounced, being variables that ranked lower in variable importance\nPerformance metrics for the retention model indicated robust predictions, with a Root Mean Square Error (RMSE) of 50.89 and a Mean Absolute Error (MAE) of 37.33. This generally indicates that predictions for duration will be, on average, between 37.33 and 50.89 days of the actual value. While the Mean Absolute Percentage Error (MAPE) was high, this can be attributed to the existence of 0-valued durations, which occurred due to the use of the predictions on acquired customer where they were not actually acquired. This underscores the importance of nuanced insights from the model rather than solely relying on error metrics.\n\n\n\nModel Performance Metrics\n\n\nModel\nAccuracy\nSensitivity\nSpecificity\n\n\n\n\nLogistic Regression\n0.74\n0.897\n0.406\n\n\nSVM\n0.75\n0.897\n0.438\n\n\nRandom Forest\n0.75\n0.912\n0.406\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nError Metrics for Predictions\n\n\nMetric\nValue\n\n\n\n\nRMSE\n50.888\n\n\nMAE\n37.332\n\n\nME\n-14.458\n\n\nMAPE\n3021.005\n\n\n\n\n\n\n\n\n7 Conclusion\nThis analysis provided valuable insights into customer acquisition and retention, leveraging predictive models to inform data-driven strategies. Random Forest emerged as the most effective model for both acquisition and retention predictions, demonstrating superior performance in handling complex, non-linear relationships and providing interpretable results through variable importance measures.\nKey findings include the importance of retention expenditures, purchase frequency, and cross-category buying as significant predictors of customer duration. However, the diminishing returns observed in retention expenditures suggest that firms should optimize spending thresholds to maximize cost efficiency. Additionally, the application of Random Forest to classify acquisition outcomes yielded high accuracy and sensitivity, making it a reliable tool for identifying prospects likely to convert into customers.\n\n\n8 Appendix\nIn addition to the primary analyses detailed in the main report, several exploratory and supporting methods were performed to validate conclusions and refine insights. These methods, documented in the attached sandbox file, include:\n\nCorrelation Analysis:\n\nA correlation matrix was used to explore relationships among variables, identifying potential multicollinearity issues. This analysis informed variable selection and preprocessing steps for models like Logistic Regression and SVM, ensuring robust inputs.\n\nExploratory Analysis:\n\nInitial data exploration involved visualizations to understand distributions and relationships among variables. These insights, referenced in the sandbox file, helped highlight key data characteristics such as the skewed retention durations and the variability in acquisition expenditures.\n\nTesting Alternative Models:\n\nSeveral models, including Decision Trees and basic linear models, were tested for comparative purposes. While these models were not selected for the final analysis, their performance helped confirm the reliability of the chosen methods and supported the final conclusions.\n\n\nThese additional analyses, as documented in the sandbox file, provided valuable context and helped validate the predictive models and findings. They reflect the rigor of the analytical approach and the effort to ensure robust, actionable insights."
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Complex Data Exercise",
    "section": "",
    "text": "This exercise will load, process, and explore a text dataset that consists of employee reviews of their current and former employers on LinkedIn. The dataset can be found from Kaggle here.\nStarting with loading our packages, tidyverse for general cleaning, jsonlite to bring in our Json file, and here to make directory referencing easier.\n\npacman::p_load(tidyverse, jsonlite, here, stringr, superml)\n\nNow we will load our data. Json files are not generally square or in a data frame format, but the fromJSON function makes this tremendously easy.\n\nemp_rev &lt;- fromJSON(here('data-exercise', 'employer-reviews.json'))\nhead(emp_rev)\n\n                      ReviewTitle\n1                      Productive\n2                       Stressful\n3 Good Company for Every employee\n4                      Productive\n5                  Non productive\n6                            Good\n                                                                                                                                                                                                                                                                                                                    CompleteReview\n1                                                                                                                                                              Good company, cool workplace, work load little bit higher. Clean environment, disciplined, good cantin, big campus, systematic workflow, lenient but reliable firm.\n2 1. Need to work on boss's whims and fancies 2. Priorities keep changing 3. No regards for work life balance 4. Completing the work before time is stressed too much than completing it well. Involves lots of reworking, blame games; etc. 5. No job boundaries, you will be asked to do any work depending on the requirements.\n3                                                                                                                                                   Good company for every Engineers dream, Full Mediclaim for entired family, Free transport services from company location to home, Township culture for employees,job security.\n4                                                                                                                                                                         I am just pass out bsc in chemistry Typical day at work Mangement Work place good The most enjoyable part of the job I learn more information in company\n5                                                                                                                                                                     Not so fun at work just blame games  Target people and less target at work Paid less  No increment Make you feel low Too much stress  No one understands you\n6                                                                                                                                                                      I working as laboratory technician form last one year in covid19 staff but we are not appreciate of any about awards we also here for work work and work ..\n                                                        URL Rating\n1 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    3.0\n2 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    3.0\n3 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    5.0\n4 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    5.0\n5 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    1.0\n6 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    3.0\n                                            ReviewDetails\n1     (Current Employee)  -  Ghansoli  -  August 30, 2021\n2               (Former Employee)  -   -  August 26, 2021\n3               (Former Employee)  -   -  August 17, 2021\n4              (Current Employee)  -   -  August 17, 2021\n5                (Former Employee)  -   -  August 9, 2021\n6 (Current Employee)  -  Dahej, Gujarat  -  July 22, 2021\n\nstr(emp_rev)\n\n'data.frame':   145209 obs. of  5 variables:\n $ ReviewTitle   : chr  \"Productive\" \"Stressful\" \"Good Company for Every employee\" \"Productive\" ...\n $ CompleteReview: chr  \"Good company, cool workplace, work load little bit higher. Clean environment, disciplined, good cantin, big cam\"| __truncated__ \"1. Need to work on boss's whims and fancies 2. Priorities keep changing 3. No regards for work life balance 4. \"| __truncated__ \"Good company for every Engineers dream, Full Mediclaim for entired family, Free transport services from company\"| __truncated__ \"I am just pass out bsc in chemistry Typical day at work Mangement Work place good The most enjoyable part of th\"| __truncated__ ...\n $ URL           : chr  \"https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews\" \"https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews\" \"https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews\" \"https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews\" ...\n $ Rating        : chr  \"3.0\" \"3.0\" \"5.0\" \"5.0\" ...\n $ ReviewDetails : chr  \"(Current Employee)  -  Ghansoli  -  August 30, 2021\" \"(Former Employee)  -   -  August 26, 2021\" \"(Former Employee)  -   -  August 17, 2021\" \"(Current Employee)  -   -  August 17, 2021\" ...\n\nsummary(emp_rev)\n\n ReviewTitle        CompleteReview         URL               Rating         \n Length:145209      Length:145209      Length:145209      Length:145209     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ReviewDetails     \n Length:145209     \n Class :character  \n Mode  :character  \n\n\nLooking at the columns, we will want to do some cleanup on some of the more categorical ones. Starting with the URL, this may contain information about the employer, which we can extract. First i want to confirm that all the urls start the same way.\n\nsubstr(emp_rev$URL, 1, 26) %&gt;% unique() #substring extracts first 26 characters,\n\n[1] \"https://in.indeed.com/cmp/\"\n\n#unique tells us all of the unique values in the substring'd column\nlength(unique(emp_rev$URL)) #tells us the number of potential company names\n\n[1] 7286\n\n\nNext, I will use some substrings and regex to extract the company name after the above url portion.\n\nd1 &lt;- emp_rev %&gt;% mutate(\n  CompNm = (substr(URL, 27, nchar(URL)) %&gt;% str_extract('.*(?=/)') %&gt;% str_replace_all('-',' '))\n)\n#substring removes the first part of the url, since its always the same at 27 characters\n#str_extract looks for and extracts the first set of characters before the \"/\"\n#str_replace_all removes all of the dashes and replaces them with spaces\nd1$CompNm %&gt;% unique()\n\n [1] \"Reliance Industries Ltd\"         \"Mphasis\"                        \n [3] \"Kpmg\"                            \"Yes Bank\"                       \n [5] \"Sutherland\"                      \"Marriott International, Inc.\"   \n [7] \"DHL\"                             \"Jio\"                            \n [9] \"Vodafoneziggo\"                   \"HP\"                             \n[11] \"Maersk\"                          \"Ride.swiggy\"                    \n[13] \"Jll\"                             \"Alstom\"                         \n[15] \"UnitedHealth Group\"              \"Tata Consultancy Services (tcs)\"\n[17] \"Capgemini\"                       \"Teleperformance\"                \n[19] \"Cognizant Technology Solutions\"  \"Mahindra & Mahindra Ltd\"        \n[21] \"L&T Technology Services Ltd.\"    \"Bharti Airtel Limited\"          \n[23] \"Indeed\"                          \"Hyatt\"                          \n[25] \"Icici Prudential Life Insurance\" \"Accenture\"                      \n[27] \"Honeywell\"                       \"Standard Chartered Bank\"        \n[29] \"Nokia\"                           \"Apollo Hospitals\"               \n[31] \"Tata Aia Life\"                   \"Hdfc Bank\"                      \n[33] \"Bosch\"                           \"Deloitte\"                       \n[35] \"Ey\"                              \"Microsoft\"                      \n[37] \"Barclays\"                        \"JPMorgan Chase\"                 \n[39] \"Muthoot Finance\"                 \"Wns Global Services\"            \n[41] \"Kotak Mahindra Bank\"             \"Infosys\"                        \n[43] \"Oracle\"                          \"Byju's\"                         \n[45] \"Deutsche Bank\"                   \"Hinduja Global Solutions\"       \n[47] \"Ericsson\"                        \"Axis Bank\"                      \n[49] \"IBM\"                             \"Concentrix\"                     \n[51] \"Wells Fargo\"                     \"Google\"                         \n[53] \"Dell Technologies\"               \"Facebook\"                       \n[55] \"Amazon.com\"                      \"Flipkart.com\"                   \n[57] \"American Express\"                \"Citi\"                           \n[59] \"HSBC\"                           \n\n\nThis is cheating a little bit, because I counted the number of characters in the first part of the URL manually, meaning this is not the most robust way to identify the company name, but observing our values it does not look like it caused any problems.\nNext let’s look at the Rating. When the file was read in it looks like it was read as a string, but it would be more useful to us as a number. We’ll start with a quick summary and completeness check.\n\nd1$Rating %&gt;% summary() #summary of variable, understand scope\n\n   Length     Class      Mode \n   145209 character character \n\nsum(d1$Rating=='') + sum(is.na(d1$Rating)) #counts empty and missing values\n\n[1] 0\n\nd1$Rating %&gt;% unique()\n\n[1] \"3.0\" \"5.0\" \"1.0\" \"4.0\" \"2.0\"\n\n\nSo the variable is a string, but does not contain any missing or null values. All of the values fall under the five-point scale, so it should be safe to convert to a number.\n\nd2 &lt;- d1 %&gt;% mutate(\n  Rating = as.numeric(Rating)\n) # converts the Rating variable to numeric and saves it to the same variable.\n\nOne last variable to look at, ReviewDetails. This looks to have three parts to it. The status of the employee, the location, and the date the review was done. I’m most interested in the status for this exercise, but let’s see if we can get all three\n\n#str_split() breaks up the column by the dashes\n#simplify = TRUE turns it into a matrix\n# dim() gives us the number of rows and columns of the matrix, expecting 3 cols\nstr_split(emp_rev$ReviewDetails, '-', simplify = TRUE) %&gt;% dim()\n\n[1] 145209      5\n\n\nThe intention was to split the variable by dashes to create three columns, however it looks like there are some values that contain a dash themselves. This causes two additional columns to appear, so we will have to make some adjustments.\n\n#check for number of columns\nstr_split(emp_rev$ReviewDetails, ' -  ', simplify = TRUE) %&gt;% dim() \n\n[1] 145209      3\n\n#check for number of unique employee statuses\nstr_split(emp_rev$ReviewDetails, ' -  ', simplify = TRUE)[,1] %&gt;% unique() %&gt;% head(10)\n\n [1] \"(Current Employee) \"                                   \n [2] \"(Former Employee) \"                                    \n [3] \"Training   (Former Employee) \"                         \n [4] \"Officer   (Former Employee) \"                          \n [5] \"Leader   (Current Employee) \"                          \n [6] \"health care   (Current Employee) \"                     \n [7] \"Good team worker   (Former Employee) \"                 \n [8] \"Officer   (Current Employee) \"                         \n [9] \"Sr.G.M.Engineering and projects .   (Former Employee) \"\n[10] \"Hospitality   (Former Employee) \"                      \n\n#check for number of empty values\nifelse(str_split(emp_rev$ReviewDetails, ' -  ', simplify = TRUE)[,3] =='',1,0) %&gt;% sum()\n\n[1] 0\n\n\nFortunately, the solution is easier than it first appeared. Originally I was going to approach the split by splitting from the left and the right for Employee Status and Review Date, then removing everything thats in the left and right for location. However, the dashes that split the different details would have two additional spaces after each, so if we include that in the split function we can get the result we are looking for.\nThe Employee Status section fo the Details field looks to have more than just the status for some observations. A quick check might be worth it to see if Employee Title would be worth pursuing.\n\n# splits the columns then checks the first column for the employee status values,\n#then counts those that don't fall into the status value only\nemp_rev %&gt;%\n  mutate(\n    Stat = str_split(ReviewDetails, ' -  ', simplify = TRUE)[,1],\n    Stat = ifelse(Stat %in% c('(Current Employee) ', '(Former Employee) '), 0, 1)\n      ) %&gt;% select(Stat) %&gt;% sum()\n\n[1] 523\n\n\nWith only 523 observations that fall outside of the Current or Former employee status, it’s relatively safe to ignore that part of the ReviewDetails field.\n\n#saves the split column into three new variables.\n#Review Date is tranformed into date format\n#Employee status uses str_extract to get the status vales only\n#Location uses trimws() to remove extrenuous blanks\nd3 &lt;- d2 %&gt;% mutate(\n  ReviewDate = (str_split(ReviewDetails, ' -  ', simplify = TRUE)[,3] %&gt;% \n                  parse_date_time('0m d, y')),\n  EmployeeStatus = (str_split(ReviewDetails, ' -  ', simplify = TRUE)[,1] %&gt;% \n                      str_extract('(Current Employee)|(Former Employee)')),\n  Location = (str_split(ReviewDetails, ' -  ', simplify = TRUE)[,2] %&gt;% \n                trimws())\n  )\n# checks for how many values actually have a location. \n#Primarily to check if the column is worth using\nd3$Location %&gt;% unique() %&gt;% length()\n\n[1] 3780\n\n#Checks to make sure only two values are in the status\nd3$EmployeeStatus %&gt;% unique() %&gt;% length()\n\n[1] 2\n\n\n\n#Null and empty checks for new columns. \n#DatNull does not check for empties because of date format limitation\nd3 %&gt;% summarize(\n  StatNull = sum(EmployeeStatus == '') + sum(is.na(EmployeeStatus)),\n  DatNull = sum(is.na(ReviewDate)),\n  LocNull = sum(Location=='') + sum(is.na(Location))\n)\n\n  StatNull DatNull LocNull\n1        0       0  129942\n\n\nLocation is a pretty empty field, so it can largely be ignored, otherwise our other two variables look great. From here we can move on to the Review text itself.\n\n#lower cases the full review text\nd3 &lt;- d3 %&gt;% mutate(\n  CompleteReview = tolower(CompleteReview)\n)\n\nBefore we do anything we do anything with the reviews, the dataset is huge, and since the next step involves creating a bag of words it would probably be a good idea to filter the dataset. We will pick two companies to filter to as our companies of interest. First let’s look at the number of reviews by company.\n\n#checks the count of reviews by company name\nd3 %&gt;% group_by(CompNm) %&gt;% \n  summarize(\n    cnt = n()\n  ) %&gt;% arrange(-cnt)\n\n# A tibble: 59 × 2\n   CompNm                            cnt\n   &lt;chr&gt;                           &lt;int&gt;\n 1 Tata Consultancy Services (tcs) 14441\n 2 IBM                             10820\n 3 Infosys                         10696\n 4 Accenture                       10137\n 5 Cognizant Technology Solutions   9626\n 6 Hdfc Bank                        6749\n 7 Capgemini                        5248\n 8 Amazon.com                       3385\n 9 L&T Technology Services Ltd.     3226\n10 Concentrix                       3162\n# ℹ 49 more rows\n\n\nLooking at the size, HP and Dell Technologies look pretty reasonable, so we can filter to those two and compare.\n\n#filters to reviews for HP and Dell Technologies, saves to new df\nd4 &lt;- d3 %&gt;% filter(CompNm %in% c('Dell Technologies', 'HP'))\n\nThe next step will create a ‘bag of words’ commonly used for machine learning, but we’re going to use it this time for to get summary information about scores based on the appearance of words.\n\n#initializes the class for CountVectorizer. \n#Only looking at top 100 most frequently used words\ncfv &lt;- CountVectorizer$new(max_features = 100)\n#Transforms the occurence of each word across all reviews into a vector\ncf_mat &lt;- cfv$fit_transform(d4$CompleteReview)\n#transposed for readability\nhead(cf_mat) %&gt;% t()\n\n              [,1] [,2] [,3] [,4] [,5] [,6]\nwork             1    1    1    1    1    0\ngood             1    0    0    4    1    0\nmanagement       0    0    0    1    0    0\ncompany          2    0    1    2    0    0\nplace            0    1    0    0    0    2\nteam             0    0    0    0    0    0\ngreat            0    1    0    0    0    0\nhp               0    1    2    0    1    0\nworking          0    0    0    0    0    1\ndell             0    0    0    0    0    0\njob              0    0    1    0    0    0\nculture          0    0    0    0    0    0\nlife             0    0    0    0    0    0\nenvironment      0    0    0    0    1    0\nlot              0    0    0    0    1    0\npart             0    0    0    0    0    0\nlearn            0    0    0    0    0    2\nbalance          0    0    0    0    0    0\nnew              0    0    0    0    0    0\nfun              0    0    0    0    1    0\nday              0    0    0    0    0    2\nexperience       0    0    0    0    1    1\nbest             0    0    0    0    0    1\ntime             0    0    0    0    0    0\nlearned          0    0    0    0    0    0\nco               0    0    0    0    0    0\nfriendly         1    0    0    2    0    0\nlearning         0    0    0    0    2    0\nemployees        0    0    0    1    0    0\npeople           0    0    0    0    0    0\nemployee         0    0    0    1    0    0\nprocess          0    0    0    0    0    0\nthings           0    0    0    0    0    0\ncan              0    0    0    2    0    0\nworkers          0    0    0    0    0    0\nnice             0    0    0    1    0    0\ns                0    1    0    0    0    0\ncustomer         1    0    0    0    0    1\nwill             0    0    0    1    0    0\nsupport          0    0    0    0    0    1\nalso             0    0    0    0    0    0\none              0    0    0    0    0    0\ncareer           0    1    0    0    0    0\nlike             0    0    0    1    0    0\nget              0    0    2    0    0    0\nmany             0    0    0    0    0    0\ngrowth           0    0    1    0    0    1\nworked           0    0    0    0    1    0\nwell             0    0    0    0    0    0\nskills           0    0    1    0    0    0\nlearnt           0    0    0    0    0    0\nsalary           1    0    0    0    0    0\nevery            0    0    0    0    1    0\nenjoyable        0    0    0    0    0    0\ntraining         0    0    0    0    0    0\nknowledge        0    0    1    0    0    1\ntechnical        0    0    1    0    0    0\nalways           0    0    0    0    0    0\ncustomers        0    0    0    0    0    0\nopportunities    0    0    0    0    0    0\nhardest          0    0    0    0    0    0\nsupportive       0    0    0    0    0    0\nexcellent        0    0    0    0    0    0\nbusiness         0    0    0    0    0    0\nopportunity      0    0    1    0    0    0\nreally           0    1    0    1    0    0\nyears            0    0    0    0    0    0\ndifferent        0    0    0    0    0    0\ngrow             0    0    0    0    0    0\nmanagers         0    0    0    0    0    1\nissues           0    0    0    0    0    0\nactivities       0    0    0    0    1    0\nmuch             0    0    0    0    0    0\ngot              1    0    0    0    0    0\nproject          0    0    0    1    0    0\nhelp             0    0    0    0    0    0\nfirst            0    0    0    0    0    0\nmanager          0    0    0    0    0    0\nclient           0    0    0    0    0    0\npressure         0    0    0    0    0    0\nprofessional     0    0    0    0    0    0\nflexible         0    0    0    0    0    0\nus               0    0    0    0    0    0\ntechnologies     0    0    0    0    0    0\nhelpful          0    0    0    0    0    0\norganization     0    0    0    0    0    0\nenjoyed          0    0    0    0    0    0\nhandling         0    0    0    0    0    0\nworkplace        0    0    0    0    0    0\nbenefits         1    0    0    0    0    0\ntechnology       0    0    0    0    0    0\nsales            0    0    0    0    0    0\noverall          0    0    0    0    1    0\nprojects         0    0    0    0    0    0\nservice          0    0    0    0    0    0\ntypical          0    0    0    0    0    0\namazing          0    0    0    0    0    0\nneed             0    0    0    0    0    0\nhome             0    0    0    0    0    0\nservices         0    0    0    0    0    0\n\n\nNow we combine the bag of words matrix to the dataframe to make summarizing a bit easier\n\n#combines bag of words with orginal data frame\nd5 &lt;- cbind(d4, cf_mat)\n\nWe can take a look at the average score for each word for both companies. Note that the average score is weighted by the number of appearances of a word, that is to say that if a word appears multiple times in a review the score will have a greater weight.\n\n#Multiples the rating by the appearance of each word, then sums that up for each word\n#Then it divides by the total number of appearances of that word\n((d5$Rating * d5[,10:109]) %&gt;% colSums())/(colSums(d5[,10:109]))\n\n         work          good    management       company         place \n     4.239728      4.152515      4.140500      4.208420      4.260095 \n         team         great            hp       working          dell \n     4.221481      4.352092      4.276878      4.249440      4.292874 \n          job       culture          life   environment           lot \n     4.180314      4.321168      4.286114      4.289959      4.220779 \n         part         learn       balance           new           fun \n     4.174620      4.230942      4.242068      4.264916      4.288538 \n          day    experience          best          time       learned \n     4.173973      4.273743      4.497076      4.168182      4.176380 \n           co      friendly      learning     employees        people \n     4.150769      4.305772      4.239370      4.270799      4.191736 \n     employee       process        things           can       workers \n     4.270408      4.169811      4.235741      4.199620      4.158397 \n         nice             s      customer          will       support \n     4.212644      4.127413      4.300797      4.062000      4.235772 \n         also           one        career          like           get \n     4.271967      4.256356      4.267094      4.221729      4.159251 \n         many        growth        worked          well        skills \n     4.176755      4.106436      4.270471      4.308458      4.300752 \n       learnt        salary         every     enjoyable      training \n     4.251256      3.877863      4.318421      4.171123      4.329640 \n    knowledge     technical        always     customers opportunities \n     4.189944      4.247887      4.219373      4.272206      4.262537 \n      hardest    supportive     excellent      business   opportunity \n     4.088496      4.281899      4.489552      4.247678      4.263492 \n       really         years     different          grow      managers \n     4.290657      4.163763      4.184397      4.370107      4.306859 \n       issues    activities          much           got       project \n     4.408397      4.310345      3.923077      4.315175      4.003922 \n         help         first       manager        client      pressure \n     4.360324      4.258621      4.210526      4.136564      4.053097 \n professional      flexible            us  technologies       helpful \n     4.355556      4.247748      4.325792      4.239819      4.303167 \n organization       enjoyed      handling     workplace      benefits \n     4.319444      4.280374      4.202830      4.241706      4.142180 \n   technology         sales       overall      projects       service \n     4.204762      4.328502      4.082927      4.113300      4.336634 \n      typical       amazing          need          home      services \n     4.080808      4.520202      3.953846      4.226804      4.300518 \n\n\nNow lets split up the data by company and see if there are any differences.\n\n#Standard filters saved as new data frames\nDell &lt;- d5 %&gt;% filter(CompNm == 'Dell Technologies')\nHP &lt;- d5 %&gt;% filter(CompNm == 'HP')\n\nWe can use the same logic as before to get the average score by word, but for each of our new dataframes for each company.\n\n#Finds the average score by word for Dell. Saves as dataframe\nDell_scores &lt;- ((Dell$Rating * Dell[,10:109]) %&gt;%\n                  colSums())/ (colSums(Dell[,10:109])) %&gt;% \n  as.data.frame()\n\n#Finds the average score by word for HP. Saves as dataframe\nHP_scores &lt;- ((HP$Rating * HP[,10:109]) %&gt;% \n     colSums())/(colSums(HP[,10:109])) %&gt;% \n  as.data.frame()\n\nDell_scores %&gt;% head()\n\n                  .\nwork       4.210822\ngood       4.118130\nmanagement 4.078498\ncompany    4.172018\nplace      4.226161\nteam       4.212121\n\nHP_scores %&gt;% head()\n\n                  .\nwork       4.264388\ngood       4.181271\nmanagement 4.189117\ncompany    4.238593\nplace      4.292148\nteam       4.229529\n\n\nNow we use these new data frames to check the difference in score for each word. This could be a potential way of identifying certain areas that employees of one company like or dislike more than employees of the other company.\n\n#determines the difference in score each word\nword_scores &lt;- Dell_scores - HP_scores\n#updates the name of the score difference so it can be referenced\ncolnames(word_scores) &lt;- c('score_diff')\n#sorts by the word score\nword_scores &lt;- word_scores %&gt;% arrange(-score_diff)\n#creates a new variable called word, easier to reference than row names\nword_scores$word &lt;- rownames(word_scores)\n\nhead(word_scores)\n\n          score_diff      word\ndell       0.7941176      dell\nfirst      0.2670235     first\nhp         0.2234513        hp\ndifferent  0.1577160 different\nhome       0.1491707      home\nalso       0.1454323      also\n\n\nUltimately we find the scores to not be so different, but we can still see words that tend to “lead” to higher scores than others. Interestingly, a review that includes either of the words Dell or HP would tend to be higher for Dell than HP,\n\n#plots the difference in score by word.\nword_scores %&gt;% head(sum(word_scores$score_diff&gt;0)) %&gt;% ggplot() +\n  geom_col(aes(y=fct_reorder(word, score_diff),x=score_diff)) +\n  labs(y='Word', x='Score Difference', title = 'Dell-HP Score Difference by Word') +\n  theme(axis.text.y = element_text(size=6, angle = 25))\n\n\n\n\n\n\n\n\nThe methodology used here was not terribly robust, so certainly more research could be done. For example using n-grams might get an idea if certain short phrases are more telling, or grouping known “problem” words for one company or another to see if they are mentioned in each others surveys. Other options would be filtering by a score range and seeing what words appear for more positive or negative results."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "",
    "text": "Code\nd1 &lt;- readRDS(here('HYLTIN-PII-project','data','processed-data','processed-crime.rds'))\ntig_zips &lt;- zctas(cb=FALSE, starts_with = c(unique(d1$Zip.Code)), year = 2020, progress_bar = FALSE)\n\ncatYearly &lt;- d1 %&gt;% mutate(\n  years = year(Occurred.Date)\n  #years = as.character(years)\n  ) %&gt;% filter(years != 2024) %&gt;% \n  group_by(Crime.Category, years) %&gt;% \n  summarize(crim.count  = n()) %&gt;% \n  ungroup() %&gt;% \n  as.data.frame()\n\nallYrsCats &lt;- expand.grid(years = unique(catYearly$years), Crime.Category = unique(catYearly$Crime.Category))\n\nyearlyAllCats &lt;- left_join(allYrsCats, catYearly, by = join_by(Crime.Category, years))  %&gt;% \n  mutate(crim.count = ifelse(is.na(crim.count), 0, crim.count))\n\nfig &lt;- plot_ly(yearlyAllCats,\n  x = ~crim.count,\n  y = ~fct_reorder(Crime.Category, crim.count),\n  type = 'bar',\n  showlegend = F,\n  frame = ~years,\n  marker = list(color = '#000067')\n)\n\ncatbarly &lt;- fig %&gt;% \n  layout(\n    yaxis = list(\n      title = '', tickangle = -30, tickfont = list(size = 8)\n      ), \n    xaxis = list(\n      title = 'Count of Occurence' \n    ),\n    title = list(\n      text = 'Yearly Occurence of Crime by Category',\n      y = 0.99, x = 0.1, xanchor = 'left', yanchor = 'top',\n      font = list(\n        size = 18\n      )\n      )\n    )\n\nyearlyCrime &lt;- d1 %&gt;% mutate(\n  Zip.Char = as.character(Zip.Code),\n  years = year(Occurred.Date),\n  years = as.character(years)\n  ) %&gt;% filter(years != 2024) %&gt;% \n  group_by(Zip.Char, years) %&gt;% \n  summarize(crim.count  = n()) %&gt;% \n  ungroup() %&gt;% \n  as.data.frame()\n  #filter(count &gt;= 500) %&gt;%\n  #filter(Zip.Char == '78741') %&gt;%\n  \n\nallYrsZips &lt;- expand.grid(years = unique(yearlyCrime$years), Zip.Char = unique(yearlyCrime$Zip.Char))\n\nyearlyCrimeAll &lt;- left_join(allYrsZips, yearlyCrime, by = join_by(Zip.Char, years))  %&gt;% \n  inner_join(tig_zips, by = join_by(Zip.Char == ZCTA5CE20)) %&gt;% \n  mutate(crim.count = ifelse(is.na(crim.count), 0, crim.count))\n  \ncpeth &lt;- yearlyCrimeAll %&gt;% filter(crim.count &gt;= 50) %&gt;% \n  ggplot() +\n  geom_sf(aes(geometry = geometry, fill = crim.count, frame = years, text = paste(Zip.Char, '&lt;br&gt;', crim.count))) +\n  scale_fill_gradient2(low = \"#E0144C\", mid = '#FFFFFF', high = \"#000067\", midpoint = -5000, \n                       trans = 'reverse') +\n  theme_classic() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.text.y=element_blank(),\n        line = element_blank(),\n        plot.title = element_text(hjust = 0, size = 15)) +\n  labs(fill = 'Crime Count',\n       title = 'Crime by Location Over Time')\n\ncplotly &lt;- cpeth %&gt;% ggplotly() %&gt;% \n  layout(hoverlabel = text) %&gt;% \n  animation_opts(1500, 1) %&gt;% \n  animation_slider(currentvalue = list(visible = FALSE)) %&gt;% \n  animation_button(x = 0, xanchor = \"left\", y = 0, yanchor = \"bottom\")\n\nRecatTable &lt;- d1 %&gt;% \n  filter(year(Occurred.Date) &lt; 2024) %&gt;% \n  mutate(\n  Occur.Years = trunc.Date(Occurred.Date, 'years')\n  ) %&gt;% \n  group_by(Crime.Category, Highest.Offense.Description, Occur.Years) %&gt;% \n  summarize(\n    Count = n(),\n    .groups = 'drop'\n  ) %&gt;% \n  arrange(by = Occur.Years) %&gt;%\n  group_by(Crime.Category, Highest.Offense.Description) %&gt;% \n  summarize(\n    mean = mean(Count),\n    sd = sd(Count),\n    sum = sum(Count),\n    .groups = 'drop'\n  ) %&gt;% \n  gt() %&gt;% \n  tab_header(\n    title = md('**Crime Recategorization Key**')\n    ) %&gt;% \n  cols_label(\n    Crime.Category = md('**New Category**'),\n    Highest.Offense.Description = md('**Original Description**'),\n    mean = md('**Mean Occurrence Across Years**'),\n    sd = md('**Standard Deviation of Yearly Occurrence**'),\n    sum = md('**Total Occurrences**')\n  ) %&gt;% \n  fmt_number(columns = mean:sd, decimals = 1) %&gt;% \n  opt_interactive()"
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#general-background-information",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#general-background-information",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "General Background Information",
    "text": "General Background Information\nLast year, Austin was ranked among the top US cities with a problem in homicide rate [1]. Earlier this year, the claim was made that crime has dropped to a new low since 2020 [2]. The intention of this analysis will be to investigate both claims, and gain a better understanding of Austin’s criminal activity. While these claims will be in mind throughout the steps of the analysis, I intend to approach it from more of an exploratory standpoint, so not to only go out and investigate the claims but really any insights that can be gleaned around Austin criminal activity and how it has changed over time. What the analysis will not cover would be things like causes of the changes in crime, at least not directly. I may identify shifts that could inform the cause, but the goal will not be to drill down into every potential socioeconomic factor and pick the primary drivers, instead simply to observe what crime in the city of Austin looks like and what we might be able to expect going forward. Aside from resources outlined in our course resources, I will also draw from this book [3]. The final output of this project, the manuscript you are reading, is rendered in my github analytics portfolio. The original repo dedicated to just this project can be found here."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#description-of-data-and-data-source",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#description-of-data-and-data-source",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Description of data and data source",
    "text": "Description of data and data source\nMy primary data source will come from here. The data is updated weekly by the Austin Police Department, and each record in the dataset represents an Incident Report, with the highest offense within an incident taking precedence in things like the description and categorization Each Incident can have other offense tied to it, however since each record is a unique incident then only the aforementioned Higehest Offense is the one represented (NOTE: At the time of this writing, 7/31/2024, the dataset has been taken down to be replaced with one that aligns more closely with the FBI National Incident Based Reporting System. The datasets are not one-to-one, so reproducibility would require more than a lift and shift, however some of the methods could still be employed).\nThe raw data is represented by several categorical, location, and time-based variables, many of which have missing values, or are formatted incorrectly for the data type, so they will need to be cleaned or recoded. After cleaning the data (done in a separate file found in this repo) we can take a look a more meaningful look.\n\n\nCode\nskim(d1)\n\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n2461621\n\n\nNumber of columns\n28\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n12\n\n\nDate\n3\n\n\ndifftime\n2\n\n\nnumeric\n9\n\n\nPOSIXct\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHighest.Offense.Description\n0\n1\n3\n48\n0\n436\n0\n\n\nFamily.Violence\n0\n1\n1\n1\n0\n2\n0\n\n\nLocation.Type\n0\n1\n7\n47\n0\n47\n0\n\n\nAddress\n0\n1\n8\n74\n0\n246951\n0\n\n\nAPD.Sector\n0\n1\n2\n5\n0\n14\n0\n\n\nAPD.District\n0\n1\n1\n2\n0\n21\n0\n\n\nPRA\n0\n1\n1\n4\n0\n742\n0\n\n\nClearance.Status\n0\n1\n0\n1\n615856\n4\n0\n\n\nUCR.Category\n0\n1\n0\n3\n1550375\n17\n0\n\n\nCategory.Description\n0\n1\n0\n18\n1550375\n8\n0\n\n\nLocation\n0\n1\n0\n27\n32335\n219842\n0\n\n\nCrime.Category\n0\n1\n4\n29\n0\n33\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Date\n0\n1.00\n2003-01-01\n2024-06-01\n2012-05-28\n7823\n\n\nReport.Date\n0\n1.00\n2002-11-29\n2024-06-02\n2012-06-06\n7825\n\n\nClearance.Date\n348308\n0.86\n2003-01-01\n2024-06-02\n2012-10-17\n7814\n\n\n\nVariable type: difftime\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Time\n0\n1\n0 secs\n86340 secs\n14:25:00\n1440\n\n\nReport.Time\n0\n1\n0 secs\n86340 secs\n14:06:00\n1440\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nIncident.Number\n0\n1.00\n6.031558e+10\n2.896224e+11\n20035.00\n2.005329e+10\n2.010505e+10\n2.017186e+10\n2.024242e+12\n▇▁▁▁▁\n\n\nHighest.Offense.Code\n0\n1.00\n1.689080e+03\n1.218280e+03\n100.00\n6.010000e+02\n1.199000e+03\n2.716000e+03\n8.905000e+03\n▇▅▁▁▁\n\n\nZip.Code\n0\n1.00\n7.873243e+04\n2.510000e+01\n76574.00\n7.871700e+04\n7.874100e+04\n7.875200e+04\n7.875900e+04\n▁▁▁▁▇\n\n\nCouncil.District\n30699\n0.99\n4.960000e+00\n2.840000e+00\n1.00\n3.000000e+00\n4.000000e+00\n7.000000e+00\n1.000000e+01\n▅▇▃▃▅\n\n\nCensus.Tract\n8822\n1.00\n2.453700e+02\n3.363970e+03\n1.00\n1.500000e+01\n2.324000e+01\n3.380000e+02\n9.508000e+05\n▇▁▁▁▁\n\n\nX.coordinate\n0\n1.00\n3.075787e+06\n3.551571e+05\n0.00\n3.108421e+06\n3.117292e+06\n3.126595e+06\n3.231806e+06\n▁▁▁▁▇\n\n\nY.coordinate\n0\n1.00\n9.946761e+06\n1.147895e+06\n0.00\n1.005743e+07\n1.007300e+07\n1.010056e+07\n1.021550e+07\n▁▁▁▁▇\n\n\nLatitude\n32335\n0.99\n3.029000e+01\n8.000000e-02\n30.01\n3.023000e+01\n3.028000e+01\n3.035000e+01\n3.067000e+01\n▁▇▇▂▁\n\n\nLongitude\n32335\n0.99\n-9.773000e+01\n5.000000e-02\n-98.18\n-9.776000e+01\n-9.773000e+01\n-9.770000e+01\n-9.737000e+01\n▁▁▇▂▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Date.Time\n0\n1\n2003-01-01 00:00:00\n2024-06-01 23:46:00\n2012-05-28 23:09:00\n1738386\n\n\nReport.Date.Time\n0\n1\n2002-11-29 05:30:00\n2024-06-02 01:20:00\n2012-06-06 11:15:00\n2169726\n\n\n\n\n\nI preserved quite a few variables from the original dataset, but the primary ones of interest will be Occurred.Date.Time, Zip.Code, Highest.Offense.Description, Category.Description, and Crime.Category. The last few variables pertaining to the type of crime committed are really all rolled into the last one, Crime.Category for the purposes of this analysis. Crime Category is a derived field that categorizes crimes into several different categories for the sake of understanding what crime occurred but without getting flooded with minute details. The categorizations were done manually by myself, and can be seen in the processing file as well as a description of the method performed. The short version is the categories are based on the UCR Crime descriptions given by the FBI when available, and are bucketed similarly from the Highest Offense field via string detection. This brings the number of unique categories in the Highest Offense field from 436 to 32 in the derived field."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#questionshypotheses-to-be-addressed",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#questionshypotheses-to-be-addressed",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Questions/Hypotheses to be addressed",
    "text": "Questions/Hypotheses to be addressed\nUltimately, I would like to explore a few questions:\n\nHas crime truly dropped, or is it expected to rise again as the year progresses?\nHas the homicide rate dropped with crime?\nHas the homicide rate been a problem for long time, or was it truly an emerging problem last year?"
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#schematic-of-workflow",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#schematic-of-workflow",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Schematic of workflow",
    "text": "Schematic of workflow\nThe intention of this analysis is to be exploratory; while I do have some questions I would like to chase down, I intend for them to be more of a compass than a map. With that said, the general strategy taken can be boiled down as follows:\n\nCheck for data quality, understand data representation\nClean the data, address problems identified in previous step, some exploration\nMore “formal” exploration. Consider what variables might be most important, and how those variables can be represented in the analysis.\nRepeat any steps performed above as necessary.\nResolve remaining or initial questions using statistical methods."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#exploratorydescriptive-analysis",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#exploratorydescriptive-analysis",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Exploratory/Descriptive analysis",
    "text": "Exploratory/Descriptive analysis\n\nA time series plot is a very natural starting part for the exploration of this analysis. given that the data is at the incident level, it is worth the effort to aggregate the count of occurrences up to some interval of time. We have values of all the way down to reported time of occurrence, but given the imputations that needed to occur in the data cleaning step, as well as the questionable reliability of whether the reported occurrence time is accurate to begin with, daily seems like the smallest interval of time that could be worth aggregating over. Figure 1 below is the corresponding aggregated time series plot. While we can see a general trend, the plot is rather dense since there are so many points plotted along the x-axis. If we want to do anything like isolating down to specific crimes like homicide, we will needed to aggregate at less fine of a grain, otherwise we will have too many intervals with 0 values for the observations.\n\n\n\n\n\n\n\n\nFigure 1: Crime Forecast Residual Plots\n\n\n\n\n\nFigure 2 below is the corresponding time series plot aggregated over each month. We still see a similar rise and fall trend like we did in the daily plot, but it is more pronounced since there are fewer outliers like there were in the daily plot. Outside of the overall trend, we can also see some potential patterns like a seasonal trend, and a change in the overall trend around 2018 until around 2020 where it briefly increases again before returning to a general decrease.\n\n\n\n\n\n\n\n\nFigure 2: Crime Forecast Residual Plots\n\n\n\n\n\nJumping ahead just a bit, after performing the simple lexicon based classification and properly identifying murder crimes, we can isolate to those crimes specifically for a time series plot. Figure 3 is exactly this, and we can see the trend is very different from what is seen in the overall time-series plot. For the most part, the rate of murder is relatively low up through 2015, and it is around 2016 that we see it spike and then slowly increase up until about 2022, where it then starts to slow back down.\n\n\n\n\n\n\n\n\nFigure 3: Monthly Murder Line Plot\n\n\n\n\n\nThese changes are interesting, primarily because we see the same shift before 2020 across overall crime and murder, but the number of murders is too small to be the primary driver in the overall crime trend. It’s also interesting because of the way the increase in murders start happening before the overall increase, and continues after the overall has started to decrease again. These may be caused by correlated factors like population, economic, or political factors, that the rate of murder could be more sensitive to than other crimes are. Furthermore, because the rate of murder is so low (relative to other types of crime), it’s likely that the variance in the trend is always going to be more sensitive to external factors. Once again the goal of this analysis is not intended to explain the why, but I think these observations could make for some good future studies into the potential causes or correlations. For now, the main question that these visuals bring me are the apparent trends; are they actually material, is the decrease seen the result of a change in the actual rate, or merely happenstance and we can expect the rate of murder to increase in future months? Before I attempt to explore this question, I want to see what variables might help me to explain it. The last chart considers the type of crime, which we will see in a moment, but it does not consider the location. For location, I have created a visualization based on Zip Code that I think can be revealing.\n\n\nCode\ncplotly\n\n\n\n\n\n\n\nThe interactive plot above shows us the number of crimes that occurred each year within each zip code, with the number of crimes represented by the continuous color scale, and the years represented by the animation so that values will change as the animation plays. We can see the overall decrease in crime from the way the Zip Codes that are more red slowly start to shift to blue or white. The zip codes that are more blue to start with don’t seem to change much as the animation plays. This suggests that the bulk of the crime is isolated to a handful of locations, and either factors that typically lead to crime are becoming less relevant in those areas, or the areas with heavier crime are seeing a greater focus in terms of prevention. This visualization is great, but with the analysis being more geared for a focus on Murder, it is unlikely to be of much help. There are probably Zip Codes where murder is more likely to occur, but the lower quantities would make it difficult to use that as a method to predict the number of murders that will happen in an interval of time small enough to have enough observations to make such a prediction in the first place. This chart does leave some questions that would be good for a future study, namely what has changed in those Zip Codes that are decreasing? Are there specific crimes that we are seeing fewer occurrences of, or is there better prevention in those areas?\nThe exploration has considered time and location as a variable, but the type of crime is still largely unexplored. The raw data, as stated before, has 434 different descriptions for the highest offense in each incident that was committed over the time period observed. The number of these descriptions makes it difficult to do anything with as a category, but coupled with other variables in the datset like the UCR Category may help us understand a better categorical representation. To accomplish this, crime that already had a UCR description for the observation was assigned this description as its category. A general description of each of these and several others can be found here[4]. Observations in this dataset were given a UCR Category code when they were considered a more serious crime identified by the FBI (this was given in the data dictionary that came with the data. The data dictionary is no longer available since the dataset was removed, but can be found in the repo for this analysis on github in the assets folder, or from the processing file directly since I have it copied there). Because there were many observations that were not given a category in the variable field, a lexicon was built by observing each individual Offense descriptions to find a way to group each offense description into a smaller categorical variable. Make no mistake, this categorization is not official categorization of the crimes themselves, nor is it intended to be a one for one match to any other categorization used by either APD or the FBI. However, it can be interpreted as one person’s layman’s terms attempt at categorizing these offenses. Essentially, how might an average person who does not regularly encounter law enforcement or crime think of each of these offenses? With that in mind, after observing each of the Highest Offense description, key words and phrases were identified that could capture either unique or similar (when possible) descriptions of the crimes, then string detection and regular expressions were used to identify observations with those descripions and then group them into a category of other similar offenses. We can see exactly how each offense description was grouped in the table below.\n\n\nCode\nRecatTable\n\n\n\n\n\nCrime Recategorization Key\n\n\n\n\n\n\n\n\nThe total count, yearly mean, and yearly standard deviation of occurrences of the offenses is provided to illustrate the disparity of the frequency of several of the offense descriptions. This is not entirely resolved by the New Category, but it does help for many of them. Again, the new categories are meant to generalize the descriptions. Page 21 of the table above contains the descriptions that I categorized as murder. A good example of how this method is imperfect is there, because of the presence of manslaughter. My initial thought process was to group instances of manslaughter into its own category, which is ultimately where I would have put Justified Homicide as well, however there were occurrences of manslaughter that were already categorized as murder by the UCR categorization, so bringing in those to the murder category would leave Justified Homicide in its own category, despite being a crime with a relatively low rate of occurrence. Ultimately this turned out to be the right call, because the FBI UCR website referenced earlier states that the “program classifies justifiable homicides separately,”[4] but for other cases not explicitly stated on this or other sites I may have just gotten it wrong judging by my first instinct. All said, a more summarized version of this chart is below, complete with sparklines as well to give an idea of the rate over time of each category.\n\n\nThis table is intended to give a general idea of the change of each crime over time and the general quantity relative to others. We can again see that our specific crime of focus, Murder, is lower than many other types, but is far from the lowest and still has a quantifiable rate. The sparklines don’t do the difference between crimes justice, because there are several that have a far greater rate of occurrence than others, but we can visualize this and the year over year change with the chart below.\n\n\nCode\ncatbarly\n\n\n\n\n\n\n\nAgain, the rate of murder is generally so low that it is dwarfed by the occurrences of most other crimes (particularly theft). We can see the bar chart peak out towards the end in 2021 and beyond, which is exactly what we expect. Overall, the rate of murder is so low compared to others that we may not be able to give it the same treatment as others, but begs the question if the rate of crime as a whole could somehow be used to help us predict each individual type of crime. We will use this idea during the Forecasting steps described below."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#time-series-and-forecasting",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#time-series-and-forecasting",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Time Series and Forecasting",
    "text": "Time Series and Forecasting\n\nTo determine if the recent decreases seen in crime overall and murder are coincidence or not, we need to establish what our expectation is. There are many ways we could do this, but a forecast seems the best, given the time series nature of our data. A few different time series models were attempted, like ARIMA and Exponential Smoothing, but the confidence interval for these was too wide for any group to make any reasonable inference with. However, utilizing the Fable package, a top-down hierarchical Seasonal ARIMA model is fit to the data and gives us a much more reasonable fit, both for the aggregated level and for the individual crime level for Murder. To fit the model the specific model, automatic model selection in the fable package was utilized while reconciling with a top-down hierarchical method. The selected model for the aggregate level is an ARIMA(0,1,2)(2,1,1)12. We can see from the residual plots for All Crime that the fit is generally fair. Innovation residuals are mostly centered around 0, the ACF plot suggests no autocorrelation, and even the distribution of the residuals is pretty normal, though there is a high frequency at 0 and just below 0 that may be questionable. The Ljung-Box test was not found to be significant, meaning we can reject the null hypothesis that the residuals are distinguishable from white noise agreeing with our observations from the ACF plot. The most questionable aspect here is likely the heteroscedasticity of the residuals, which appears to decrease and then increase over time, but this is not a requirement for a good time series model.\n\n\n\n\n\n\n\n\nFigure 4: Crime Forecast Residual Plots\n\n\n\n\n\n\n\nIt is not quite the same story regarding the residuals for murder. The selected model for the frequency of murder is ARIMA(0,1,1)(0,0,1)12. The plot of the residuals over time is still mostly centered at zero but the variance is less steady than it was for all crime, increasing quite a bit in the last few years, which makes sense since that is when we started to see the rate of Murder increase. Two spikes in the ACF plot are found to be significant, but this is not terribly abormal since 24 spikes are plotted, and the Ljung-Box test results are also not found to be significant. The distribution of the residuals is the most questionable, it is fairly asymmetrical so it is unlikely to be normally distributed, but again this is less important than the requirements for uncorrelated and mean 0 residuals.\n\n\n\n\n\n\n\n\nFigure 5: Murder Forecast Residual Plots\n\n\n\n\n\n\n\nThe plots below show these forecasts, the aggregation all crime and for murder, as well as two other crimes picked for their similarity to murder, Aggregated Assault and Unlawful Restraint (kidnapping and similar crimes). Aggravated Assault was chosen due to its similarity in how the crime is performed, and Unlawful Restraint was chosen due to the number of occurrences being similar to Murder. The plot for Murder is repeated enlarged for readability.\n\n\n\n\n\n\n\n\nFigure 6: Monthly Crime Forecast\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Murder Forecast\n\n\n\n\n\nFrom these forecasts, we can see that for overall crime the decrease observed earlier is expected to continue, however the confidence interval is quite wide so it could just as easily change course and begin to increase . For Murder, the decrease after the observed period is much less pronounced, but the confidence interval is much smaller . The difference in confidence interval is likely due to the smaller magnitude of crime counts compared to the aggregated model, and in this case the bounds are often not enough for more than one integer value between . That’s somewhat problematic early on, because it means the forecast is truly only predicting one value since it is a count, but it matters very little since the last value is well outside of any of the confidence bounds . The table below shows the point estimates of the forecast versus the exact value, and we can see that the the estimates were not terribly bad except for March, where we see the large dip in the plot . It is worth noting that April and May were not plotted since the article that spawned the question at the heart of this analysis was only observing the count of murders through the first quarter of the year ."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#interpretation-and-conlusions",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#interpretation-and-conlusions",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Interpretation and Conlusions",
    "text": "Interpretation and Conlusions\n\nThe findings in the analysis are somewhat inconclusive, though they were really intended to be exploratory. There are some clear trends in crime rate over time, and forecasting methods do not look to be beyond the realm of possibility but should should certainly be taken with a grain of salt. There was a drop in the count of Murders in the first quarter, primarily due to a drop in the month of March. The forecast suggested it was expected to be higher, but we are talking about a difference of 2.7 actual vs forecast. The counts of murder are so low that there is bound to be error, and if not it would likely be due confidence intervals so wide they would be almost meaningless. with that in mind, the table above suggests that we have 5 fewer murders in the first 5 months of 2024 than expected, so cautious optimism may be warranted.\nThe model itself is certainly not perfect, and perhaps future analyses could explore ways to improve the predictability. Hierarchical time series models can have multiple levels in their hierarchy, so it’s not impossible that other variables could have been combined with the category to improve results. Other crimes may also be more worthwhile in predicting; theft as an example dwarfed all other crimes and may very well be the biggest opportunity for the city of Austin. Models built on other higher frequency crimes can also likely make better use of location variables as well. More recent crime reporting data on the Austin data portal site have different methods of classification, and include individual charges with each incident, so it seems that they would be ripe for new variables to be implemented. Socio-economic variable could also potentially be used to see the impacts that they have on the rates of crime as whole or certain specific crimes. Comparisons with other cities would also be interesting, especially those with similarities to Austin to see how individual differences could lead to changes in the city’s environment. These are all ideas I could see myself returning to this analyses for, and I hope this analyses can provide myself or anyone else some inspiration for a more detailed analysis."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "For this exercise we are tasked with recreating a high-quality publication level chart found online using R and various AI tools. I have chosen to recreate the area chart on this page. Since this is coming from FiveThirtyEight and the original plot is interactive I have decided to recreate the plot with Highcharter.\nFirst we are tasked with requesting AI (in this case I have chosen to use ChatGPT) to attempt to recreate the plot, which I do with my initial prompt here:\n  Can you provide code using Highcharter in R that would recreate the area plot titled “Baby boomers are the biggest generation in Congress today” on this page https://fivethirtyeight.com/features/aging-congress-boomers/\n  raw data can be found here: https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\nThe first round of code had a handful of problems, primarily with processing the data, but it came pretty close in terms of structuring the overall request. I provided the column names in the raw data to help with processing, as well as correcting things like unnecessary filters. One of the outputs from ChatGPT ended up with pivoted data, which was not as easy to use for what we were doing, but the unpivoted data was still pretty close so I modified it myself and fed that back as a prompt.\nI’m still having problems with the highcharter section of code. I’ve modified the processing code slightly to adjust the pct values as well, because it did not look like it was previously giving the correct values for pct. I’ve also created a year variable to match what is done in the chart. Here is the processing code after my modifications: data1 &lt;- data %&gt;% mutate(year = as.integer(str_sub(start_date, 1, 4))) %&gt;% group_by(year, generation) %&gt;% summarise(count = n()) %&gt;% ungroup() %&gt;% group_by(year) %&gt;% mutate(pct = count / sum(count) * 100) Can you utilize this dataframe without pivoting to create the chart in Highcharter?\nThe output from this prompt got me most of the way there. The code runs, but some of the generations were out of order, and there were several tweaks needed to get the chart to look like the one on the page. Still the bones were there, so I took over from this point on, aside from some one off prompts to get exactly what I was looking for, with an example below.\nCan you adjust the tooltip in the chart to have the following format: Generation Year Percent (in percent format 00.0%) So for example: Missionary 1937 44.9%\nThe code and output below is the final product after the combined efforts of ChatGPT and myself.\n\n# Load libraries\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(highcharter)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n# Read the data from the URL\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\"\ndata &lt;- read_csv(url)\n\nRows: 29120 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): chamber, state_abbrev, bioname, bioguide_id, generation\ndbl  (6): congress, party_code, cmltv_cong, cmltv_chamber, age_days, age_years\ndate (2): start_date, birthday\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Calculate percentage of each generation in each congress\ndata1 &lt;- data %&gt;%\n  mutate(year = as.integer(str_sub(start_date, 1, 4))) %&gt;% \n  group_by(year, generation) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(year) %&gt;% \n  mutate(pct = count / sum(count) * 100)\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\ngen_order &lt;- data %&gt;% mutate(byear = as.integer(str_sub(birthday, 1, 4))) %&gt;% group_by(generation) %&gt;% summarize(oldyr = min(byear)) %&gt;% arrange(by = oldyr) %&gt;% select(generation)\n\n\ndata1$generation &lt;- fct_rev(fct_relevel(data1$generation, as.vector(gen_order)))\n\nWarning: Outer names are only allowed for unnamed scalar atomic inputs\n\n\n\ngeneration_colors &lt;- c(\n  \"#D26E8C\",\n  \"#92DCE0\",\n  \"#A593E9\",\n  \"#FD867E\",\n  \"#FDE384\",\n  \"#86D09D\",\n  \"#8B8887\",\n  \"#E98CCA\",\n  \"#FFE7E5\",\n  \"#E9E9E9\"\n)\n\n\nhighchart(type = \"chart\") %&gt;%\n  hc_chart(type = \"area\") %&gt;%\n  hc_title(text = \"&lt;b&gt;Baby boomers are the biggest generation in Congress today&lt;/b&gt;\",\n           align = 'left',\n           style = list(fontSize = \"24px\",\n                        fontFamily = 'Calibri')) %&gt;%\n  hc_subtitle(text = 'Share of members in Congress from each generation, 1919 to 2023',\n              align = 'left',\n              style = list(fontSize = \"20px\",\n                           fontFamily = 'Calibri')) %&gt;% \n  hc_xAxis(gridLineWidth = 0,\n    labels = list(\n    style = list(fontSize = \"16px\")\n    )\n    ) %&gt;%\n  hc_yAxis(max = 100,\n           labels = list(\n      formatter = JS(\"function() {\n                      if (this.value == 100) {\n                        return this.value + '%';\n                      } else {\n                        return this.value;\n                      }\n                    }\"),\n      style = list(fontSize = \"16px\")\n           )\n      ) %&gt;%\n  hc_add_series(data1, \"area\", hcaes(x = year, y = pct, group = generation),\n                color = generation_colors) %&gt;%\n  hc_legend(align = \"left\",\n            verticalAlign = \"top\",\n            layout = \"horizontal\",\n            itemStyle = list(fontFamily = 'Copperplate Gothic Light', fontSize = \"12px\")) %&gt;%\n  hc_tooltip(\n    headerFormat = \"&lt;b&gt;{series.name}&lt;/b&gt;&lt;br/&gt;\",\n    pointFormat = \"{point.x}&lt;br/&gt;{point.y:.1f}%\",\n    crosshairs = TRUE\n  ) %&gt;%\n  hc_plotOptions(\n    area = list(\n      stacking = \"normal\",\n      marker = list(enabled = FALSE,\n                    states = list(\n                      hover = list(\n                        enabled = FALSE\n                        )\n                      )\n                    )  # Disable markers\n      ),\n    series = list(\n      lineColor = '#EDF6E9',\n      trackByArea = TRUE,\n      stickyTracking = FALSE\n    )\n  ) %&gt;% hc_add_theme(hc_theme_538())\n\n\n\n\n\nI believe my output is pretty close to the original, with the exception of a few things like fonts, and the hover animations. I could not figure out how to get the crosshair for the tooltip to stay inside of the relevant generation group, nor could I figure out how to change the shape of the color indicators in the legend from circular to square, but even without those changes it still like it was high quality and close enough."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#publication-quality-chart",
    "href": "presentation-exercise/presentation-exercise.html#publication-quality-chart",
    "title": "Presentation Exercise",
    "section": "",
    "text": "For this exercise we are tasked with recreating a high-quality publication level chart found online using R and various AI tools. I have chosen to recreate the area chart on this page. Since this is coming from FiveThirtyEight and the original plot is interactive I have decided to recreate the plot with Highcharter.\nFirst we are tasked with requesting AI (in this case I have chosen to use ChatGPT) to attempt to recreate the plot, which I do with my initial prompt here:\n  Can you provide code using Highcharter in R that would recreate the area plot titled “Baby boomers are the biggest generation in Congress today” on this page https://fivethirtyeight.com/features/aging-congress-boomers/\n  raw data can be found here: https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\nThe first round of code had a handful of problems, primarily with processing the data, but it came pretty close in terms of structuring the overall request. I provided the column names in the raw data to help with processing, as well as correcting things like unnecessary filters. One of the outputs from ChatGPT ended up with pivoted data, which was not as easy to use for what we were doing, but the unpivoted data was still pretty close so I modified it myself and fed that back as a prompt.\nI’m still having problems with the highcharter section of code. I’ve modified the processing code slightly to adjust the pct values as well, because it did not look like it was previously giving the correct values for pct. I’ve also created a year variable to match what is done in the chart. Here is the processing code after my modifications: data1 &lt;- data %&gt;% mutate(year = as.integer(str_sub(start_date, 1, 4))) %&gt;% group_by(year, generation) %&gt;% summarise(count = n()) %&gt;% ungroup() %&gt;% group_by(year) %&gt;% mutate(pct = count / sum(count) * 100) Can you utilize this dataframe without pivoting to create the chart in Highcharter?\nThe output from this prompt got me most of the way there. The code runs, but some of the generations were out of order, and there were several tweaks needed to get the chart to look like the one on the page. Still the bones were there, so I took over from this point on, aside from some one off prompts to get exactly what I was looking for, with an example below.\nCan you adjust the tooltip in the chart to have the following format: Generation Year Percent (in percent format 00.0%) So for example: Missionary 1937 44.9%\nThe code and output below is the final product after the combined efforts of ChatGPT and myself.\n\n# Load libraries\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(highcharter)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n# Read the data from the URL\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\"\ndata &lt;- read_csv(url)\n\nRows: 29120 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): chamber, state_abbrev, bioname, bioguide_id, generation\ndbl  (6): congress, party_code, cmltv_cong, cmltv_chamber, age_days, age_years\ndate (2): start_date, birthday\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Calculate percentage of each generation in each congress\ndata1 &lt;- data %&gt;%\n  mutate(year = as.integer(str_sub(start_date, 1, 4))) %&gt;% \n  group_by(year, generation) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(year) %&gt;% \n  mutate(pct = count / sum(count) * 100)\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\ngen_order &lt;- data %&gt;% mutate(byear = as.integer(str_sub(birthday, 1, 4))) %&gt;% group_by(generation) %&gt;% summarize(oldyr = min(byear)) %&gt;% arrange(by = oldyr) %&gt;% select(generation)\n\n\ndata1$generation &lt;- fct_rev(fct_relevel(data1$generation, as.vector(gen_order)))\n\nWarning: Outer names are only allowed for unnamed scalar atomic inputs\n\n\n\ngeneration_colors &lt;- c(\n  \"#D26E8C\",\n  \"#92DCE0\",\n  \"#A593E9\",\n  \"#FD867E\",\n  \"#FDE384\",\n  \"#86D09D\",\n  \"#8B8887\",\n  \"#E98CCA\",\n  \"#FFE7E5\",\n  \"#E9E9E9\"\n)\n\n\nhighchart(type = \"chart\") %&gt;%\n  hc_chart(type = \"area\") %&gt;%\n  hc_title(text = \"&lt;b&gt;Baby boomers are the biggest generation in Congress today&lt;/b&gt;\",\n           align = 'left',\n           style = list(fontSize = \"24px\",\n                        fontFamily = 'Calibri')) %&gt;%\n  hc_subtitle(text = 'Share of members in Congress from each generation, 1919 to 2023',\n              align = 'left',\n              style = list(fontSize = \"20px\",\n                           fontFamily = 'Calibri')) %&gt;% \n  hc_xAxis(gridLineWidth = 0,\n    labels = list(\n    style = list(fontSize = \"16px\")\n    )\n    ) %&gt;%\n  hc_yAxis(max = 100,\n           labels = list(\n      formatter = JS(\"function() {\n                      if (this.value == 100) {\n                        return this.value + '%';\n                      } else {\n                        return this.value;\n                      }\n                    }\"),\n      style = list(fontSize = \"16px\")\n           )\n      ) %&gt;%\n  hc_add_series(data1, \"area\", hcaes(x = year, y = pct, group = generation),\n                color = generation_colors) %&gt;%\n  hc_legend(align = \"left\",\n            verticalAlign = \"top\",\n            layout = \"horizontal\",\n            itemStyle = list(fontFamily = 'Copperplate Gothic Light', fontSize = \"12px\")) %&gt;%\n  hc_tooltip(\n    headerFormat = \"&lt;b&gt;{series.name}&lt;/b&gt;&lt;br/&gt;\",\n    pointFormat = \"{point.x}&lt;br/&gt;{point.y:.1f}%\",\n    crosshairs = TRUE\n  ) %&gt;%\n  hc_plotOptions(\n    area = list(\n      stacking = \"normal\",\n      marker = list(enabled = FALSE,\n                    states = list(\n                      hover = list(\n                        enabled = FALSE\n                        )\n                      )\n                    )  # Disable markers\n      ),\n    series = list(\n      lineColor = '#EDF6E9',\n      trackByArea = TRUE,\n      stickyTracking = FALSE\n    )\n  ) %&gt;% hc_add_theme(hc_theme_538())\n\n\n\n\n\nI believe my output is pretty close to the original, with the exception of a few things like fonts, and the hover animations. I could not figure out how to get the crosshair for the tooltip to stay inside of the relevant generation group, nor could I figure out how to change the shape of the color indicators in the legend from circular to square, but even without those changes it still like it was high quality and close enough."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#publication-quality-tables",
    "href": "presentation-exercise/presentation-exercise.html#publication-quality-tables",
    "title": "Presentation Exercise",
    "section": "Publication Quality Tables",
    "text": "Publication Quality Tables\nUsing the same dataset, I wanted create a quality table that would show what the congressional membership distribution but include things like the mean age and count of members. The chart above does a great job of showing the relative distribution and the change over time, but the actual age is still an important factor. For example, we may have a lot of Congress members from the Baby Boomer generation in today’s Congress, but do they themselves skew younger or older?\nI decided I would use GT tables. I can’t say i have any experience with gt tables, so I thought this might be a challenge, but I was intrigued by the ability to use ggplots as images in the tables themself. I started off with ChatGPT again with the following prompt, continued off the prompts from teh Highcharter exercise:\n  Moving on to a new task with the same raw dataset. Can you create a publication quality table using the gt package in R with the following columns:\n  Generation, Mean Age, Count of Members, and an in-table distribution plot\n  And have the rows grouped by year, similar to a pivot table?\nThe output got some of the overall structure but was very lacking on the details, which again proved challenging due to my unfamiliarity with gt tables. For example, it knew I would need to create a way to call the plots formulaically, however it used the base histogram function instead of ggplot, which from what I can tell would not work with gt tables or at least not in an intuitive way. it also seemed to struggle with how to actually add the distribution plot in to the table, the code it generated would have added the plots after creating the tables initially, which is fine but it used the wrong functions to do so repeatedly. After trading prompts a number of times and getting mostly nowhere I decided to take matters into my own hands and researched how to use gt tables via the provided tutorial, which helped tremendously.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.3.3\n\n# Read the data from the URL\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\"\ndata &lt;- read_csv(url)\n\nRows: 29120 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): chamber, state_abbrev, bioname, bioguide_id, generation\ndbl  (6): congress, party_code, cmltv_cong, cmltv_chamber, age_days, age_years\ndate (2): start_date, birthday\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndata2 &lt;- data %&gt;%\n  select(start_date, generation, age_years)\n\n# Extract year from start_date\ndata2$year &lt;- as.integer(substr(data2$start_date, 1, 4))\n\ndata2 &lt;- data2 %&gt;% arrange(by = -year)\n\ngen_order &lt;- data %&gt;% mutate(byear = as.integer(str_sub(birthday, 1, 4))) %&gt;% group_by(generation) %&gt;% summarize(oldyr = min(byear)) %&gt;% arrange(by = oldyr) %&gt;% select(generation)\n\ndata2$generation &lt;- fct_rev(fct_relevel(data2$generation, as.vector(gen_order)))\n\nWarning: Outer names are only allowed for unnamed scalar atomic inputs\n\n# Group by year and generation, calculate mean age and count of members\ndata_summary &lt;- data2 %&gt;%\n  group_by(year, generation) %&gt;%\n  summarise(mean_age = mean(age_years, na.rm = TRUE),\n            count_members = n()) %&gt;%\n  ungroup() %&gt;%\n  filter(!is.na(mean_age)) %&gt;% arrange(by = -year) # Remove rows with NA mean_age for clarity\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\ndist_plot &lt;- function(fun_gen, fun_yr) {\n  full_range &lt;- data2 %&gt;% \n  pull(age_years) %&gt;% \n  range()\n  \n  data2 %&gt;% \n    filter(generation == !!fun_gen, year == !!fun_yr) %&gt;% \n    ggplot() +\n    geom_violin(aes(x=age_years, y = generation), fill = 'black') +\n    theme_minimal() +\n    scale_y_discrete(breaks = NULL) +\n    scale_x_continuous(breaks = NULL) +\n    labs(x = element_blank(), y = element_blank()) +\n    coord_cartesian(xlim = full_range)\n}\ndist_plot('Boomers', 2021) #testing function and plot appearance\n\n\n\n\n\n\n\n\n\n# Initialize the GT table\ngt_table &lt;- data_summary %&gt;%\n  group_by(generation, year) %&gt;% \n  mutate(Distribution = list(c(as.character(generation), year))) %&gt;%\n  ungroup() %&gt;% \n  gt(groupname_col = 'year', rowname_col = 'generation') %&gt;%\n  tab_header(\n    title = md(\"**Generation and Congress Membership Statistics**\"),\n    subtitle = \"Mean Age, Count of Members, and Distribution Plot by Year\"\n  ) %&gt;%\n  cols_label(\n    generation = md(\"**Generation**\"),\n    mean_age = md(\"**Mean Age**\"),\n    count_members = md(\"**Count**\"),\n    Distribution = md('**Distribution**')\n  ) %&gt;%\n  fmt_number(columns = c(mean_age), decimals = 0) %&gt;% \n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = cells_row_groups()\n  ) %&gt;% \n  tab_style(\n    style = cell_text(align = 'center'),\n    locations = cells_column_labels()\n  ) %&gt;% \n  text_transform(\n    locations = cells_body(columns = 'Distribution'),\n    fn = function(column) {\n      map(column, ~str_split_1(., ', ')) %&gt;% \n        map(~dist_plot(.[1], .[2])) %&gt;% \n        ggplot_image(height = px(30), aspect_ratio =  3)\n    }\n  ) %&gt;%\n  tab_footnote(\n    \"Note: Mean Age is rounded to nearest whole year. Distribution plots represent age distributions within each generation.\"\n  ) %&gt;% \n  tab_options(\n    data_row.padding = px(1),\n    row_group.padding = px(4)\n  )\ngt_table %&gt;% \n  opt_stylize(\n    style = 5, color = 'blue'\n    )\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\n\n\n\n\n\n\n\nGeneration and Congress Membership Statistics\n\n\nMean Age, Count of Members, and Distribution Plot by Year\n\n\n\nMean Age\nCount\nDistribution\n\n\n\n\n2023\n\n\nGen Z\n26\n1\n\n\n\nMillennial\n38\n55\n\n\n\nGen X\n50\n192\n\n\n\nBoomers\n67\n259\n\n\n\nSilent\n81\n29\n\n\n\n2021\n\n\nMillennial\n36\n37\n\n\n\nGen X\n49\n175\n\n\n\nBoomers\n65\n301\n\n\n\nSilent\n80\n38\n\n\n\n2019\n\n\nMillennial\n35\n26\n\n\n\nGen X\n47\n162\n\n\n\nBoomers\n63\n303\n\n\n\nSilent\n78\n53\n\n\n\n2017\n\n\nMillennial\n34\n6\n\n\n\nGen X\n45\n136\n\n\n\nBoomers\n61\n351\n\n\n\nSilent\n77\n62\n\n\n\n2015\n\n\nMillennial\n32\n4\n\n\n\nGen X\n44\n122\n\n\n\nBoomers\n60\n340\n\n\n\nSilent\n75\n75\n\n\n\n2013\n\n\nMillennial\n31\n3\n\n\n\nGen X\n42\n101\n\n\n\nBoomers\n58\n346\n\n\n\nSilent\n73\n95\n\n\n\nGreatest\n88\n3\n\n\n\n2011\n\n\nMillennial\n30\n1\n\n\n\nGen X\n41\n80\n\n\n\nBoomers\n56\n336\n\n\n\nSilent\n71\n123\n\n\n\nGreatest\n86\n6\n\n\n\n2009\n\n\nMillennial\n28\n1\n\n\n\nGen X\n39\n55\n\n\n\nBoomers\n55\n330\n\n\n\nSilent\n69\n160\n\n\n\nGreatest\n85\n7\n\n\n\n2007\n\n\nGen X\n38\n35\n\n\n\nBoomers\n53\n328\n\n\n\nSilent\n67\n176\n\n\n\nGreatest\n83\n10\n\n\n\n2005\n\n\nGen X\n36\n25\n\n\n\nBoomers\n52\n305\n\n\n\nSilent\n65\n199\n\n\n\nGreatest\n81\n11\n\n\n\n2003\n\n\nGen X\n35\n19\n\n\n\nBoomers\n50\n289\n\n\n\nSilent\n63\n217\n\n\n\nGreatest\n78\n14\n\n\n\n2001\n\n\nGen X\n33\n12\n\n\n\nBoomers\n48\n274\n\n\n\nSilent\n62\n239\n\n\n\nGreatest\n77\n22\n\n\n\n1999\n\n\nGen X\n32\n7\n\n\n\nBoomers\n46\n251\n\n\n\nSilent\n60\n252\n\n\n\nGreatest\n75\n29\n\n\n\n1997\n\n\nGen X\n30\n6\n\n\n\nBoomers\n45\n235\n\n\n\nSilent\n58\n268\n\n\n\nGreatest\n73\n35\n\n\n\n1995\n\n\nGen X\n29\n3\n\n\n\nBoomers\n43\n207\n\n\n\nSilent\n57\n285\n\n\n\nGreatest\n72\n48\n\n\n\n1993\n\n\nBoomers\n42\n164\n\n\n\nSilent\n55\n314\n\n\n\nGreatest\n70\n68\n\n\n\n1991\n\n\nBoomers\n41\n113\n\n\n\nSilent\n54\n331\n\n\n\nGreatest\n69\n104\n\n\n\n1989\n\n\nBoomers\n39\n95\n\n\n\nSilent\n52\n338\n\n\n\nGreatest\n67\n111\n\n\n\nLost\n88\n1\n\n\n\n1987\n\n\nBoomers\n37\n83\n\n\n\nSilent\n50\n330\n\n\n\nGreatest\n66\n130\n\n\n\nLost\n86\n1\n\n\n\n1985\n\n\nBoomers\n36\n67\n\n\n\nSilent\n48\n318\n\n\n\nGreatest\n64\n155\n\n\n\nLost\n84\n1\n\n\n\n1983\n\n\nBoomers\n34\n59\n\n\n\nSilent\n46\n308\n\n\n\nGreatest\n62\n176\n\n\n\nLost\n82\n1\n\n\n\n1981\n\n\nBoomers\n32\n40\n\n\n\nSilent\n45\n305\n\n\n\nGreatest\n60\n199\n\n\n\nLost\n80\n1\n\n\n\n1979\n\n\nBoomers\n31\n18\n\n\n\nSilent\n43\n277\n\n\n\nGreatest\n59\n247\n\n\n\nLost\n80\n2\n\n\n\n1977\n\n\nBoomers\n29\n10\n\n\n\nSilent\n42\n236\n\n\n\nGreatest\n58\n298\n\n\n\nLost\n78\n7\n\n\n\n1975\n\n\nBoomers\n27\n4\n\n\n\nSilent\n40\n189\n\n\n\nGreatest\n57\n343\n\n\n\nLost\n77\n15\n\n\n\n1973\n\n\nSilent\n40\n129\n\n\n\nGreatest\n55\n398\n\n\n\nLost\n75\n21\n\n\n\n1971\n\n\nSilent\n39\n92\n\n\n\nGreatest\n54\n422\n\n\n\nLost\n74\n34\n\n\n\n1969\n\n\nSilent\n37\n65\n\n\n\nGreatest\n53\n441\n\n\n\nLost\n73\n45\n\n\n\n1967\n\n\nSilent\n36\n45\n\n\n\nGreatest\n51\n436\n\n\n\nLost\n71\n60\n\n\n\nMissionary\n87\n2\n\n\n\n1965\n\n\nSilent\n34\n30\n\n\n\nGreatest\n50\n437\n\n\n\nLost\n69\n79\n\n\n\nMissionary\n85\n2\n\n\n\n1963\n\n\nSilent\n32\n17\n\n\n\nGreatest\n49\n415\n\n\n\nLost\n67\n115\n\n\n\nMissionary\n84\n4\n\n\n\n1961\n\n\nSilent\n32\n5\n\n\n\nGreatest\n48\n390\n\n\n\nLost\n66\n154\n\n\n\nMissionary\n82\n9\n\n\n\n1959\n\n\nSilent\n31\n1\n\n\n\nGreatest\n47\n368\n\n\n\nLost\n64\n169\n\n\n\nMissionary\n81\n14\n\n\n\n1957\n\n\nGreatest\n46\n305\n\n\n\nLost\n63\n215\n\n\n\nMissionary\n79\n25\n\n\n\n1955\n\n\nGreatest\n45\n278\n\n\n\nLost\n61\n235\n\n\n\nMissionary\n76\n27\n\n\n\n1953\n\n\nGreatest\n44\n251\n\n\n\nLost\n59\n263\n\n\n\nMissionary\n75\n38\n\n\n\n1951\n\n\nGreatest\n43\n220\n\n\n\nLost\n57\n283\n\n\n\nMissionary\n74\n50\n\n\n\n1949\n\n\nGreatest\n41\n198\n\n\n\nLost\n56\n289\n\n\n\nMissionary\n72\n66\n\n\n\n1947\n\n\nGreatest\n40\n167\n\n\n\nLost\n54\n297\n\n\n\nMissionary\n70\n89\n\n\n\n1945\n\n\nGreatest\n40\n120\n\n\n\nLost\n53\n316\n\n\n\nMissionary\n68\n127\n\n\n\nProgressive\n87\n1\n\n\n\n1943\n\n\nGreatest\n38\n92\n\n\n\nLost\n51\n306\n\n\n\nMissionary\n67\n155\n\n\n\nProgressive\n85\n1\n\n\n\n1941\n\n\nGreatest\n36\n74\n\n\n\nLost\n49\n311\n\n\n\nMissionary\n65\n173\n\n\n\nProgressive\n84\n3\n\n\n\n1939\n\n\nGreatest\n34\n54\n\n\n\nLost\n47\n293\n\n\n\nMissionary\n64\n211\n\n\n\nProgressive\n80\n3\n\n\n\n1937\n\n\nGreatest\n33\n32\n\n\n\nLost\n46\n271\n\n\n\nMissionary\n62\n249\n\n\n\nProgressive\n79\n2\n\n\n\n1935\n\n\nGreatest\n31\n21\n\n\n\nLost\n44\n239\n\n\n\nMissionary\n61\n284\n\n\n\nProgressive\n77\n5\n\n\n\n1933\n\n\nGreatest\n30\n9\n\n\n\nLost\n43\n198\n\n\n\nMissionary\n60\n329\n\n\n\nProgressive\n76\n11\n\n\n\n1931\n\n\nGreatest\n27\n2\n\n\n\nLost\n41\n146\n\n\n\nMissionary\n58\n396\n\n\n\nProgressive\n74\n19\n\n\n\n1929\n\n\nGreatest\n27\n1\n\n\n\nLost\n40\n107\n\n\n\nMissionary\n57\n424\n\n\n\nProgressive\n73\n34\n\n\n\nGilded\n88\n1\n\n\n\n1927\n\n\nLost\n39\n89\n\n\n\nMissionary\n55\n414\n\n\n\nProgressive\n71\n47\n\n\n\nGilded\n86\n1\n\n\n\n1925\n\n\nLost\n37\n69\n\n\n\nMissionary\n54\n419\n\n\n\nProgressive\n69\n59\n\n\n\nGilded\n84\n1\n\n\n\n1923\n\n\nLost\n36\n49\n\n\n\nMissionary\n52\n431\n\n\n\nProgressive\n68\n74\n\n\n\nGilded\n84\n3\n\n\n\n1921\n\n\nLost\n36\n31\n\n\n\nMissionary\n50\n418\n\n\n\nProgressive\n66\n107\n\n\n\nGilded\n83\n4\n\n\n\n1919\n\n\nLost\n33\n19\n\n\n\nMissionary\n49\n412\n\n\n\nProgressive\n65\n119\n\n\n\nGilded\n80\n5\n\n\n\n\nNote: Mean Age is rounded to nearest whole year. Distribution plots represent age distributions within each generation.\n\n\n\n\n\n\n\n\n\nI’m fairly satisfied with the output. The inline violin charts were especially satisfying to make, and I’m happy with the availability of all the different customization options, while simultaneously there are several quick theme options to make styling even easier. That said, the syntax is less intuitive than I would prefer, I was truly hoping for a more ggplot-like experience."
  }
]