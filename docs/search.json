[
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Placeholder file for the future Tidy Tuesday exercise."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "Antonio Flores contributed to this exercise.\nThe structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nIn addition to the standard variables of Height (in centimeters), Weight (in kilograms), and identified gender (Male, Female, Other), I have included twp additional variables: Generation and Salary. Generation represents the respondents age categorized into the self reported generation to which they belong, e.g. Gen Z are those with birth years between 1995 and 2012. Salary is the self reported annual salary in thousands of US dollars, e.g. a value of 70 would represent an annual salary of $70,000."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nfactor\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGeneration\n0\n1\nFALSE\n4\nGen: 5, Gen: 2, Bab: 1, Mil: 1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nSalary\n0\n1\nNA\nNA\nNA\n95.44444\n36.62005\n44\n70\n81\n133\n144\n▂▇▂▁▆"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(here) #to set paths\n\nhere() starts at C:/Users/Hylti/OneDrive/Documents/School/Practicum II/williamhyltin-P2-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                    `Allowed Values`     \n  &lt;chr&gt;           &lt;chr&gt;                                    &lt;chr&gt;                \n1 Height          height in centimeters                    numeric value &gt;0 or …\n2 Weight          weight in kilograms                      numeric value &gt;0 or …\n3 Gender          identified gender (male/female/other)    M/F/O/NA             \n4 Generation      Field representing a person's generation Gen Z/Millennial/Gen…\n5 Salary          Annual Salary in thousands of dollars    numeric value &gt;0 or …\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height     &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"15…\n$ Weight     &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender     &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\"…\n$ Generation &lt;chr&gt; \"Millennial\", \"Gen Z\", \"Baby Boomer\", \"Baby Boomer\", \"Baby …\n$ Salary     &lt;dbl&gt; 144, 77, 140, 68, 54, 133, 70, 44, 142, 77, 99, 144, 98, 81\n\nsummary(rawdata)\n\n    Height              Weight          Gender           Generation       \n Length:14          Min.   :  45.0   Length:14          Length:14         \n Class :character   1st Qu.:  55.0   Class :character   Class :character  \n Mode  :character   Median :  70.0   Mode  :character   Mode  :character  \n                    Mean   : 602.7                                        \n                    3rd Qu.:  90.0                                        \n                    Max.   :7000.0                                        \n                    NA's   :1                                             \n     Salary      \n Min.   : 44.00  \n 1st Qu.: 71.75  \n Median : 89.50  \n Mean   : 97.93  \n 3rd Qu.:138.25  \n Max.   :144.00  \n                 \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender Generation  Salary\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt;\n1 180        80 M      Millennial     144\n2 175        70 O      Gen Z           77\n3 sixty      60 F      Baby Boomer    140\n4 178        76 F      Baby Boomer     68\n5 192        90 NA     Baby Boomer     54\n6 6          55 F      Gen X          133\n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nGeneration\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70.0\n90.00\n7000\n▇▁▁▁▁\n\n\nSalary\n0\n1.00\n97.93\n36.02\n44\n71.75\n89.5\n138.25\n144\n▃▇▃▁▇\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nGeneration\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nSalary\n0\n1.00\n94.69\n35.31\n44\n70.00\n81\n133\n144\n▃▇▃▁▆\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nGeneration\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nSalary\n0\n1.00\n94.69\n35.31\n44\n70.00\n81\n133\n144\n▃▇▃▁▆\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nGeneration\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nSalary\n0\n1\n95.91\n38.22\n44\n69.0\n81\n137.5\n144\n▃▇▂▁▇\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\nWe will also change ‘Generation’ to a factor variable\n\nd3$Gender &lt;- as.factor(d3$Gender)\nd3$Generation &lt;- as.factor(d3$Generation)\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\nGeneration\n0\n1\nFALSE\n4\nGen: 6, Bab: 2, Gen: 2, Mil: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nSalary\n0\n1\n95.91\n38.22\n44\n69.0\n81\n137.5\n144\n▃▇▂▁▇\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\nGeneration\n0\n1\nFALSE\n4\nGen: 5, Gen: 2, Bab: 1, Mil: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nSalary\n0\n1\n95.44\n36.62\n44\n70\n81\n133\n144\n▂▇▂▁▆\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at C:/Users/Hylti/OneDrive/Documents/School/Practicum II/williamhyltin-P2-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "For this exercise we are tasked with recreating a high-quality publication level chart found online using R and various AI tools. I have chosen to recreate the area chart on this page. Since this is coming from FiveThirtyEight and the original plot is interactive I have decided to recreate the plot with Highcharter.\nFirst we are tasked with requesting AI (in this case I have chosen to use ChatGPT) to attempt to recreate the plot, which I do with my initial prompt here:\n  Can you provide code using Highcharter in R that would recreate the area plot titled “Baby boomers are the biggest generation in Congress today” on this page https://fivethirtyeight.com/features/aging-congress-boomers/\n  raw data can be found here: https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\nThe first round of code had a handful of problems, primarily with processing the data, but it came pretty close in terms of structuring the overall request. I provided the column names in the raw data to help with processing, as well as correcting things like unnecessary filters. One of the outputs from ChatGPT ended up with pivoted data, which was not as easy to use for what we were doing, but the unpivoted data was still pretty close so I modified it myself and fed that back as a prompt.\nI’m still having problems with the highcharter section of code. I’ve modified the processing code slightly to adjust the pct values as well, because it did not look like it was previously giving the correct values for pct. I’ve also created a year variable to match what is done in the chart. Here is the processing code after my modifications: data1 &lt;- data %&gt;% mutate(year = as.integer(str_sub(start_date, 1, 4))) %&gt;% group_by(year, generation) %&gt;% summarise(count = n()) %&gt;% ungroup() %&gt;% group_by(year) %&gt;% mutate(pct = count / sum(count) * 100) Can you utilize this dataframe without pivoting to create the chart in Highcharter?\nThe output from this prompt got me most of the way there. The code runs, but some of the generations were out of order, and there were several tweaks needed to get the chart to look like the one on the page. Still the bones were there, so I took over from this point on, aside from some one off prompts to get exactly what I was looking for, with an example below.\nCan you adjust the tooltip in the chart to have the following format: Generation Year Percent (in percent format 00.0%) So for example: Missionary 1937 44.9%\nThe code and output below is the final product after the combined efforts of ChatGPT and myself.\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(highcharter)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n# Read the data from the URL\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\"\ndata &lt;- read_csv(url)\n\nRows: 29120 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): chamber, state_abbrev, bioname, bioguide_id, generation\ndbl  (6): congress, party_code, cmltv_cong, cmltv_chamber, age_days, age_years\ndate (2): start_date, birthday\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Calculate percentage of each generation in each congress\ndata1 &lt;- data %&gt;%\n  mutate(year = as.integer(str_sub(start_date, 1, 4))) %&gt;% \n  group_by(year, generation) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(year) %&gt;% \n  mutate(pct = count / sum(count) * 100)\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\ngen_order &lt;- data %&gt;% mutate(byear = as.integer(str_sub(birthday, 1, 4))) %&gt;% group_by(generation) %&gt;% summarize(oldyr = min(byear)) %&gt;% arrange(by = oldyr) %&gt;% select(generation)\n\n\ndata1$generation &lt;- fct_rev(fct_relevel(data1$generation, as.vector(gen_order)))\n\nWarning: Outer names are only allowed for unnamed scalar atomic inputs\n\n\n\ngeneration_colors &lt;- c(\n  \"#D26E8C\",\n  \"#92DCE0\",\n  \"#A593E9\",\n  \"#FD867E\",\n  \"#FDE384\",\n  \"#86D09D\",\n  \"#8B8887\",\n  \"#E98CCA\",\n  \"#FFE7E5\",\n  \"#E9E9E9\"\n)\n\n\nhighchart(type = \"chart\") %&gt;%\n  hc_chart(type = \"area\") %&gt;%\n  hc_title(text = \"&lt;b&gt;Baby boomers are the biggest generation in Congress today&lt;/b&gt;\",\n           align = 'left',\n           style = list(fontSize = \"24px\",\n                        fontFamily = 'Calibri')) %&gt;%\n  hc_subtitle(text = 'Share of members in Congress from each generation, 1919 to 2023',\n              align = 'left',\n              style = list(fontSize = \"20px\",\n                           fontFamily = 'Calibri')) %&gt;% \n  hc_xAxis(gridLineWidth = 0,\n    labels = list(\n    style = list(fontSize = \"16px\")\n    )\n    ) %&gt;%\n  hc_yAxis(max = 100,\n           labels = list(\n      formatter = JS(\"function() {\n                      if (this.value == 100) {\n                        return this.value + '%';\n                      } else {\n                        return this.value;\n                      }\n                    }\"),\n      style = list(fontSize = \"16px\")\n           )\n      ) %&gt;%\n  hc_add_series(data1, \"area\", hcaes(x = year, y = pct, group = generation),\n                color = generation_colors) %&gt;%\n  hc_legend(align = \"left\",\n            verticalAlign = \"top\",\n            layout = \"horizontal\",\n            itemStyle = list(fontFamily = 'Copperplate Gothic Light', fontSize = \"12px\")) %&gt;%\n  hc_tooltip(\n    headerFormat = \"&lt;b&gt;{series.name}&lt;/b&gt;&lt;br/&gt;\",\n    pointFormat = \"{point.x}&lt;br/&gt;{point.y:.1f}%\",\n    crosshairs = TRUE\n  ) %&gt;%\n  hc_plotOptions(\n    area = list(\n      stacking = \"normal\",\n      marker = list(enabled = FALSE,\n                    states = list(\n                      hover = list(\n                        enabled = FALSE\n                        )\n                      )\n                    )  # Disable markers\n      ),\n    series = list(\n      lineColor = '#EDF6E9',\n      trackByArea = TRUE,\n      stickyTracking = FALSE\n    )\n  ) %&gt;% hc_add_theme(hc_theme_538())\n\n\n\n\n\nI believe my output is pretty close to the original, with the exception of a few things like fonts, and the hover animations. I could not figure out how to get the crosshair for the tooltip to stay inside of the relevant generation group, nor could I figure out how to change the shape of the color indicators in the legend from circular to square, but even without those changes it still like it was high quality and close enough."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#publication-quality-chart",
    "href": "presentation-exercise/presentation-exercise.html#publication-quality-chart",
    "title": "Presentation Exercise",
    "section": "",
    "text": "For this exercise we are tasked with recreating a high-quality publication level chart found online using R and various AI tools. I have chosen to recreate the area chart on this page. Since this is coming from FiveThirtyEight and the original plot is interactive I have decided to recreate the plot with Highcharter.\nFirst we are tasked with requesting AI (in this case I have chosen to use ChatGPT) to attempt to recreate the plot, which I do with my initial prompt here:\n  Can you provide code using Highcharter in R that would recreate the area plot titled “Baby boomers are the biggest generation in Congress today” on this page https://fivethirtyeight.com/features/aging-congress-boomers/\n  raw data can be found here: https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\nThe first round of code had a handful of problems, primarily with processing the data, but it came pretty close in terms of structuring the overall request. I provided the column names in the raw data to help with processing, as well as correcting things like unnecessary filters. One of the outputs from ChatGPT ended up with pivoted data, which was not as easy to use for what we were doing, but the unpivoted data was still pretty close so I modified it myself and fed that back as a prompt.\nI’m still having problems with the highcharter section of code. I’ve modified the processing code slightly to adjust the pct values as well, because it did not look like it was previously giving the correct values for pct. I’ve also created a year variable to match what is done in the chart. Here is the processing code after my modifications: data1 &lt;- data %&gt;% mutate(year = as.integer(str_sub(start_date, 1, 4))) %&gt;% group_by(year, generation) %&gt;% summarise(count = n()) %&gt;% ungroup() %&gt;% group_by(year) %&gt;% mutate(pct = count / sum(count) * 100) Can you utilize this dataframe without pivoting to create the chart in Highcharter?\nThe output from this prompt got me most of the way there. The code runs, but some of the generations were out of order, and there were several tweaks needed to get the chart to look like the one on the page. Still the bones were there, so I took over from this point on, aside from some one off prompts to get exactly what I was looking for, with an example below.\nCan you adjust the tooltip in the chart to have the following format: Generation Year Percent (in percent format 00.0%) So for example: Missionary 1937 44.9%\nThe code and output below is the final product after the combined efforts of ChatGPT and myself.\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(highcharter)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n# Read the data from the URL\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\"\ndata &lt;- read_csv(url)\n\nRows: 29120 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): chamber, state_abbrev, bioname, bioguide_id, generation\ndbl  (6): congress, party_code, cmltv_cong, cmltv_chamber, age_days, age_years\ndate (2): start_date, birthday\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Calculate percentage of each generation in each congress\ndata1 &lt;- data %&gt;%\n  mutate(year = as.integer(str_sub(start_date, 1, 4))) %&gt;% \n  group_by(year, generation) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(year) %&gt;% \n  mutate(pct = count / sum(count) * 100)\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\ngen_order &lt;- data %&gt;% mutate(byear = as.integer(str_sub(birthday, 1, 4))) %&gt;% group_by(generation) %&gt;% summarize(oldyr = min(byear)) %&gt;% arrange(by = oldyr) %&gt;% select(generation)\n\n\ndata1$generation &lt;- fct_rev(fct_relevel(data1$generation, as.vector(gen_order)))\n\nWarning: Outer names are only allowed for unnamed scalar atomic inputs\n\n\n\ngeneration_colors &lt;- c(\n  \"#D26E8C\",\n  \"#92DCE0\",\n  \"#A593E9\",\n  \"#FD867E\",\n  \"#FDE384\",\n  \"#86D09D\",\n  \"#8B8887\",\n  \"#E98CCA\",\n  \"#FFE7E5\",\n  \"#E9E9E9\"\n)\n\n\nhighchart(type = \"chart\") %&gt;%\n  hc_chart(type = \"area\") %&gt;%\n  hc_title(text = \"&lt;b&gt;Baby boomers are the biggest generation in Congress today&lt;/b&gt;\",\n           align = 'left',\n           style = list(fontSize = \"24px\",\n                        fontFamily = 'Calibri')) %&gt;%\n  hc_subtitle(text = 'Share of members in Congress from each generation, 1919 to 2023',\n              align = 'left',\n              style = list(fontSize = \"20px\",\n                           fontFamily = 'Calibri')) %&gt;% \n  hc_xAxis(gridLineWidth = 0,\n    labels = list(\n    style = list(fontSize = \"16px\")\n    )\n    ) %&gt;%\n  hc_yAxis(max = 100,\n           labels = list(\n      formatter = JS(\"function() {\n                      if (this.value == 100) {\n                        return this.value + '%';\n                      } else {\n                        return this.value;\n                      }\n                    }\"),\n      style = list(fontSize = \"16px\")\n           )\n      ) %&gt;%\n  hc_add_series(data1, \"area\", hcaes(x = year, y = pct, group = generation),\n                color = generation_colors) %&gt;%\n  hc_legend(align = \"left\",\n            verticalAlign = \"top\",\n            layout = \"horizontal\",\n            itemStyle = list(fontFamily = 'Copperplate Gothic Light', fontSize = \"12px\")) %&gt;%\n  hc_tooltip(\n    headerFormat = \"&lt;b&gt;{series.name}&lt;/b&gt;&lt;br/&gt;\",\n    pointFormat = \"{point.x}&lt;br/&gt;{point.y:.1f}%\",\n    crosshairs = TRUE\n  ) %&gt;%\n  hc_plotOptions(\n    area = list(\n      stacking = \"normal\",\n      marker = list(enabled = FALSE,\n                    states = list(\n                      hover = list(\n                        enabled = FALSE\n                        )\n                      )\n                    )  # Disable markers\n      ),\n    series = list(\n      lineColor = '#EDF6E9',\n      trackByArea = TRUE,\n      stickyTracking = FALSE\n    )\n  ) %&gt;% hc_add_theme(hc_theme_538())\n\n\n\n\n\nI believe my output is pretty close to the original, with the exception of a few things like fonts, and the hover animations. I could not figure out how to get the crosshair for the tooltip to stay inside of the relevant generation group, nor could I figure out how to change the shape of the color indicators in the legend from circular to square, but even without those changes it still like it was high quality and close enough."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#publication-quality-tables",
    "href": "presentation-exercise/presentation-exercise.html#publication-quality-tables",
    "title": "Presentation Exercise",
    "section": "Publication Quality Tables",
    "text": "Publication Quality Tables\nUsing the same dataset, I wanted create a quality table that would show what the congressional membership distribution but include things like the mean age and count of members. The chart above does a great job of showing the relative distribution and the change over time, but the actual age is still an important factor. For example, we may have a lot of Congress members from the Baby Boomer generation in today’s Congress, but do they themselves skew younger or older?\nI decided I would use GT tables. I can’t say i have any experience with gt tables, so I thought this might be a challenge, but I was intrigued by the ability to use ggplots as images in the tables themself. I started off with ChatGPT again with the following prompt, continued off the prompts from teh Highcharter exercise:\n  Moving on to a new task with the same raw dataset. Can you create a publication quality table using the gt package in R with the following columns:\n  Generation, Mean Age, Count of Members, and an in-table distribution plot\n  And have the rows grouped by year, similar to a pivot table?\nThe output got some of the overall structure but was very lacking on the details, which again proved challenging due to my unfamiliarity with gt tables. For example, it knew I would need to create a way to call the plots formulaically, however it used the base histogram function instead of ggplot, which from what I can tell would not work with gt tables or at least not in an intuitive way. it also seemed to struggle with how to actually add the distribution plot in to the table, the code it generated would have added the plots after creating the tables initially, which is fine but it used the wrong functions to do so repeatedly. After trading prompts a number of times and getting mostly nowhere I decided to take matters into my own hands and researched how to use gt tables via the provided tutorial, which helped tremendously.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.3.3\n\n# Read the data from the URL\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\"\ndata &lt;- read_csv(url)\n\nRows: 29120 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): chamber, state_abbrev, bioname, bioguide_id, generation\ndbl  (6): congress, party_code, cmltv_cong, cmltv_chamber, age_days, age_years\ndate (2): start_date, birthday\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndata2 &lt;- data %&gt;%\n  select(start_date, generation, age_years)\n\n# Extract year from start_date\ndata2$year &lt;- as.integer(substr(data2$start_date, 1, 4))\n\ndata2 &lt;- data2 %&gt;% arrange(by = -year)\n\ngen_order &lt;- data %&gt;% mutate(byear = as.integer(str_sub(birthday, 1, 4))) %&gt;% group_by(generation) %&gt;% summarize(oldyr = min(byear)) %&gt;% arrange(by = oldyr) %&gt;% select(generation)\n\ndata2$generation &lt;- fct_rev(fct_relevel(data2$generation, as.vector(gen_order)))\n\nWarning: Outer names are only allowed for unnamed scalar atomic inputs\n\n# Group by year and generation, calculate mean age and count of members\ndata_summary &lt;- data2 %&gt;%\n  group_by(year, generation) %&gt;%\n  summarise(mean_age = mean(age_years, na.rm = TRUE),\n            count_members = n()) %&gt;%\n  ungroup() %&gt;%\n  filter(!is.na(mean_age)) %&gt;% arrange(by = -year) # Remove rows with NA mean_age for clarity\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\ndist_plot &lt;- function(fun_gen, fun_yr) {\n  full_range &lt;- data2 %&gt;% \n  pull(age_years) %&gt;% \n  range()\n  \n  data2 %&gt;% \n    filter(generation == !!fun_gen, year == !!fun_yr) %&gt;% \n    ggplot() +\n    geom_violin(aes(x=age_years, y = generation), fill = 'black') +\n    theme_minimal() +\n    scale_y_discrete(breaks = NULL) +\n    scale_x_continuous(breaks = NULL) +\n    labs(x = element_blank(), y = element_blank()) +\n    coord_cartesian(xlim = full_range)\n}\ndist_plot('Boomers', 2021) #testing function and plot appearance\n\n\n\n\n\n\n\n\n\n# Initialize the GT table\ngt_table &lt;- data_summary %&gt;%\n  group_by(generation, year) %&gt;% \n  mutate(Distribution = list(c(as.character(generation), year))) %&gt;%\n  ungroup() %&gt;% \n  gt(groupname_col = 'year', rowname_col = 'generation') %&gt;%\n  tab_header(\n    title = md(\"**Generation and Congress Membership Statistics**\"),\n    subtitle = \"Mean Age, Count of Members, and Distribution Plot by Year\"\n  ) %&gt;%\n  cols_label(\n    generation = md(\"**Generation**\"),\n    mean_age = md(\"**Mean Age**\"),\n    count_members = md(\"**Count**\"),\n    Distribution = md('**Distribution**')\n  ) %&gt;%\n  fmt_number(columns = c(mean_age), decimals = 0) %&gt;% \n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = cells_row_groups()\n  ) %&gt;% \n  tab_style(\n    style = cell_text(align = 'center'),\n    locations = cells_column_labels()\n  ) %&gt;% \n  text_transform(\n    locations = cells_body(columns = 'Distribution'),\n    fn = function(column) {\n      map(column, ~str_split_1(., ', ')) %&gt;% \n        map(~dist_plot(.[1], .[2])) %&gt;% \n        ggplot_image(height = px(30), aspect_ratio =  3)\n    }\n  ) %&gt;%\n  tab_footnote(\n    \"Note: Mean Age is rounded to nearest whole year. Distribution plots represent age distributions within each generation.\"\n  ) %&gt;% \n  tab_options(\n    data_row.padding = px(1),\n    row_group.padding = px(4)\n  )\ngt_table %&gt;% \n  opt_stylize(\n    style = 5, color = 'blue'\n    )\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(data$density): no non-missing arguments to max; returning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(data$density): no non-missing arguments to max; returning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(data$density): no non-missing arguments to max; returning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(data$density): no non-missing arguments to max; returning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(data$density): no non-missing arguments to max; returning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(data$density): no non-missing arguments to max; returning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(data$density): no non-missing arguments to max; returning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(data$density): no non-missing arguments to max; returning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(data$density): no non-missing arguments to max; returning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(data$density): no non-missing arguments to max; returning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(data$density): no non-missing arguments to max; returning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(data$density): no non-missing arguments to max; returning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(data$density): no non-missing arguments to max; returning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(data$density): no non-missing arguments to max; returning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(data$density): no non-missing arguments to max; returning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\n\n\n\n\n\n\n\nGeneration and Congress Membership Statistics\n\n\nMean Age, Count of Members, and Distribution Plot by Year\n\n\n\nMean Age\nCount\nDistribution\n\n\n\n\n2023\n\n\nGen Z\n26\n1\n\n\n\nMillennial\n38\n55\n\n\n\nGen X\n50\n192\n\n\n\nBoomers\n67\n259\n\n\n\nSilent\n81\n29\n\n\n\n2021\n\n\nMillennial\n36\n37\n\n\n\nGen X\n49\n175\n\n\n\nBoomers\n65\n301\n\n\n\nSilent\n80\n38\n\n\n\n2019\n\n\nMillennial\n35\n26\n\n\n\nGen X\n47\n162\n\n\n\nBoomers\n63\n303\n\n\n\nSilent\n78\n53\n\n\n\n2017\n\n\nMillennial\n34\n6\n\n\n\nGen X\n45\n136\n\n\n\nBoomers\n61\n351\n\n\n\nSilent\n77\n62\n\n\n\n2015\n\n\nMillennial\n32\n4\n\n\n\nGen X\n44\n122\n\n\n\nBoomers\n60\n340\n\n\n\nSilent\n75\n75\n\n\n\n2013\n\n\nMillennial\n31\n3\n\n\n\nGen X\n42\n101\n\n\n\nBoomers\n58\n346\n\n\n\nSilent\n73\n95\n\n\n\nGreatest\n88\n3\n\n\n\n2011\n\n\nMillennial\n30\n1\n\n\n\nGen X\n41\n80\n\n\n\nBoomers\n56\n336\n\n\n\nSilent\n71\n123\n\n\n\nGreatest\n86\n6\n\n\n\n2009\n\n\nMillennial\n28\n1\n\n\n\nGen X\n39\n55\n\n\n\nBoomers\n55\n330\n\n\n\nSilent\n69\n160\n\n\n\nGreatest\n85\n7\n\n\n\n2007\n\n\nGen X\n38\n35\n\n\n\nBoomers\n53\n328\n\n\n\nSilent\n67\n176\n\n\n\nGreatest\n83\n10\n\n\n\n2005\n\n\nGen X\n36\n25\n\n\n\nBoomers\n52\n305\n\n\n\nSilent\n65\n199\n\n\n\nGreatest\n81\n11\n\n\n\n2003\n\n\nGen X\n35\n19\n\n\n\nBoomers\n50\n289\n\n\n\nSilent\n63\n217\n\n\n\nGreatest\n78\n14\n\n\n\n2001\n\n\nGen X\n33\n12\n\n\n\nBoomers\n48\n274\n\n\n\nSilent\n62\n239\n\n\n\nGreatest\n77\n22\n\n\n\n1999\n\n\nGen X\n32\n7\n\n\n\nBoomers\n46\n251\n\n\n\nSilent\n60\n252\n\n\n\nGreatest\n75\n29\n\n\n\n1997\n\n\nGen X\n30\n6\n\n\n\nBoomers\n45\n235\n\n\n\nSilent\n58\n268\n\n\n\nGreatest\n73\n35\n\n\n\n1995\n\n\nGen X\n29\n3\n\n\n\nBoomers\n43\n207\n\n\n\nSilent\n57\n285\n\n\n\nGreatest\n72\n48\n\n\n\n1993\n\n\nBoomers\n42\n164\n\n\n\nSilent\n55\n314\n\n\n\nGreatest\n70\n68\n\n\n\n1991\n\n\nBoomers\n41\n113\n\n\n\nSilent\n54\n331\n\n\n\nGreatest\n69\n104\n\n\n\n1989\n\n\nBoomers\n39\n95\n\n\n\nSilent\n52\n338\n\n\n\nGreatest\n67\n111\n\n\n\nLost\n88\n1\n\n\n\n1987\n\n\nBoomers\n37\n83\n\n\n\nSilent\n50\n330\n\n\n\nGreatest\n66\n130\n\n\n\nLost\n86\n1\n\n\n\n1985\n\n\nBoomers\n36\n67\n\n\n\nSilent\n48\n318\n\n\n\nGreatest\n64\n155\n\n\n\nLost\n84\n1\n\n\n\n1983\n\n\nBoomers\n34\n59\n\n\n\nSilent\n46\n308\n\n\n\nGreatest\n62\n176\n\n\n\nLost\n82\n1\n\n\n\n1981\n\n\nBoomers\n32\n40\n\n\n\nSilent\n45\n305\n\n\n\nGreatest\n60\n199\n\n\n\nLost\n80\n1\n\n\n\n1979\n\n\nBoomers\n31\n18\n\n\n\nSilent\n43\n277\n\n\n\nGreatest\n59\n247\n\n\n\nLost\n80\n2\n\n\n\n1977\n\n\nBoomers\n29\n10\n\n\n\nSilent\n42\n236\n\n\n\nGreatest\n58\n298\n\n\n\nLost\n78\n7\n\n\n\n1975\n\n\nBoomers\n27\n4\n\n\n\nSilent\n40\n189\n\n\n\nGreatest\n57\n343\n\n\n\nLost\n77\n15\n\n\n\n1973\n\n\nSilent\n40\n129\n\n\n\nGreatest\n55\n398\n\n\n\nLost\n75\n21\n\n\n\n1971\n\n\nSilent\n39\n92\n\n\n\nGreatest\n54\n422\n\n\n\nLost\n74\n34\n\n\n\n1969\n\n\nSilent\n37\n65\n\n\n\nGreatest\n53\n441\n\n\n\nLost\n73\n45\n\n\n\n1967\n\n\nSilent\n36\n45\n\n\n\nGreatest\n51\n436\n\n\n\nLost\n71\n60\n\n\n\nMissionary\n87\n2\n\n\n\n1965\n\n\nSilent\n34\n30\n\n\n\nGreatest\n50\n437\n\n\n\nLost\n69\n79\n\n\n\nMissionary\n85\n2\n\n\n\n1963\n\n\nSilent\n32\n17\n\n\n\nGreatest\n49\n415\n\n\n\nLost\n67\n115\n\n\n\nMissionary\n84\n4\n\n\n\n1961\n\n\nSilent\n32\n5\n\n\n\nGreatest\n48\n390\n\n\n\nLost\n66\n154\n\n\n\nMissionary\n82\n9\n\n\n\n1959\n\n\nSilent\n31\n1\n\n\n\nGreatest\n47\n368\n\n\n\nLost\n64\n169\n\n\n\nMissionary\n81\n14\n\n\n\n1957\n\n\nGreatest\n46\n305\n\n\n\nLost\n63\n215\n\n\n\nMissionary\n79\n25\n\n\n\n1955\n\n\nGreatest\n45\n278\n\n\n\nLost\n61\n235\n\n\n\nMissionary\n76\n27\n\n\n\n1953\n\n\nGreatest\n44\n251\n\n\n\nLost\n59\n263\n\n\n\nMissionary\n75\n38\n\n\n\n1951\n\n\nGreatest\n43\n220\n\n\n\nLost\n57\n283\n\n\n\nMissionary\n74\n50\n\n\n\n1949\n\n\nGreatest\n41\n198\n\n\n\nLost\n56\n289\n\n\n\nMissionary\n72\n66\n\n\n\n1947\n\n\nGreatest\n40\n167\n\n\n\nLost\n54\n297\n\n\n\nMissionary\n70\n89\n\n\n\n1945\n\n\nGreatest\n40\n120\n\n\n\nLost\n53\n316\n\n\n\nMissionary\n68\n127\n\n\n\nProgressive\n87\n1\n\n\n\n1943\n\n\nGreatest\n38\n92\n\n\n\nLost\n51\n306\n\n\n\nMissionary\n67\n155\n\n\n\nProgressive\n85\n1\n\n\n\n1941\n\n\nGreatest\n36\n74\n\n\n\nLost\n49\n311\n\n\n\nMissionary\n65\n173\n\n\n\nProgressive\n84\n3\n\n\n\n1939\n\n\nGreatest\n34\n54\n\n\n\nLost\n47\n293\n\n\n\nMissionary\n64\n211\n\n\n\nProgressive\n80\n3\n\n\n\n1937\n\n\nGreatest\n33\n32\n\n\n\nLost\n46\n271\n\n\n\nMissionary\n62\n249\n\n\n\nProgressive\n79\n2\n\n\n\n1935\n\n\nGreatest\n31\n21\n\n\n\nLost\n44\n239\n\n\n\nMissionary\n61\n284\n\n\n\nProgressive\n77\n5\n\n\n\n1933\n\n\nGreatest\n30\n9\n\n\n\nLost\n43\n198\n\n\n\nMissionary\n60\n329\n\n\n\nProgressive\n76\n11\n\n\n\n1931\n\n\nGreatest\n27\n2\n\n\n\nLost\n41\n146\n\n\n\nMissionary\n58\n396\n\n\n\nProgressive\n74\n19\n\n\n\n1929\n\n\nGreatest\n27\n1\n\n\n\nLost\n40\n107\n\n\n\nMissionary\n57\n424\n\n\n\nProgressive\n73\n34\n\n\n\nGilded\n88\n1\n\n\n\n1927\n\n\nLost\n39\n89\n\n\n\nMissionary\n55\n414\n\n\n\nProgressive\n71\n47\n\n\n\nGilded\n86\n1\n\n\n\n1925\n\n\nLost\n37\n69\n\n\n\nMissionary\n54\n419\n\n\n\nProgressive\n69\n59\n\n\n\nGilded\n84\n1\n\n\n\n1923\n\n\nLost\n36\n49\n\n\n\nMissionary\n52\n431\n\n\n\nProgressive\n68\n74\n\n\n\nGilded\n84\n3\n\n\n\n1921\n\n\nLost\n36\n31\n\n\n\nMissionary\n50\n418\n\n\n\nProgressive\n66\n107\n\n\n\nGilded\n83\n4\n\n\n\n1919\n\n\nLost\n33\n19\n\n\n\nMissionary\n49\n412\n\n\n\nProgressive\n65\n119\n\n\n\nGilded\n80\n5\n\n\n\n\nNote: Mean Age is rounded to nearest whole year. Distribution plots represent age distributions within each generation.\n\n\n\n\n\n\n\n\n\nI’m fairly satisfied with the output. The inline violin charts were especially satisfying to make, and I’m happy with the availability of all the different customization options, while simultaneously there are several quick theme options to make styling even easier. That said, the syntax is less intuitive than I would prefer, I was truly hoping for a more ggplot-like experience."
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "This exercise will load, process, and explore a text dataset that consists of employee reviews of their current and former employers on LinkedIn. The dataset can be found from Kaggle here.\nStarting with loading our packages, tidyverse for general cleaning, jsonlite to bring in our Json file, and here to make directory referencing easier.\n\npacman::p_load(tidyverse, jsonlite, here, stringr, superml)\n\nNow we will load our data. Json files are not generally square or in a data frame format, but the fromJSON function makes this tremendously easy.\n\nemp_rev &lt;- fromJSON(here('data-exercise', 'employer-reviews.json'))\nhead(emp_rev)\n\n                      ReviewTitle\n1                      Productive\n2                       Stressful\n3 Good Company for Every employee\n4                      Productive\n5                  Non productive\n6                            Good\n                                                                                                                                                                                                                                                                                                                    CompleteReview\n1                                                                                                                                                              Good company, cool workplace, work load little bit higher. Clean environment, disciplined, good cantin, big campus, systematic workflow, lenient but reliable firm.\n2 1. Need to work on boss's whims and fancies 2. Priorities keep changing 3. No regards for work life balance 4. Completing the work before time is stressed too much than completing it well. Involves lots of reworking, blame games; etc. 5. No job boundaries, you will be asked to do any work depending on the requirements.\n3                                                                                                                                                   Good company for every Engineers dream, Full Mediclaim for entired family, Free transport services from company location to home, Township culture for employees,job security.\n4                                                                                                                                                                         I am just pass out bsc in chemistry Typical day at work Mangement Work place good The most enjoyable part of the job I learn more information in company\n5                                                                                                                                                                     Not so fun at work just blame games  Target people and less target at work Paid less  No increment Make you feel low Too much stress  No one understands you\n6                                                                                                                                                                      I working as laboratory technician form last one year in covid19 staff but we are not appreciate of any about awards we also here for work work and work ..\n                                                        URL Rating\n1 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    3.0\n2 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    3.0\n3 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    5.0\n4 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    5.0\n5 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    1.0\n6 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    3.0\n                                            ReviewDetails\n1     (Current Employee)  -  Ghansoli  -  August 30, 2021\n2               (Former Employee)  -   -  August 26, 2021\n3               (Former Employee)  -   -  August 17, 2021\n4              (Current Employee)  -   -  August 17, 2021\n5                (Former Employee)  -   -  August 9, 2021\n6 (Current Employee)  -  Dahej, Gujarat  -  July 22, 2021\n\nstr(emp_rev)\n\n'data.frame':   145209 obs. of  5 variables:\n $ ReviewTitle   : chr  \"Productive\" \"Stressful\" \"Good Company for Every employee\" \"Productive\" ...\n $ CompleteReview: chr  \"Good company, cool workplace, work load little bit higher. Clean environment, disciplined, good cantin, big cam\"| __truncated__ \"1. Need to work on boss's whims and fancies 2. Priorities keep changing 3. No regards for work life balance 4. \"| __truncated__ \"Good company for every Engineers dream, Full Mediclaim for entired family, Free transport services from company\"| __truncated__ \"I am just pass out bsc in chemistry Typical day at work Mangement Work place good The most enjoyable part of th\"| __truncated__ ...\n $ URL           : chr  \"https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews\" \"https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews\" \"https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews\" \"https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews\" ...\n $ Rating        : chr  \"3.0\" \"3.0\" \"5.0\" \"5.0\" ...\n $ ReviewDetails : chr  \"(Current Employee)  -  Ghansoli  -  August 30, 2021\" \"(Former Employee)  -   -  August 26, 2021\" \"(Former Employee)  -   -  August 17, 2021\" \"(Current Employee)  -   -  August 17, 2021\" ...\n\nsummary(emp_rev)\n\n ReviewTitle        CompleteReview         URL               Rating         \n Length:145209      Length:145209      Length:145209      Length:145209     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ReviewDetails     \n Length:145209     \n Class :character  \n Mode  :character  \n\n\nLooking at the columns, we will want to do some cleanup on some of the more categorical ones. Starting with the URL, this may contain information about the employer, which we can extract. First i want to confirm that all the urls start the same way.\n\nsubstr(emp_rev$URL, 1, 26) %&gt;% unique() #substring extracts first 26 characters,\n\n[1] \"https://in.indeed.com/cmp/\"\n\n#unique tells us all of the unique values in the substring'd column\nlength(unique(emp_rev$URL)) #tells us the number of potential company names\n\n[1] 7286\n\n\nNext, I will use some substrings and regex to extract the company name after the above url portion.\n\nd1 &lt;- emp_rev %&gt;% mutate(\n  CompNm = (substr(URL, 27, nchar(URL)) %&gt;% str_extract('.*(?=/)') %&gt;% str_replace_all('-',' '))\n)\n#substring removes the first part of the url, since its always the same at 27 characters\n#str_extract looks for and extracts the first set of characters before the \"/\"\n#str_replace_all removes all of the dashes and replaces them with spaces\nd1$CompNm %&gt;% unique()\n\n [1] \"Reliance Industries Ltd\"         \"Mphasis\"                        \n [3] \"Kpmg\"                            \"Yes Bank\"                       \n [5] \"Sutherland\"                      \"Marriott International, Inc.\"   \n [7] \"DHL\"                             \"Jio\"                            \n [9] \"Vodafoneziggo\"                   \"HP\"                             \n[11] \"Maersk\"                          \"Ride.swiggy\"                    \n[13] \"Jll\"                             \"Alstom\"                         \n[15] \"UnitedHealth Group\"              \"Tata Consultancy Services (tcs)\"\n[17] \"Capgemini\"                       \"Teleperformance\"                \n[19] \"Cognizant Technology Solutions\"  \"Mahindra & Mahindra Ltd\"        \n[21] \"L&T Technology Services Ltd.\"    \"Bharti Airtel Limited\"          \n[23] \"Indeed\"                          \"Hyatt\"                          \n[25] \"Icici Prudential Life Insurance\" \"Accenture\"                      \n[27] \"Honeywell\"                       \"Standard Chartered Bank\"        \n[29] \"Nokia\"                           \"Apollo Hospitals\"               \n[31] \"Tata Aia Life\"                   \"Hdfc Bank\"                      \n[33] \"Bosch\"                           \"Deloitte\"                       \n[35] \"Ey\"                              \"Microsoft\"                      \n[37] \"Barclays\"                        \"JPMorgan Chase\"                 \n[39] \"Muthoot Finance\"                 \"Wns Global Services\"            \n[41] \"Kotak Mahindra Bank\"             \"Infosys\"                        \n[43] \"Oracle\"                          \"Byju's\"                         \n[45] \"Deutsche Bank\"                   \"Hinduja Global Solutions\"       \n[47] \"Ericsson\"                        \"Axis Bank\"                      \n[49] \"IBM\"                             \"Concentrix\"                     \n[51] \"Wells Fargo\"                     \"Google\"                         \n[53] \"Dell Technologies\"               \"Facebook\"                       \n[55] \"Amazon.com\"                      \"Flipkart.com\"                   \n[57] \"American Express\"                \"Citi\"                           \n[59] \"HSBC\"                           \n\n\nThis is cheating a little bit, because I counted the number of characters in the first part of the URL manually, meaning this is not the most robust way to identify the company name, but observing our values it does not look like it caused any problems.\nNext let’s look at the Rating. When the file was read in it looks like it was read as a string, but it would be more useful to us as a number. We’ll start with a quick summary and completeness check.\n\nd1$Rating %&gt;% summary() #summary of variable, understand scope\n\n   Length     Class      Mode \n   145209 character character \n\nsum(d1$Rating=='') + sum(is.na(d1$Rating)) #counts empty and missing values\n\n[1] 0\n\nd1$Rating %&gt;% unique()\n\n[1] \"3.0\" \"5.0\" \"1.0\" \"4.0\" \"2.0\"\n\n\nSo the variable is a string, but does not contain any missing or null values. All of the values fall under the five-point scale, so it should be safe to convert to a number.\n\nd2 &lt;- d1 %&gt;% mutate(\n  Rating = as.numeric(Rating)\n) # converts the Rating variable to numeric and saves it to the same variable.\n\nOne last variable to look at, ReviewDetails. This looks to have three parts to it. The status of the employee, the location, and the date the review was done. I’m most interested in the status for this exercise, but let’s see if we can get all three\n\n#str_split() breaks up the column by the dashes\n#simplify = TRUE turns it into a matrix\n# dim() gives us the number of rows and columns of the matrix, expecting 3 cols\nstr_split(emp_rev$ReviewDetails, '-', simplify = TRUE) %&gt;% dim()\n\n[1] 145209      5\n\n\nThe intention was to split the variable by dashes to create three columns, however it looks like there are some values that contain a dash themselves. This causes two additional columns to appear, so we will have to make some adjustments.\n\n#check for number of columns\nstr_split(emp_rev$ReviewDetails, ' -  ', simplify = TRUE) %&gt;% dim() \n\n[1] 145209      3\n\n#check for number of unique employee statuses\nstr_split(emp_rev$ReviewDetails, ' -  ', simplify = TRUE)[,1] %&gt;% unique() %&gt;% head(10)\n\n [1] \"(Current Employee) \"                                   \n [2] \"(Former Employee) \"                                    \n [3] \"Training   (Former Employee) \"                         \n [4] \"Officer   (Former Employee) \"                          \n [5] \"Leader   (Current Employee) \"                          \n [6] \"health care   (Current Employee) \"                     \n [7] \"Good team worker   (Former Employee) \"                 \n [8] \"Officer   (Current Employee) \"                         \n [9] \"Sr.G.M.Engineering and projects .   (Former Employee) \"\n[10] \"Hospitality   (Former Employee) \"                      \n\n#check for number of empty values\nifelse(str_split(emp_rev$ReviewDetails, ' -  ', simplify = TRUE)[,3] =='',1,0) %&gt;% sum()\n\n[1] 0\n\n\nFortunately, the solution is easier than it first appeared. Originally I was going to approach the split by splitting from the left and the right for Employee Status and Review Date, then removing everything thats in the left and right for location. However, the dashes that split the different details would have two additional spaces after each, so if we include that in the split function we can get the result we are looking for.\nThe Employee Status section fo the Details field looks to have more than just the status for some observations. A quick check might be worth it to see if Employee Title would be worth pursuing.\n\n# splits the columns then checks the first column for the employee status values,\n#then counts those that don't fall into the status value only\nemp_rev %&gt;%\n  mutate(\n    Stat = str_split(ReviewDetails, ' -  ', simplify = TRUE)[,1],\n    Stat = ifelse(Stat %in% c('(Current Employee) ', '(Former Employee) '), 0, 1)\n      ) %&gt;% select(Stat) %&gt;% sum()\n\n[1] 523\n\n\nWith only 523 observations that fall outside of the Current or Former employee status, it’s relatively safe to ignore that part of the ReviewDetails field.\n\n#saves the split column into three new variables.\n#Review Date is tranformed into date format\n#Employee status uses str_extract to get the status vales only\n#Location uses trimws() to remove extrenuous blanks\nd3 &lt;- d2 %&gt;% mutate(\n  ReviewDate = (str_split(ReviewDetails, ' -  ', simplify = TRUE)[,3] %&gt;% \n                  parse_date_time('0m d, y')),\n  EmployeeStatus = (str_split(ReviewDetails, ' -  ', simplify = TRUE)[,1] %&gt;% \n                      str_extract('(Current Employee)|(Former Employee)')),\n  Location = (str_split(ReviewDetails, ' -  ', simplify = TRUE)[,2] %&gt;% \n                trimws())\n  )\n# checks for how many values actually have a location. \n#Primarily to check if the column is worth using\nd3$Location %&gt;% unique() %&gt;% length()\n\n[1] 3780\n\n#Checks to make sure only two values are in the status\nd3$EmployeeStatus %&gt;% unique() %&gt;% length()\n\n[1] 2\n\n\n\n#Null and empty checks for new columns. \n#DatNull does not check for empties because of date format limitation\nd3 %&gt;% summarize(\n  StatNull = sum(EmployeeStatus == '') + sum(is.na(EmployeeStatus)),\n  DatNull = sum(is.na(ReviewDate)),\n  LocNull = sum(Location=='') + sum(is.na(Location))\n)\n\n  StatNull DatNull LocNull\n1        0       0  129942\n\n\nLocation is a pretty empty field, so it can largely be ignored, otherwise our other two variables look great. From here we can move on to the Review text itself.\n\n#lower cases the full review text\nd3 &lt;- d3 %&gt;% mutate(\n  CompleteReview = tolower(CompleteReview)\n)\n\nBefore we do anything we do anything with the reviews, the dataset is huge, and since the next step involves creating a bag of words it would probably be a good idea to filter the dataset. We will pick two companies to filter to as our companies of interest. First let’s look at the number of reviews by company.\n\n#checks the count of reviews by company name\nd3 %&gt;% group_by(CompNm) %&gt;% \n  summarize(\n    cnt = n()\n  ) %&gt;% arrange(-cnt)\n\n# A tibble: 59 × 2\n   CompNm                            cnt\n   &lt;chr&gt;                           &lt;int&gt;\n 1 Tata Consultancy Services (tcs) 14441\n 2 IBM                             10820\n 3 Infosys                         10696\n 4 Accenture                       10137\n 5 Cognizant Technology Solutions   9626\n 6 Hdfc Bank                        6749\n 7 Capgemini                        5248\n 8 Amazon.com                       3385\n 9 L&T Technology Services Ltd.     3226\n10 Concentrix                       3162\n# ℹ 49 more rows\n\n\nLooking at the size, HP and Dell Technologies look pretty reasonable, so we can filter to those two and compare.\n\n#filters to reviews for HP and Dell Technologies, saves to new df\nd4 &lt;- d3 %&gt;% filter(CompNm %in% c('Dell Technologies', 'HP'))\n\nThe next step will create a ‘bag of words’ commonly used for machine learning, but we’re going to use it this time for to get summary information about scores based on the appearance of words.\n\n#initializes the class for CountVectorizer. \n#Only looking at top 100 most frequently used words\ncfv &lt;- CountVectorizer$new(max_features = 100)\n#Transforms the occurence of each word across all reviews into a vector\ncf_mat &lt;- cfv$fit_transform(d4$CompleteReview)\n#transposed for readability\nhead(cf_mat) %&gt;% t()\n\n              [,1] [,2] [,3] [,4] [,5] [,6]\nwork             1    1    1    1    1    0\ngood             1    0    0    4    1    0\nmanagement       0    0    0    1    0    0\ncompany          2    0    1    2    0    0\nplace            0    1    0    0    0    2\nteam             0    0    0    0    0    0\ngreat            0    1    0    0    0    0\nhp               0    1    2    0    1    0\nworking          0    0    0    0    0    1\ndell             0    0    0    0    0    0\njob              0    0    1    0    0    0\nculture          0    0    0    0    0    0\nlife             0    0    0    0    0    0\nenvironment      0    0    0    0    1    0\nlot              0    0    0    0    1    0\npart             0    0    0    0    0    0\nlearn            0    0    0    0    0    2\nbalance          0    0    0    0    0    0\nnew              0    0    0    0    0    0\nfun              0    0    0    0    1    0\nday              0    0    0    0    0    2\nexperience       0    0    0    0    1    1\nbest             0    0    0    0    0    1\ntime             0    0    0    0    0    0\nlearned          0    0    0    0    0    0\nco               0    0    0    0    0    0\nfriendly         1    0    0    2    0    0\nlearning         0    0    0    0    2    0\nemployees        0    0    0    1    0    0\npeople           0    0    0    0    0    0\nemployee         0    0    0    1    0    0\nprocess          0    0    0    0    0    0\nthings           0    0    0    0    0    0\ncan              0    0    0    2    0    0\nworkers          0    0    0    0    0    0\nnice             0    0    0    1    0    0\ns                0    1    0    0    0    0\ncustomer         1    0    0    0    0    1\nwill             0    0    0    1    0    0\nsupport          0    0    0    0    0    1\nalso             0    0    0    0    0    0\none              0    0    0    0    0    0\ncareer           0    1    0    0    0    0\nlike             0    0    0    1    0    0\nget              0    0    2    0    0    0\nmany             0    0    0    0    0    0\ngrowth           0    0    1    0    0    1\nworked           0    0    0    0    1    0\nwell             0    0    0    0    0    0\nskills           0    0    1    0    0    0\nlearnt           0    0    0    0    0    0\nsalary           1    0    0    0    0    0\nevery            0    0    0    0    1    0\nenjoyable        0    0    0    0    0    0\ntraining         0    0    0    0    0    0\nknowledge        0    0    1    0    0    1\ntechnical        0    0    1    0    0    0\nalways           0    0    0    0    0    0\ncustomers        0    0    0    0    0    0\nopportunities    0    0    0    0    0    0\nhardest          0    0    0    0    0    0\nsupportive       0    0    0    0    0    0\nexcellent        0    0    0    0    0    0\nbusiness         0    0    0    0    0    0\nopportunity      0    0    1    0    0    0\nreally           0    1    0    1    0    0\nyears            0    0    0    0    0    0\ndifferent        0    0    0    0    0    0\ngrow             0    0    0    0    0    0\nmanagers         0    0    0    0    0    1\nissues           0    0    0    0    0    0\nactivities       0    0    0    0    1    0\nmuch             0    0    0    0    0    0\ngot              1    0    0    0    0    0\nproject          0    0    0    1    0    0\nhelp             0    0    0    0    0    0\nfirst            0    0    0    0    0    0\nmanager          0    0    0    0    0    0\nclient           0    0    0    0    0    0\npressure         0    0    0    0    0    0\nprofessional     0    0    0    0    0    0\nflexible         0    0    0    0    0    0\nus               0    0    0    0    0    0\ntechnologies     0    0    0    0    0    0\nhelpful          0    0    0    0    0    0\norganization     0    0    0    0    0    0\nenjoyed          0    0    0    0    0    0\nhandling         0    0    0    0    0    0\nworkplace        0    0    0    0    0    0\nbenefits         1    0    0    0    0    0\ntechnology       0    0    0    0    0    0\nsales            0    0    0    0    0    0\noverall          0    0    0    0    1    0\nprojects         0    0    0    0    0    0\nservice          0    0    0    0    0    0\ntypical          0    0    0    0    0    0\namazing          0    0    0    0    0    0\nneed             0    0    0    0    0    0\nhome             0    0    0    0    0    0\nservices         0    0    0    0    0    0\n\n\nNow we combine the bag of words matrix to the dataframe to make summarizing a bit easier\n\n#combines bag of words with orginal data frame\nd5 &lt;- cbind(d4, cf_mat)\n\nWe can take a look at the average score for each word for both companies. Note that the average score is weighted by the number of appearances of a word, that is to say that if a word appears multiple times in a review the score will have a greater weight.\n\n#Multiples the rating by the appearance of each word, then sums that up for each word\n#Then it divides by the total number of appearances of that word\n((d5$Rating * d5[,10:109]) %&gt;% colSums())/(colSums(d5[,10:109]))\n\n         work          good    management       company         place \n     4.239728      4.152515      4.140500      4.208420      4.260095 \n         team         great            hp       working          dell \n     4.221481      4.352092      4.276878      4.249440      4.292874 \n          job       culture          life   environment           lot \n     4.180314      4.321168      4.286114      4.289959      4.220779 \n         part         learn       balance           new           fun \n     4.174620      4.230942      4.242068      4.264916      4.288538 \n          day    experience          best          time       learned \n     4.173973      4.273743      4.497076      4.168182      4.176380 \n           co      friendly      learning     employees        people \n     4.150769      4.305772      4.239370      4.270799      4.191736 \n     employee       process        things           can       workers \n     4.270408      4.169811      4.235741      4.199620      4.158397 \n         nice             s      customer          will       support \n     4.212644      4.127413      4.300797      4.062000      4.235772 \n         also           one        career          like           get \n     4.271967      4.256356      4.267094      4.221729      4.159251 \n         many        growth        worked          well        skills \n     4.176755      4.106436      4.270471      4.308458      4.300752 \n       learnt        salary         every     enjoyable      training \n     4.251256      3.877863      4.318421      4.171123      4.329640 \n    knowledge     technical        always     customers opportunities \n     4.189944      4.247887      4.219373      4.272206      4.262537 \n      hardest    supportive     excellent      business   opportunity \n     4.088496      4.281899      4.489552      4.247678      4.263492 \n       really         years     different          grow      managers \n     4.290657      4.163763      4.184397      4.370107      4.306859 \n       issues    activities          much           got       project \n     4.408397      4.310345      3.923077      4.315175      4.003922 \n         help         first       manager        client      pressure \n     4.360324      4.258621      4.210526      4.136564      4.053097 \n professional      flexible            us  technologies       helpful \n     4.355556      4.247748      4.325792      4.239819      4.303167 \n organization       enjoyed      handling     workplace      benefits \n     4.319444      4.280374      4.202830      4.241706      4.142180 \n   technology         sales       overall      projects       service \n     4.204762      4.328502      4.082927      4.113300      4.336634 \n      typical       amazing          need          home      services \n     4.080808      4.520202      3.953846      4.226804      4.300518 \n\n\nNow lets split up the data by company and see if there are any differences.\n\n#Standard filters saved as new data frames\nDell &lt;- d5 %&gt;% filter(CompNm == 'Dell Technologies')\nHP &lt;- d5 %&gt;% filter(CompNm == 'HP')\n\nWe can use the same logic as before to get the average score by word, but for each of our new dataframes for each company.\n\n#Finds the average score by word for Dell. Saves as dataframe\nDell_scores &lt;- ((Dell$Rating * Dell[,10:109]) %&gt;%\n                  colSums())/ (colSums(Dell[,10:109])) %&gt;% \n  as.data.frame()\n\n#Finds the average score by word for HP. Saves as dataframe\nHP_scores &lt;- ((HP$Rating * HP[,10:109]) %&gt;% \n     colSums())/(colSums(HP[,10:109])) %&gt;% \n  as.data.frame()\n\nDell_scores %&gt;% head()\n\n                  .\nwork       4.210822\ngood       4.118130\nmanagement 4.078498\ncompany    4.172018\nplace      4.226161\nteam       4.212121\n\nHP_scores %&gt;% head()\n\n                  .\nwork       4.264388\ngood       4.181271\nmanagement 4.189117\ncompany    4.238593\nplace      4.292148\nteam       4.229529\n\n\nNow we use these new data frames to check the difference in score for each word. This could be a potential way of identifying certain areas that employees of one company like or dislike more than employees of the other company.\n\n#determines the difference in score each word\nword_scores &lt;- Dell_scores - HP_scores\n#updates the name of the score difference so it can be referenced\ncolnames(word_scores) &lt;- c('score_diff')\n#sorts by the word score\nword_scores &lt;- word_scores %&gt;% arrange(-score_diff)\n#creates a new variable called word, easier to reference than row names\nword_scores$word &lt;- rownames(word_scores)\n\nhead(word_scores)\n\n          score_diff      word\ndell       0.7941176      dell\nfirst      0.2670235     first\nhp         0.2234513        hp\ndifferent  0.1577160 different\nhome       0.1491707      home\nalso       0.1454323      also\n\n\nUltimately we find the scores to not be so different, but we can still see words that tend to “lead” to higher scores than others. Interestingly, a review that includes either of the words Dell or HP would tend to be higher for Dell than HP,\n\n#plots the difference in score by word.\nword_scores %&gt;% head(sum(word_scores$score_diff&gt;0)) %&gt;% ggplot() +\n  geom_col(aes(y=fct_reorder(word, score_diff),x=score_diff)) +\n  labs(y='Word', x='Score Difference', title = 'Dell-HP Score Difference by Word') +\n  theme(axis.text.y = element_text(size=6, angle = 25))\n\n\n\n\n\n\n\n\nThe methodology used here was not terribly robust, so certainly more research could be done. For example using n-grams might get an idea if certain short phrases are more telling, or grouping known “problem” words for one company or another to see if they are mentioned in each others surveys. Other options would be filtering by a score range and seeing what words appear for more positive or negative results."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "The CDC data set I have chosen for this exercise is the Tobacco Usage Behavioral Risk Factor data set. The data comes from a survey administered by the CDC as part of the State Tobacco Activities Tracking and Evaluation (STATE) system. The data set contains information about the year the survey was administered, respondent location (State or territory), demographics (namely Race, Age, Education, and Gender), and it asks questions about a person’s tobacco usage status (current, former, never used), their cessation status (whether they have quit or attempted to quit in the last year), and the frequency that they use tobacco for those that do (daily, some days). The usage questions are asked for three different types of Tobacco products, Cigarettes, Smokeless Tobacco, and E-cigarettes.\n\npacman::p_load(here, tidyverse, skimr, plotly, synthpop, patchwork)\n\nInstalling package into 'C:/Users/Hylti/AppData/Local/R/win-library/4.3'\n(as 'lib' is unspecified)\n\n\nalso installing the dependencies 'TH.data', 'libcoin', 'multcomp', 'truncnorm', 'modeltools', 'strucchange', 'coin', 'sandwich', 'cmm', 'Rsolnp', 'party', 'proto', 'polspline', 'randomForest', 'ranger', 'mipfp', 'rmutil', 'broman'\n\n\nWarning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3:\n  cannot open URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3/PACKAGES'\n\n\npackage 'TH.data' successfully unpacked and MD5 sums checked\npackage 'libcoin' successfully unpacked and MD5 sums checked\npackage 'multcomp' successfully unpacked and MD5 sums checked\npackage 'truncnorm' successfully unpacked and MD5 sums checked\npackage 'modeltools' successfully unpacked and MD5 sums checked\npackage 'strucchange' successfully unpacked and MD5 sums checked\npackage 'coin' successfully unpacked and MD5 sums checked\npackage 'sandwich' successfully unpacked and MD5 sums checked\npackage 'cmm' successfully unpacked and MD5 sums checked\npackage 'Rsolnp' successfully unpacked and MD5 sums checked\npackage 'party' successfully unpacked and MD5 sums checked\npackage 'proto' successfully unpacked and MD5 sums checked\npackage 'polspline' successfully unpacked and MD5 sums checked\npackage 'randomForest' successfully unpacked and MD5 sums checked\npackage 'ranger' successfully unpacked and MD5 sums checked\npackage 'mipfp' successfully unpacked and MD5 sums checked\npackage 'rmutil' successfully unpacked and MD5 sums checked\npackage 'broman' successfully unpacked and MD5 sums checked\npackage 'synthpop' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\Hylti\\AppData\\Local\\Temp\\RtmpsZ5jhL\\downloaded_packages\n\n\n\nsynthpop installed\n\n\nWarning: package 'synthpop' was built under R version 4.3.3\n\n\n\nrawdata &lt;- read_csv('cdc-data-raw.csv')\n\nRows: 43341 Columns: 31\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (25): YEAR, LocationAbbr, LocationDesc, TopicType, TopicDesc, MeasureDes...\ndbl  (6): Data_Value, Data_Value_Std_Err, Low_Confidence_Limit, High_Confide...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nstr(rawdata) # getting an idea for data structure\n\nspc_tbl_ [43,341 × 31] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ YEAR                      : chr [1:43341] \"2017\" \"2018\" \"2017\" \"2016\" ...\n $ LocationAbbr              : chr [1:43341] \"GU\" \"US\" \"US\" \"GU\" ...\n $ LocationDesc              : chr [1:43341] \"Guam\" \"National Median (States and DC)\" \"National Median (States and DC)\" \"Guam\" ...\n $ TopicType                 : chr [1:43341] \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" ...\n $ TopicDesc                 : chr [1:43341] \"Cigarette Use (Adults)\" \"Cigarette Use (Adults)\" \"Cigarette Use (Adults)\" \"Smokeless Tobacco Use (Adults)\" ...\n $ MeasureDesc               : chr [1:43341] \"Current Smoking\" \"Smoking Status\" \"Smoking Status\" \"Current Use\" ...\n $ DataSource                : chr [1:43341] \"BRFSS\" \"BRFSS\" \"BRFSS\" \"BRFSS\" ...\n $ Response                  : chr [1:43341] NA \"Current\" \"Never\" NA ...\n $ Data_Value_Unit           : chr [1:43341] \"%\" \"%\" \"%\" \"%\" ...\n $ Data_Value_Type           : chr [1:43341] \"Percentage\" \"Percentage\" \"Percentage\" \"Percentage\" ...\n $ Data_Value                : num [1:43341] 30 16.1 58.2 0.5 21.9 21.6 2.8 3.6 29.2 14.8 ...\n $ Data_Value_Footnote_Symbol: chr [1:43341] NA NA NA NA ...\n $ Data_Value_Footnote       : chr [1:43341] NA NA NA NA ...\n $ Data_Value_Std_Err        : num [1:43341] 2.3 NA NA 0.4 4.9 0.8 0.3 0.4 2.4 0.6 ...\n $ Low_Confidence_Limit      : num [1:43341] 25.4 NA NA 0 12.4 20 2.2 2.7 24.5 13.6 ...\n $ High_Confidence_Limit     : num [1:43341] 34.6 NA NA 1.2 31.4 23.2 3.4 4.5 33.9 16 ...\n $ Sample_Size               : num [1:43341] 692 NA NA 123 209 ...\n $ Gender                    : chr [1:43341] \"Male\" \"Overall\" \"Overall\" \"Overall\" ...\n $ Race                      : chr [1:43341] \"All Races\" \"All Races\" \"All Races\" \"Hispanic\" ...\n $ Age                       : chr [1:43341] \"All Ages\" \"All Ages\" \"All Ages\" \"All Ages\" ...\n $ Education                 : chr [1:43341] \"All Grades\" \"All Grades\" \"All Grades\" \"All Grades\" ...\n $ GeoLocation               : chr [1:43341] \"(13.444304, 144.793731)\" NA NA \"(13.444304, 144.793731)\" ...\n $ TopicTypeId               : chr [1:43341] \"BEH\" \"BEH\" \"BEH\" \"BEH\" ...\n $ TopicId                   : chr [1:43341] \"100BEH\" \"100BEH\" \"100BEH\" \"150BEH\" ...\n $ MeasureId                 : chr [1:43341] \"110CSA\" \"165SSA\" \"165SSA\" \"177SCU\" ...\n $ StratificationID1         : chr [1:43341] \"2GEN\" \"1GEN\" \"1GEN\" \"1GEN\" ...\n $ StratificationID2         : chr [1:43341] \"8AGE\" \"8AGE\" \"8AGE\" \"8AGE\" ...\n $ StratificationID3         : chr [1:43341] \"6RAC\" \"6RAC\" \"6RAC\" \"4RAC\" ...\n $ StratificationID4         : chr [1:43341] \"6EDU\" \"6EDU\" \"6EDU\" \"6EDU\" ...\n $ SubMeasureID              : chr [1:43341] \"BRF21\" \"BRF27\" \"BRF28\" \"BRF69\" ...\n $ DisplayOrder              : num [1:43341] 21 27 28 69 22 21 77 71 26 27 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   YEAR = col_character(),\n  ..   LocationAbbr = col_character(),\n  ..   LocationDesc = col_character(),\n  ..   TopicType = col_character(),\n  ..   TopicDesc = col_character(),\n  ..   MeasureDesc = col_character(),\n  ..   DataSource = col_character(),\n  ..   Response = col_character(),\n  ..   Data_Value_Unit = col_character(),\n  ..   Data_Value_Type = col_character(),\n  ..   Data_Value = col_double(),\n  ..   Data_Value_Footnote_Symbol = col_character(),\n  ..   Data_Value_Footnote = col_character(),\n  ..   Data_Value_Std_Err = col_double(),\n  ..   Low_Confidence_Limit = col_double(),\n  ..   High_Confidence_Limit = col_double(),\n  ..   Sample_Size = col_double(),\n  ..   Gender = col_character(),\n  ..   Race = col_character(),\n  ..   Age = col_character(),\n  ..   Education = col_character(),\n  ..   GeoLocation = col_character(),\n  ..   TopicTypeId = col_character(),\n  ..   TopicId = col_character(),\n  ..   MeasureId = col_character(),\n  ..   StratificationID1 = col_character(),\n  ..   StratificationID2 = col_character(),\n  ..   StratificationID3 = col_character(),\n  ..   StratificationID4 = col_character(),\n  ..   SubMeasureID = col_character(),\n  ..   DisplayOrder = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(rawdata) # summary statistics\n\n     YEAR           LocationAbbr       LocationDesc        TopicType        \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  TopicDesc         MeasureDesc         DataSource          Response        \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n Data_Value_Unit    Data_Value_Type      Data_Value   \n Length:43341       Length:43341       Min.   : 0.00  \n Class :character   Class :character   1st Qu.: 5.10  \n Mode  :character   Mode  :character   Median :17.10  \n                                       Mean   :25.36  \n                                       3rd Qu.:38.90  \n                                       Max.   :99.90  \n                                       NA's   :2117   \n Data_Value_Footnote_Symbol Data_Value_Footnote Data_Value_Std_Err\n Length:43341               Length:43341        Min.   : 0.000    \n Class :character           Class :character    1st Qu.: 0.600    \n Mode  :character           Mode  :character    Median : 1.100    \n                                                Mean   : 1.711    \n                                                3rd Qu.: 2.200    \n                                                Max.   :16.600    \n                                                NA's   :2195      \n Low_Confidence_Limit High_Confidence_Limit  Sample_Size       Gender         \n Min.   : 0.00        Min.   :  0.00        Min.   :   50   Length:43341      \n 1st Qu.: 3.20        1st Qu.:  6.90        1st Qu.:  480   Class :character  \n Median :14.30        Median : 19.80        Median : 1798   Mode  :character  \n Mean   :22.01        Mean   : 28.68        Mean   : 3050                     \n 3rd Qu.:30.50        3rd Qu.: 47.40        3rd Qu.: 4203                     \n Max.   :99.80        Max.   :100.00        Max.   :40726                     \n NA's   :2195         NA's   :2195          NA's   :2195                      \n     Race               Age             Education         GeoLocation       \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n TopicTypeId          TopicId           MeasureId         StratificationID1 \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n StratificationID2  StratificationID3  StratificationID4  SubMeasureID      \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  DisplayOrder  \n Min.   : 5.00  \n 1st Qu.:24.00  \n Median :51.00  \n Mean   :47.08  \n 3rd Qu.:71.00  \n Max.   :81.00  \n                \n\nskim(rawdata) # primarily to get completion rates\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n43341\n\n\nNumber of columns\n31\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n25\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nYEAR\n0\n1.00\n4\n9\n0\n17\n0\n\n\nLocationAbbr\n0\n1.00\n2\n2\n0\n54\n0\n\n\nLocationDesc\n0\n1.00\n4\n31\n0\n54\n0\n\n\nTopicType\n0\n1.00\n25\n25\n0\n1\n0\n\n\nTopicDesc\n0\n1.00\n18\n30\n0\n4\n0\n\n\nMeasureDesc\n0\n1.00\n11\n59\n0\n10\n0\n\n\nDataSource\n0\n1.00\n5\n5\n0\n1\n0\n\n\nResponse\n28323\n0.35\n5\n11\n0\n6\n0\n\n\nData_Value_Unit\n0\n1.00\n1\n1\n0\n1\n0\n\n\nData_Value_Type\n0\n1.00\n10\n10\n0\n1\n0\n\n\nData_Value_Footnote_Symbol\n41224\n0.05\n1\n1\n0\n1\n0\n\n\nData_Value_Footnote\n41224\n0.05\n71\n71\n0\n1\n0\n\n\nGender\n0\n1.00\n4\n7\n0\n3\n0\n\n\nRace\n0\n1.00\n5\n29\n0\n6\n0\n\n\nAge\n0\n1.00\n8\n18\n0\n8\n0\n\n\nEducation\n0\n1.00\n10\n12\n0\n4\n0\n\n\nGeoLocation\n78\n1.00\n23\n41\n0\n53\n0\n\n\nTopicTypeId\n0\n1.00\n3\n3\n0\n1\n0\n\n\nTopicId\n0\n1.00\n6\n6\n0\n4\n0\n\n\nMeasureId\n0\n1.00\n6\n6\n0\n14\n0\n\n\nStratificationID1\n0\n1.00\n4\n4\n0\n3\n0\n\n\nStratificationID2\n0\n1.00\n4\n4\n0\n8\n0\n\n\nStratificationID3\n0\n1.00\n4\n4\n0\n6\n0\n\n\nStratificationID4\n0\n1.00\n4\n4\n0\n4\n0\n\n\nSubMeasureID\n0\n1.00\n5\n5\n0\n53\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nData_Value\n2117\n0.95\n25.36\n25.00\n0\n5.1\n17.1\n38.9\n99.9\n▇▃▂▁▁\n\n\nData_Value_Std_Err\n2195\n0.95\n1.71\n1.72\n0\n0.6\n1.1\n2.2\n16.6\n▇▁▁▁▁\n\n\nLow_Confidence_Limit\n2195\n0.95\n22.01\n24.14\n0\n3.2\n14.3\n30.5\n99.8\n▇▂▂▁▁\n\n\nHigh_Confidence_Limit\n2195\n0.95\n28.68\n26.21\n0\n6.9\n19.8\n47.4\n100.0\n▇▃▂▂▁\n\n\nSample_Size\n2195\n0.95\n3050.17\n3798.87\n50\n480.0\n1798.0\n4203.0\n40726.0\n▇▁▁▁▁\n\n\nDisplayOrder\n0\n1.00\n47.08\n24.49\n5\n24.0\n51.0\n71.0\n81.0\n▂▇▁▂▇\n\n\n\n\n\n\nhead(rawdata, 20) # getting first 20 rows\n\n# A tibble: 20 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2017     GU           Guam         Tobacco … Cigarett… Current Sm… BRFSS     \n 2 2018     US           National Me… Tobacco … Cigarett… Smoking St… BRFSS     \n 3 2017     US           National Me… Tobacco … Cigarett… Smoking St… BRFSS     \n 4 2016     GU           Guam         Tobacco … Smokeles… Current Use BRFSS     \n 5 2014     GU           Guam         Tobacco … Cigarett… Current Sm… BRFSS     \n 6 2012     IN           Indiana      Tobacco … Cigarett… Current Sm… BRFSS     \n 7 2018-20… DE           Delaware     Tobacco … Smokeles… Current Us… BRFSS     \n 8 2011     MO           Missouri     Tobacco … Smokeles… Current Use BRFSS     \n 9 2013     ME           Maine        Tobacco … Cigarett… Smoking Fr… BRFSS     \n10 2017     WA           Washington   Tobacco … Cigarett… Smoking St… BRFSS     \n11 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n12 2011     MN           Minnesota    Tobacco … Smokeles… User Status BRFSS     \n13 2018     VA           Virginia     Tobacco … Cigarett… Smoking Fr… BRFSS     \n14 2014     MN           Minnesota    Tobacco … Cigarett… Smoking St… BRFSS     \n15 2017-20… WA           Washington   Tobacco … Cigarett… Current Sm… BRFSS     \n16 2017     UT           Utah         Tobacco … Smokeles… Current Use BRFSS     \n17 2012     CA           California   Tobacco … Smokeles… Frequency … BRFSS     \n18 2015     FL           Florida      Tobacco … Smokeles… Current Use BRFSS     \n19 2015     RI           Rhode Island Tobacco … Smokeles… Current Use BRFSS     \n20 2019     AZ           Arizona      Tobacco … Smokeles… Current Use BRFSS     \n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, StratificationID1 &lt;chr&gt;,\n#   StratificationID2 &lt;chr&gt;, StratificationID3 &lt;chr&gt;, …\n\n\nThe data set is not the most analyst friendly format. Generally the ideal format would be variables in the columns, observations in the rows. This data set has variables across several rows, and it is mostly aggregated. The questions are broken up by the different response options, which are spread across rows even though the values are proportions and therefore the samples sizes listed for the different response options for a single question come from the same audience. Aggregations grouped by each of the demographics, locations, and timeframes are also in the rows, similar to a pivot table, so getting things like summary statistics will require filtering these aggregated values. Some survey question variables have missing values, but with the data in this format it’s difficult to make much of them, but we will come back to that. There are a few variables that only serve to give information about the data set itself, like Data_Value_Unit, which only contains one value, “Percentage”, to inform that the Value column is a percentage. Variable like this (Zero or Near Zero Variance) can be removed, since they give no information about the individual observations.\nVariables to be removed: Zero Variance variables: Data_Value_Unit, Data_Value_Type, DataSource, TopicType, TopicTypeId NZV: Data_Value_Footnote_Symbol, Data_Value_Footnote\n\nrawdata %&gt;% filter(is.na(Data_Value)) #looking at nulls to determine why they are there\n\n# A tibble: 2,117 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n 2 2016     IA           Iowa         Tobacco … Smokeles… Frequency … BRFSS     \n 3 2014     ID           Idaho        Tobacco … Smokeles… Frequency … BRFSS     \n 4 2019     MI           Michigan     Tobacco … Smokeles… Frequency … BRFSS     \n 5 2016-20… MA           Massachuset… Tobacco … Smokeles… Current Us… BRFSS     \n 6 2013-20… DC           District of… Tobacco … Cigarett… Current Sm… BRFSS     \n 7 2013     ME           Maine        Tobacco … Cigarett… Current Sm… BRFSS     \n 8 2011     AZ           Arizona      Tobacco … Smokeles… Frequency … BRFSS     \n 9 2018     AL           Alabama      Tobacco … Smokeles… Frequency … BRFSS     \n10 2018     NC           North Carol… Tobacco … Smokeles… Frequency … BRFSS     \n# ℹ 2,107 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\nMissing values from the Data_Value column occur when the sample size is not large enough to report results. Some of these missing values may have the opportunity for imputation, and filtering these could cause some confusion in the actual structure of the data (complete and consistent combinations of categorical values) so we won’t filter anything out just yet. note this is evidenced by the Footnote column with the following message: “Data in these cells have been suppressed because of a small sample size.”\n\nunique(paste(rawdata$LocationDesc,rawdata$LocationAbbr)) # paste concatenates columns, unique to get distinct values.\n\n [1] \"Guam GU\"                            \"National Median (States and DC) US\"\n [3] \"Indiana IN\"                         \"Delaware DE\"                       \n [5] \"Missouri MO\"                        \"Maine ME\"                          \n [7] \"Washington WA\"                      \"Illinois IL\"                       \n [9] \"Minnesota MN\"                       \"Virginia VA\"                       \n[11] \"Utah UT\"                            \"California CA\"                     \n[13] \"Florida FL\"                         \"Rhode Island RI\"                   \n[15] \"Arizona AZ\"                         \"District of Columbia DC\"           \n[17] \"Kansas KS\"                          \"Nevada NV\"                         \n[19] \"Alabama AL\"                         \"West Virginia WV\"                  \n[21] \"Wisconsin WI\"                       \"Oklahoma OK\"                       \n[23] \"New York NY\"                        \"Iowa IA\"                           \n[25] \"Colorado CO\"                        \"Idaho ID\"                          \n[27] \"Alaska AK\"                          \"North Dakota ND\"                   \n[29] \"North Carolina NC\"                  \"New Hampshire NH\"                  \n[31] \"Vermont VT\"                         \"Texas TX\"                          \n[33] \"Nebraska NE\"                        \"Hawaii HI\"                         \n[35] \"Wyoming WY\"                         \"South Dakota SD\"                   \n[37] \"Louisiana LA\"                       \"Kentucky KY\"                       \n[39] \"Puerto Rico PR\"                     \"Arkansas AR\"                       \n[41] \"Maryland MD\"                        \"Ohio OH\"                           \n[43] \"South Carolina SC\"                  \"Massachusetts MA\"                  \n[45] \"Michigan MI\"                        \"Oregon OR\"                         \n[47] \"Montana MT\"                         \"Tennessee TN\"                      \n[49] \"Connecticut CT\"                     \"Mississippi MS\"                    \n[51] \"New Mexico NM\"                      \"Pennsylvania PA\"                   \n[53] \"New Jersey NJ\"                      \"Georgia GA\"                        \n\n\nTo understand the scope of the data set and check for things like inconsistencies, mismatched abbreviations, etc we look at all the unique values for LocationDesc and LocationAbbr. For simplicity sake I will remove the Abbreviated column in the future.\n\nunique(rawdata$YEAR) # seeing unique values of year\n\n [1] \"2017\"      \"2018\"      \"2016\"      \"2014\"      \"2012\"      \"2018-2019\"\n [7] \"2011\"      \"2013\"      \"2014-2015\" \"2017-2018\" \"2015\"      \"2019\"     \n[13] \"2016-2017\" \"2013-2014\" \"2015-2016\" \"2011-2012\" \"2012-2013\"\n\n\n\nrawdata %&gt;% filter(nchar(YEAR) &gt; 4) # confirming what the two-year values are\n\n# A tibble: 4,670 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2018-20… DE           Delaware     Tobacco … Smokeles… Current Us… BRFSS     \n 2 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n 3 2017-20… WA           Washington   Tobacco … Cigarett… Current Sm… BRFSS     \n 4 2018-20… DC           District of… Tobacco … Smokeles… Current Us… BRFSS     \n 5 2016-20… ID           Idaho        Tobacco … Cigarett… Current Sm… BRFSS     \n 6 2017-20… CO           Colorado     Tobacco … Smokeles… Current Us… BRFSS     \n 7 2018-20… TX           Texas        Tobacco … Cigarett… Current Sm… BRFSS     \n 8 2018-20… KS           Kansas       Tobacco … Cigarett… Current Sm… BRFSS     \n 9 2014-20… KS           Kansas       Tobacco … Smokeles… Current Us… BRFSS     \n10 2013-20… VT           Vermont      Tobacco … Cigarett… Current Sm… BRFSS     \n# ℹ 4,660 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\nAgain, understanding the scope of the data. Two things I’m noticing, first being that the year column is a character variable, and not a number. Second being that there are some two-year aggregations that are not necessary since we have every year between 2011 and 2019, these can likely be filtered out.\n\n# filtering down to one Locationa and a smaller year set, again to diagnose the two-year observations\nrawdata %&gt;% filter(LocationAbbr == 'IL', YEAR %in% c('2014-2015', '2014', '2015')) %&gt;% arrange(DisplayOrder)\n\n# A tibble: 152 × 31\n   YEAR  LocationAbbr LocationDesc TopicType    TopicDesc MeasureDesc DataSource\n   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 2 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 3 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 4 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 5 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 6 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 7 2014  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n 8 2015  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n 9 2015  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n10 2014  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n# ℹ 142 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\n\nrawdata %&gt;% filter(LocationAbbr == 'IL', YEAR %in% c('2014-2015', '2014', '2015')) %&gt;% \n  group_by(YEAR, Race, Gender, MeasureDesc) %&gt;% # groups picked to confirm aggregation hierarchy\n  summarize(\n    Samp = sum(Sample_Size, na.rm = TRUE) # sums sample size across the above groups\n  )\n\n`summarise()` has grouped output by 'YEAR', 'Race', 'Gender'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 78 × 5\n# Groups:   YEAR, Race, Gender [21]\n   YEAR  Race             Gender  MeasureDesc                               Samp\n   &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;                                    &lt;dbl&gt;\n 1 2014  African American Overall Current Smoking                            488\n 2 2014  African American Overall Current Use                                489\n 3 2014  All Races        Female  Current Smoking                           3459\n 4 2014  All Races        Female  Current Use                               3454\n 5 2014  All Races        Female  Frequency of Use                             0\n 6 2014  All Races        Female  Percent of Former Smokers Among Ever Sm…  1056\n 7 2014  All Races        Female  Quit Attempt in Past Year Among Every D…   225\n 8 2014  All Races        Female  Smoking Frequency                          686\n 9 2014  All Races        Female  Smoking Status                            8241\n10 2014  All Races        Female  User Status                               5486\n# ℹ 68 more rows\n\n\nThis was primarily to confirm that the two-year rows are in fact aggregations of the data we have in the single-year rows, which we can see that they are. Filtering to a specific location and timeframe also gives us an idea of data structure.\nNote: Values like Current Use and Current Smoking are intentionally different. ‘Use’ Corresponds to smokeless tobacco use, ‘Smoking’ refers to cigarrette/ non-electronic usage.\nThis is enough exploration to give me an idea of what I would want my final data set to look like after cleaning and processing.\nGoal Data Set Mapping:\n|Year|Location|Age|Race|Gender|Education|Value for Measure Desc combined with Values for Response, Surveyed then Freq\nThis would ultimately be a wider data set than we have now, and we will get there in steps.\n\nd1 &lt;- rawdata %&gt;% filter(nchar(YEAR) == 4) %&gt;% \n  mutate(\n  QA = paste(TopicDesc, MeasureDesc,Response), # concatenating all of the question-response related columns\n  Data_Value = as.numeric(Data_Value)/100, # transforming percentages into proper proportions\n  Year = as.numeric(YEAR) # Year variable was previously a string\n) %&gt;% select(Year, LocationDesc, Age, Race, Gender, Education, QA, Data_Value, Sample_Size) # reordering some but mostly getting rid of the original columns that were concatenated\n\nhead(d1)\n\n# A tibble: 6 × 9\n   Year LocationDesc   Age   Race  Gender Education QA    Data_Value Sample_Size\n  &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1  2017 Guam           All … All … Male   All Grad… Ciga…      0.3           692\n2  2018 National Medi… All … All … Overa… All Grad… Ciga…      0.161          NA\n3  2017 National Medi… All … All … Overa… All Grad… Ciga…      0.582          NA\n4  2016 Guam           All … Hisp… Overa… All Grad… Smok…      0.005         123\n5  2014 Guam           All … White Overa… All Grad… Ciga…      0.219         209\n6  2012 Indiana        All … All … Female All Grad… Ciga…      0.216        5165\n\n\nCombining Topic, Measures, and response effectively gives us a unique identifier for each answer choice for each question. Since the data set is already aggregated, we can use these combined question identifiers as variables and line them up with our categorical variables. This gets us one step closer to the rows as observations, or aggregations of observations in this case. Note that we essentially have two numeric values, the sample size and the observed proportion of interest. The sample size is not the same for each question or demographic, so we cant really solve for one without solving for both. To make this easier, I will split these up into two data sets then bring them back together at the end.\n\nd2 &lt;- d1 %&gt;% select(-Sample_Size) %&gt;% # removing sample size so the pivot over Data_Value works correctly\n  pivot_wider(names_from = QA, values_from = Data_Value) \n# transposes unique values of question-response identifier (QA) into columns, \n#essentially grouping by year, Location, and Demographics\n\nhead(d2)\n\n# A tibble: 6 × 25\n   Year LocationDesc         Age   Race  Gender Education Cigarette Use (Adult…¹\n  &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;dbl&gt;\n1  2017 Guam                 All … All … Male   All Grad…                  0.3  \n2  2018 National Median (St… All … All … Overa… All Grad…                 NA    \n3  2017 National Median (St… All … All … Overa… All Grad…                 NA    \n4  2016 Guam                 All … Hisp… Overa… All Grad…                  0.283\n5  2014 Guam                 All … White Overa… All Grad…                  0.219\n6  2012 Indiana              All … All … Female All Grad…                  0.216\n# ℹ abbreviated name: ¹​`Cigarette Use (Adults) Current Smoking NA`\n# ℹ 18 more variables: `Cigarette Use (Adults) Smoking Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Never` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) Current Use NA` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Frequency Some Days` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) User Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Former` &lt;dbl&gt;, …\n\n\n\nd3 &lt;- d1 %&gt;% select(-Data_Value) %&gt;% # removing Date_Value so the pivot over Sample_Size works correctly\n  pivot_wider(names_from = QA, values_from = Sample_Size)\n# transposes unique values of question-response identifier (QA) into columns, \n#essentially grouping by year, Location, and Demographics\n\nhead(d3)\n\n# A tibble: 6 × 25\n   Year LocationDesc         Age   Race  Gender Education Cigarette Use (Adult…¹\n  &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;dbl&gt;\n1  2017 Guam                 All … All … Male   All Grad…                    692\n2  2018 National Median (St… All … All … Overa… All Grad…                     NA\n3  2017 National Median (St… All … All … Overa… All Grad…                     NA\n4  2016 Guam                 All … Hisp… Overa… All Grad…                    123\n5  2014 Guam                 All … White Overa… All Grad…                    209\n6  2012 Indiana              All … All … Female All Grad…                   5165\n# ℹ abbreviated name: ¹​`Cigarette Use (Adults) Current Smoking NA`\n# ℹ 18 more variables: `Cigarette Use (Adults) Smoking Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Never` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) Current Use NA` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Frequency Some Days` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) User Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Former` &lt;dbl&gt;, …\n\n\n\nsum(d2[,1:6] != d3[,1:6]) # counts number of mismatches between categorical columns in the split dataset \n\n[1] 0\n\n\nThe pivot allows us to get each question-response identifier into a variable and line them up with categorical variables that are the same. Our d2 data frame is now our response frequency data set, and d3 is now our sample size data set. The last chunk is just checking that my categorical variables maintained the same structure after pivoting, which they should.\n\nd2 &lt;- d2 %&gt;%  select(c(names(d2[,1:6]),sort(names(d2[,7:25])))) %&gt;% \n  arrange(Year, LocationDesc, Age, Race)\n# Reorder the columns so like questions are next to each other, then reorders rows so like observations are next to each other\nd3 &lt;- d3 %&gt;%  select(c(names(d3[,1:6]),sort(names(d3[,7:25])))) %&gt;% \n  arrange(Year, LocationDesc, Age, Race)\n\nnames(d3)\n\n [1] \"Year\"                                                                             \n [2] \"LocationDesc\"                                                                     \n [3] \"Age\"                                                                              \n [4] \"Race\"                                                                             \n [5] \"Gender\"                                                                           \n [6] \"Education\"                                                                        \n [7] \"Cessation (Adults) Percent of Former Smokers Among Ever Smokers NA\"               \n [8] \"Cessation (Adults) Quit Attempt in Past Year Among Every Day Cigarette Smokers NA\"\n [9] \"Cigarette Use (Adults) Current Smoking NA\"                                        \n[10] \"Cigarette Use (Adults) Smoking Frequency Every Day\"                               \n[11] \"Cigarette Use (Adults) Smoking Frequency Some Days\"                               \n[12] \"Cigarette Use (Adults) Smoking Status Current\"                                    \n[13] \"Cigarette Use (Adults) Smoking Status Former\"                                     \n[14] \"Cigarette Use (Adults) Smoking Status Never\"                                      \n[15] \"E-Cigarette Use (Adults) Current Use NA\"                                          \n[16] \"E-Cigarette Use (Adults) Frequency of Use Every Day\"                              \n[17] \"E-Cigarette Use (Adults) Frequency of Use Some Days\"                              \n[18] \"E-Cigarette Use (Adults) User Status Current\"                                     \n[19] \"E-Cigarette Use (Adults) User Status Former\"                                      \n[20] \"E-Cigarette Use (Adults) User Status Never\"                                       \n[21] \"Smokeless Tobacco Use (Adults) Current Use NA\"                                    \n[22] \"Smokeless Tobacco Use (Adults) Frequency of Use Every Day\"                        \n[23] \"Smokeless Tobacco Use (Adults) Frequency of Use Some Days\"                        \n[24] \"Smokeless Tobacco Use (Adults) User Status Current\"                               \n[25] \"Smokeless Tobacco Use (Adults) User Status Not Current\"                           \n\n\nSince we lead off the question-response identifiers with the topic and Measure, we can get sort the variables alphabetically to get like questions next to each other. We do this for each data frame.\n\n# Manually renaming columns for conciseness\nnames(d3) &lt;- c(names(d3[,1:6]), \n               'QuitPctFrmr',\n               'QuitAttmpt',\n               \n               'CigCurrSmker',\n               'CigFreqDaily',\n               'CigFreqSome',\n               'CigStatCurr',\n               'CigStatFrmr',\n               'CigStatNvr',\n               \n               'EcigCurrUse',\n               'EcigFreqDaily',\n               'EcigFreqSome',\n               'EcigStatCurr',\n               'EcigStatFrmr',\n               'EcigStatNvr',\n               \n               'TobCurrUse',\n               'TobFreqDaily',\n               'TobFreqSome',\n               'TobStatCurr',\n               'TobStatNonCurr'\n               )\n\nredict &lt;- cbind(names(d3), names(d2)) # creates pseduo-dictionary\n\nnames(d2) &lt;- c(names(d3)) # copies new name convention from d3 to d2\n\n# concatenates the type of value identifier onto the columns, makes it easier to interpret\nnames(d2) &lt;- c(names(d2[,1:6]),paste0(names(d2[,7:25]), 'RespFreq')) \nnames(d3) &lt;- c(names(d3[,1:6]),paste0(names(d3[,7:25]), 'SrvCnt'))\n\nThere are shorter ways to make the column names usable, but I want column names that are also more concise. Here I have renamed each question-response identifier individually, to make them much easier to undrstand while I work with them. the redict part also creates a pseudo-data dictionary so I can remember what each renamed column corresponds to, if i forget. I rename the columns for d2 the lazy way, by copying the naming conventions from d3 since the structre is the same. Finally I add ‘RespFreq’ to the column names of my response frequency dataset, and ‘SrvCnt’ to the Survey Count sample size for my Sample Size dataset.\n\nsum(d2[,1:6] != d3[,1:6]) # one last check to make sure nothing got shuffled incorrectly\n\n[1] 0\n\nd4 &lt;- merge(d2,d3) # merging the two datasets into a master\n\nAgain, one last check to make sure everything is in the correct order, then merging the data sets into one master data set.\n\n# filter to narrow scope and see if transformations worked correctly and see what can be removed.\nd4 %&gt;% filter(Year == 2016, LocationDesc == 'Wyoming')\n\n   Year LocationDesc                Age                          Race  Gender\n1  2016      Wyoming     18 to 24 Years                     All Races Overall\n2  2016      Wyoming     18 to 44 Years                     All Races  Female\n3  2016      Wyoming     25 to 44 Years                     All Races Overall\n4  2016      Wyoming     45 to 64 Years                     All Races Overall\n5  2016      Wyoming 65 Years and Older                     All Races Overall\n6  2016      Wyoming   Age 20 and Older                     All Races Overall\n7  2016      Wyoming   Age 20 and Older                     All Races Overall\n8  2016      Wyoming   Age 20 and Older                     All Races Overall\n9  2016      Wyoming   Age 25 and Older                     All Races Overall\n10 2016      Wyoming   Age 25 and Older                     All Races Overall\n11 2016      Wyoming   Age 25 and Older                     All Races Overall\n12 2016      Wyoming           All Ages              African American Overall\n13 2016      Wyoming           All Ages                     All Races  Female\n14 2016      Wyoming           All Ages                     All Races    Male\n15 2016      Wyoming           All Ages                     All Races Overall\n16 2016      Wyoming           All Ages American Indian/Alaska Native Overall\n17 2016      Wyoming           All Ages        Asian/Pacific Islander Overall\n18 2016      Wyoming           All Ages                      Hispanic Overall\n19 2016      Wyoming           All Ages                         White Overall\n      Education QuitPctFrmrRespFreq QuitAttmptRespFreq CigCurrSmkerRespFreq\n1    All Grades                  NA                 NA                0.225\n2    All Grades                  NA                 NA                0.239\n3    All Grades                  NA                 NA                0.240\n4    All Grades                  NA                 NA                0.181\n5    All Grades                  NA                 NA                0.095\n6  &lt; 12th Grade                  NA                 NA                0.402\n7  &gt; 12th Grade                  NA                 NA                0.122\n8    12th Grade                  NA                 NA                0.267\n9  &lt; 12th Grade                  NA                 NA                0.386\n10 &gt; 12th Grade                  NA                 NA                0.122\n11   12th Grade                  NA                 NA                0.270\n12   All Grades                  NA                 NA                   NA\n13   All Grades               0.542              0.518                0.191\n14   All Grades               0.611              0.393                0.188\n15   All Grades               0.580              0.457                0.189\n16   All Grades                  NA                 NA                   NA\n17   All Grades                  NA                 NA                   NA\n18   All Grades                  NA                 NA                0.201\n19   All Grades                  NA                 NA                0.181\n   CigFreqDailyRespFreq CigFreqSomeRespFreq CigStatCurrRespFreq\n1                    NA                  NA                  NA\n2                    NA                  NA                  NA\n3                    NA                  NA                  NA\n4                    NA                  NA                  NA\n5                    NA                  NA                  NA\n6                    NA                  NA                  NA\n7                    NA                  NA                  NA\n8                    NA                  NA                  NA\n9                    NA                  NA                  NA\n10                   NA                  NA                  NA\n11                   NA                  NA                  NA\n12                   NA                  NA                  NA\n13                0.796               0.204               0.191\n14                0.745               0.255               0.188\n15                0.771               0.229               0.189\n16                   NA                  NA                  NA\n17                   NA                  NA                  NA\n18                   NA                  NA                  NA\n19                   NA                  NA                  NA\n   CigStatFrmrRespFreq CigStatNvrRespFreq EcigCurrUseRespFreq\n1                   NA                 NA               0.130\n2                   NA                 NA               0.075\n3                   NA                 NA               0.077\n4                   NA                 NA               0.033\n5                   NA                 NA               0.010\n6                   NA                 NA               0.100\n7                   NA                 NA               0.039\n8                   NA                 NA               0.066\n9                   NA                 NA               0.036\n10                  NA                 NA               0.042\n11                  NA                 NA               0.055\n12                  NA                 NA                  NA\n13               0.227              0.582               0.050\n14               0.295              0.518               0.061\n15               0.261              0.549               0.055\n16                  NA                 NA                  NA\n17                  NA                 NA                  NA\n18                  NA                 NA               0.033\n19                  NA                 NA               0.055\n   EcigFreqDailyRespFreq EcigFreqSomeRespFreq EcigStatCurrRespFreq\n1                     NA                   NA                   NA\n2                     NA                   NA                   NA\n3                     NA                   NA                   NA\n4                     NA                   NA                   NA\n5                     NA                   NA                   NA\n6                     NA                   NA                   NA\n7                     NA                   NA                   NA\n8                     NA                   NA                   NA\n9                     NA                   NA                   NA\n10                    NA                   NA                   NA\n11                    NA                   NA                   NA\n12                    NA                   NA                   NA\n13                 0.290                0.710                0.050\n14                 0.419                0.581                0.061\n15                 0.362                0.638                0.055\n16                    NA                   NA                   NA\n17                    NA                   NA                   NA\n18                    NA                   NA                   NA\n19                    NA                   NA                   NA\n   EcigStatFrmrRespFreq EcigStatNvrRespFreq TobCurrUseRespFreq\n1                    NA                  NA              0.175\n2                    NA                  NA              0.044\n3                    NA                  NA              0.139\n4                    NA                  NA              0.063\n5                    NA                  NA              0.038\n6                    NA                  NA              0.084\n7                    NA                  NA              0.079\n8                    NA                  NA              0.137\n9                    NA                  NA              0.086\n10                   NA                  NA              0.068\n11                   NA                  NA              0.127\n12                   NA                  NA                 NA\n13                0.157               0.793              0.023\n14                0.215               0.725              0.170\n15                0.186               0.758              0.098\n16                   NA                  NA                 NA\n17                   NA                  NA                 NA\n18                   NA                  NA              0.061\n19                   NA                  NA              0.097\n   TobFreqDailyRespFreq TobFreqSomeRespFreq TobStatCurrRespFreq\n1                    NA                  NA                  NA\n2                    NA                  NA                  NA\n3                    NA                  NA                  NA\n4                    NA                  NA                  NA\n5                    NA                  NA                  NA\n6                    NA                  NA                  NA\n7                    NA                  NA                  NA\n8                    NA                  NA                  NA\n9                    NA                  NA                  NA\n10                   NA                  NA                  NA\n11                   NA                  NA                  NA\n12                   NA                  NA                  NA\n13                   NA                  NA               0.023\n14                0.693               0.307               0.170\n15                0.672               0.328               0.098\n16                   NA                  NA                  NA\n17                   NA                  NA                  NA\n18                   NA                  NA                  NA\n19                   NA                  NA                  NA\n   TobStatNonCurrRespFreq QuitPctFrmrSrvCnt QuitAttmptSrvCnt CigCurrSmkerSrvCnt\n1                      NA                NA               NA                140\n2                      NA                NA               NA                469\n3                      NA                NA               NA                758\n4                      NA                NA               NA               1707\n5                      NA                NA               NA               1788\n6                      NA                NA               NA                211\n7                      NA                NA               NA               2896\n8                      NA                NA               NA               1231\n9                      NA                NA               NA                203\n10                     NA                NA               NA               2840\n11                     NA                NA               NA               1200\n12                     NA                NA               NA                 NA\n13                  0.977              1019              258               2492\n14                  0.830               951              217               1901\n15                  0.902              1970              475               4393\n16                     NA                NA               NA                 NA\n17                     NA                NA               NA                 NA\n18                     NA                NA               NA                213\n19                     NA                NA               NA               3961\n   CigFreqDailySrvCnt CigFreqSomeSrvCnt CigStatCurrSrvCnt CigStatFrmrSrvCnt\n1                  NA                NA                NA                NA\n2                  NA                NA                NA                NA\n3                  NA                NA                NA                NA\n4                  NA                NA                NA                NA\n5                  NA                NA                NA                NA\n6                  NA                NA                NA                NA\n7                  NA                NA                NA                NA\n8                  NA                NA                NA                NA\n9                  NA                NA                NA                NA\n10                 NA                NA                NA                NA\n11                 NA                NA                NA                NA\n12                 NA                NA                NA                NA\n13                339               339              2492              2492\n14                285               285              1901              1901\n15                624               624              4393              4393\n16                 NA                NA                NA                NA\n17                 NA                NA                NA                NA\n18                 NA                NA                NA                NA\n19                 NA                NA                NA                NA\n   CigStatNvrSrvCnt EcigCurrUseSrvCnt EcigFreqDailySrvCnt EcigFreqSomeSrvCnt\n1                NA               139                  NA                 NA\n2                NA               470                  NA                 NA\n3                NA               759                  NA                 NA\n4                NA              1713                  NA                 NA\n5                NA              1799                  NA                 NA\n6                NA               212                  NA                 NA\n7                NA              2902                  NA                 NA\n8                NA              1239                  NA                 NA\n9                NA               204                  NA                 NA\n10               NA              2847                  NA                 NA\n11               NA              1208                  NA                 NA\n12               NA                NA                  NA                 NA\n13             2492              2500                  76                 76\n14             1901              1910                  65                 65\n15             4393              4410                 141                141\n16               NA                NA                  NA                 NA\n17               NA                NA                  NA                 NA\n18               NA               213                  NA                 NA\n19               NA              3978                  NA                 NA\n   EcigStatCurrSrvCnt EcigStatFrmrSrvCnt EcigStatNvrSrvCnt TobCurrUseSrvCnt\n1                  NA                 NA                NA              140\n2                  NA                 NA                NA              470\n3                  NA                 NA                NA              756\n4                  NA                 NA                NA             1715\n5                  NA                 NA                NA             1799\n6                  NA                 NA                NA              213\n7                  NA                 NA                NA             2901\n8                  NA                 NA                NA             1239\n9                  NA                 NA                NA              205\n10                 NA                 NA                NA             2845\n11                 NA                 NA                NA             1208\n12                 NA                 NA                NA               NA\n13               2500               2500              2500             2503\n14               1910               1910              1910             1907\n15               4410               4410              4410             4410\n16                 NA                 NA                NA               NA\n17                 NA                 NA                NA               NA\n18                 NA                 NA                NA              214\n19                 NA                 NA                NA             3978\n   TobFreqDailySrvCnt TobFreqSomeSrvCnt TobStatCurrSrvCnt TobStatNonCurrSrvCnt\n1                  NA                NA                NA                   NA\n2                  NA                NA                NA                   NA\n3                  NA                NA                NA                   NA\n4                  NA                NA                NA                   NA\n5                  NA                NA                NA                   NA\n6                  NA                NA                NA                   NA\n7                  NA                NA                NA                   NA\n8                  NA                NA                NA                   NA\n9                  NA                NA                NA                   NA\n10                 NA                NA                NA                   NA\n11                 NA                NA                NA                   NA\n12                 NA                NA                NA                   NA\n13                 NA                NA              2503                 2503\n14                252               252              1907                 1907\n15                278               278              4410                 4410\n16                 NA                NA                NA                   NA\n17                 NA                NA                NA                   NA\n18                 NA                NA                NA                   NA\n19                 NA                NA                NA                   NA\n\n\nFiltering down to one Year and Location, so I can see how some of the aggregated values roll up and how they combine with other demographic variables. This gives me some insight into which questions were missing at what grains, and also allows me to check my merge to make sure it looks right. We finally have the data in a shape and layout that makes sense, and we see that its still kind of a mess. The intention was to maintain the separated demographic columns, and get each survey question into a column or columns. We accomplished that, however we can see that the demographics don’t stack in most cases. For example, I can see aggregated sample sizes for each Gender and the frequency for each question’s response, but I cannot see the age, education, or race makeup for each gender, or any combination of the demographic variables really. This was likely a deliberate choice for sample size constraints, nonetheless it complicates the next steps of this exercise. For a more in depth exercise, I would probably consider imputing the missing sample sizes or rates to get stratification by gender, age and race combined. However for the sake of this exercise, I will break each of the demographics into separate data sets since they have different questions considered. Once done, I will focus in on the Gender data set, since it looks like the gender demographic is the most complete across all questions.\nThe aggregated values by demographic aren’t necessary in the final version of any of these data sets, but we may need them to get the response volumes of some of the demographic values if there few responses. So, we can filter against all of the other demographic variables to only included their roll-up or overall value. First we will check for what those values are for each variable.\n\nunique(d4$Age) #'All Ages', also note some of the age bins are overlapping.\n\n[1] \"18 to 24 Years\"     \"18 to 44 Years\"     \"25 to 44 Years\"    \n[4] \"45 to 64 Years\"     \"65 Years and Older\" \"Age 20 and Older\"  \n[7] \"Age 25 and Older\"   \"All Ages\"          \n\nunique(d4$Gender) #'Overall'\n\n[1] \"Overall\" \"Female\"  \"Male\"   \n\nunique(d4$Race) #'All Races'\n\n[1] \"All Races\"                     \"African American\"             \n[3] \"American Indian/Alaska Native\" \"Asian/Pacific Islander\"       \n[5] \"Hispanic\"                      \"White\"                        \n\nunique(d4$Education) #'All Grades'\n\n[1] \"All Grades\"   \"&lt; 12th Grade\" \"&gt; 12th Grade\" \"12th Grade\"  \n\n\n\n# Age dataset, looking at rollup for all other variables except age, \n# then selecting relevant columns and filtering any last missing values\nAgeSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age != 'All Ages'\n              ,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\n# Gender Dataset. This time keeping the rollup value as well as the others,\n# but still filtering all other variables to their rollup\n# Also filtering out the national Median rows since its not the same grain as the others\n# finally removing last missing values. Will select relevant columns later.\nGndSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age == 'All Ages'\n              #,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  filter(LocationDesc != 'National Median (States and DC)',\n         !is.na(CigCurrSmkerRespFreq)\n         )\n\n# Race dataset, looking at rollup for all other variables except Race, \n# then selecting relevant columns and filtering any last missing values\nRaceSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age == 'All Ages'\n              ,Gender == 'Overall'\n              #,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\n# Education dataset, looking at rollup for all other variables except Education and Age,\n#since age actually is startified with education (interestingly, not vice-versa)\n# then selecting relevant columns and filtering any last missing values\nEdSrv &lt;- d4 %&gt;% filter(1==1\n              #,Age == 'All Ages'\n              ,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education != 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\nNow to focus in on the gender data set. I did not select specific columns earlier because the treatment was a bit different than the other data sets. This data set in particular has a lot more opportunity for imputing and other transformations, but for the simplicity of this exercise we will select only a few interesting and complete columns. Also one last note, the Gender variables has three values, Male, Female, and Overall. Since the Response frequency is the true variable of interest, I’m going to leave the “Overall” values in. Under other circumstances I would remove these and calculate overall values only when they apply, that way there aren’t “repeated” observations in the data set. However, the response frequency variables are pre-processed and rounded, so it would only unnecessarily complicate things to try and calculate response counts to get a proper aggregation.\n\n#Selecting relevant columns for the remainder of the exercise\nGndSrv &lt;- GndSrv %&gt;% \n  select(\n    Year\n    ,LocationDesc\n    ,Gender\n    ,QuitAttmptRespFreq\n    ,CigStatCurrRespFreq\n    ,CigStatFrmrRespFreq\n    ,CigStatNvrRespFreq\n    )\n\nI have decided to focus in on the questions for Cigarette usage, namely status and if they have attempted to quit in the last year. I also kept Year and Location since those variables were the least problematic, and of course Gender since this is the Gender-specific dataset.\n\nGndSrv %&gt;% group_by(Gender) %&gt;% #grouped by general to see potential differences in mean.\n  summarize( # getting mean values for each of my questions\n    AvgQuitAttempt = mean(QuitAttmptRespFreq)\n    ,AvgCurentSmokers = mean(CigStatCurrRespFreq)\n    ,AvgFormerSmokers = mean(CigStatFrmrRespFreq)\n    ,AvgNeverSmoked = mean(CigStatNvrRespFreq)\n  )\n\n# A tibble: 3 × 5\n  Gender  AvgQuitAttempt AvgCurentSmokers AvgFormerSmokers AvgNeverSmoked\n  &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1 Female           0.526            0.163            0.215          0.622\n2 Male             0.510            0.203            0.283          0.514\n3 Overall          0.518            0.183            0.248          0.569\n\n\nLet’s start by getting some quick descriptive statistics. Notice that this table will get the average Rate for each of the questions across all states and timeframes. The number of combinations we have between categorical variables, especially since the Location variable has so many categories, complicates our ability to get a closer look. Faceting and framing will help with this, as we will see in a moment.\n\nbox1o &lt;- GndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_boxplot() +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\nbox1o\n\n\n\n\n\n\n\nhist1o &lt;- GndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\nhist1o\n\n\n\n\n\n\n\n\nthe boxplots should show the distribution of the Percent of Current Smokers across the different locations. I have filter to 2011, the earliest year in the data set, and 2019, the latest year. Here I wanted to see if there would be a large difference from the beginning of the timeframe to the end, and for the most part we can see that there is. It appears as though we have a lower rate of “Current” Smokers from beginning to end. the distribution is not particularly clean, but it also looks to be shifting left while keeping some outliers on the higher end to make for a bit of a right skew.\n\nGndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=QuitAttmptRespFreq)) + # Quit Attempt Rate observed\n  geom_boxplot() +\n  labs(x = 'Percent of Quit Attempts\\nAmong Current Smokers') +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\n\n\n\n\n\n\n\nGndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=QuitAttmptRespFreq)) + # Quit Attempt Rate observed\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Quit Attempts\\nAmong Current Smokers', y = NULL) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\n\n\n\n\n\n\n\n\nGoing into the quit rate, it’s a bit of a different story here. There is maybe a little bit of a left shift in the Males and Overall, but it is far less pronounced than it was for the percent of current smokers. This makes sense, since it is the quit rate for current smokers, meaning we would expect it to either stay the same or potentially increase over time. This begs the question if the decrease in current smokers is coming from more people quitting or fewer people starting. The distribution is also all over the place, so there is a wider variance. This will be apparent in the next visual.\n\naniplot &lt;- GndSrv %&gt;% filter(Gender != 'Overall') %&gt;% # wanting Male and Female only\n  ggplot(aes(x=CigStatCurrRespFreq, y = QuitAttmptRespFreq)) + #plotting current smokers against quit attempt rate\n  geom_point(aes(frame = Year, color = Gender, ids = LocationDesc)) + #frame for animation, ids for hover text, color for comparison\n  labs(x = 'Percent of Current Smokers', y = 'Percent of Quit Attempts\\nAmong Current Smokers')\n\nWarning in geom_point(aes(frame = Year, color = Gender, ids = LocationDesc)):\nIgnoring unknown aesthetics: frame and ids\n\nggplotly(aniplot) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animaton, also slows down the slider when i click play\n\n\n\n\n\nThis was a pretty telling visual, so a few observations. First, there tend to be a higher rate of men who are current smokers over women. Second, the distribution is slowly but surely shifting left over time, it was not simply a fluke between 2011 and 2019 earlier. Finally, the wide variance in the percent of quit attempts is apparent, with the amount of vertical movement each point had in the visual. Still the distribution did not look to have any large shift up or down over time, which tells me the rate of people quitting is roughly the same, so more like the decrease in current smokers is happening because fewer people ever start smoking.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in later chunks\n  ggplot(aes(x = CigStatCurrRespFreq, y = fct_reorder(LocationDesc, CigStatCurrRespFreq), frame = Year)) + #fct_reorder orders location by Current smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of Current Smokers', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nMostly wanting to see if any outliers are more apparent here. Guam looks to have the highest rate of people who smoke, but they also have wide swings, likely due to a smaller sample size. Worth noting that the ranking of rates by state does not change significantly over time either.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in other chunks\n  ggplot(aes(x = CigStatFrmrRespFreq, y = fct_reorder(LocationDesc, CigStatFrmrRespFreq), frame = Year)) + #fct_reorder orders location by former smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of Former Smokers', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nThe rate of Former smokers does not look like it shares the trend. Again, if we saw an increase in the rates of people quitting we might expect the rate of Former Smokers to increase over time, but we don’t really see that much here.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in other chunks\n  ggplot(aes(x = CigStatNvrRespFreq, y = fct_reorder(LocationDesc, CigStatNvrRespFreq), frame = Year)) + #fct_reorder orders location by Never smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of People Who Have Never Smoked', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nHere we can see that increase in rate of people who have never smoked, though admittedly it still seems less pronounced than it was for the decrease in Current smokers. Still, it’s in line with the hypothesis that people are more likely are more likely to have never started smoking than they were in the past, and that is why we are seeing such a decrease in the rate of Current Smokers. Not to end on a darker note, but this is also likely a result of those who were Current Smokers in the earlier years dying off at a higher rate than non-smokers. This may suggest that younger people are much less likely to start smoking as well."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#exploring-the-original-data-set",
    "href": "cdcdata-exercise/cdcdata-exercise.html#exploring-the-original-data-set",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "The CDC data set I have chosen for this exercise is the Tobacco Usage Behavioral Risk Factor data set. The data comes from a survey administered by the CDC as part of the State Tobacco Activities Tracking and Evaluation (STATE) system. The data set contains information about the year the survey was administered, respondent location (State or territory), demographics (namely Race, Age, Education, and Gender), and it asks questions about a person’s tobacco usage status (current, former, never used), their cessation status (whether they have quit or attempted to quit in the last year), and the frequency that they use tobacco for those that do (daily, some days). The usage questions are asked for three different types of Tobacco products, Cigarettes, Smokeless Tobacco, and E-cigarettes.\n\npacman::p_load(here, tidyverse, skimr, plotly, synthpop, patchwork)\n\nInstalling package into 'C:/Users/Hylti/AppData/Local/R/win-library/4.3'\n(as 'lib' is unspecified)\n\n\nalso installing the dependencies 'TH.data', 'libcoin', 'multcomp', 'truncnorm', 'modeltools', 'strucchange', 'coin', 'sandwich', 'cmm', 'Rsolnp', 'party', 'proto', 'polspline', 'randomForest', 'ranger', 'mipfp', 'rmutil', 'broman'\n\n\nWarning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3:\n  cannot open URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3/PACKAGES'\n\n\npackage 'TH.data' successfully unpacked and MD5 sums checked\npackage 'libcoin' successfully unpacked and MD5 sums checked\npackage 'multcomp' successfully unpacked and MD5 sums checked\npackage 'truncnorm' successfully unpacked and MD5 sums checked\npackage 'modeltools' successfully unpacked and MD5 sums checked\npackage 'strucchange' successfully unpacked and MD5 sums checked\npackage 'coin' successfully unpacked and MD5 sums checked\npackage 'sandwich' successfully unpacked and MD5 sums checked\npackage 'cmm' successfully unpacked and MD5 sums checked\npackage 'Rsolnp' successfully unpacked and MD5 sums checked\npackage 'party' successfully unpacked and MD5 sums checked\npackage 'proto' successfully unpacked and MD5 sums checked\npackage 'polspline' successfully unpacked and MD5 sums checked\npackage 'randomForest' successfully unpacked and MD5 sums checked\npackage 'ranger' successfully unpacked and MD5 sums checked\npackage 'mipfp' successfully unpacked and MD5 sums checked\npackage 'rmutil' successfully unpacked and MD5 sums checked\npackage 'broman' successfully unpacked and MD5 sums checked\npackage 'synthpop' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\Hylti\\AppData\\Local\\Temp\\RtmpsZ5jhL\\downloaded_packages\n\n\n\nsynthpop installed\n\n\nWarning: package 'synthpop' was built under R version 4.3.3\n\n\n\nrawdata &lt;- read_csv('cdc-data-raw.csv')\n\nRows: 43341 Columns: 31\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (25): YEAR, LocationAbbr, LocationDesc, TopicType, TopicDesc, MeasureDes...\ndbl  (6): Data_Value, Data_Value_Std_Err, Low_Confidence_Limit, High_Confide...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nstr(rawdata) # getting an idea for data structure\n\nspc_tbl_ [43,341 × 31] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ YEAR                      : chr [1:43341] \"2017\" \"2018\" \"2017\" \"2016\" ...\n $ LocationAbbr              : chr [1:43341] \"GU\" \"US\" \"US\" \"GU\" ...\n $ LocationDesc              : chr [1:43341] \"Guam\" \"National Median (States and DC)\" \"National Median (States and DC)\" \"Guam\" ...\n $ TopicType                 : chr [1:43341] \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" ...\n $ TopicDesc                 : chr [1:43341] \"Cigarette Use (Adults)\" \"Cigarette Use (Adults)\" \"Cigarette Use (Adults)\" \"Smokeless Tobacco Use (Adults)\" ...\n $ MeasureDesc               : chr [1:43341] \"Current Smoking\" \"Smoking Status\" \"Smoking Status\" \"Current Use\" ...\n $ DataSource                : chr [1:43341] \"BRFSS\" \"BRFSS\" \"BRFSS\" \"BRFSS\" ...\n $ Response                  : chr [1:43341] NA \"Current\" \"Never\" NA ...\n $ Data_Value_Unit           : chr [1:43341] \"%\" \"%\" \"%\" \"%\" ...\n $ Data_Value_Type           : chr [1:43341] \"Percentage\" \"Percentage\" \"Percentage\" \"Percentage\" ...\n $ Data_Value                : num [1:43341] 30 16.1 58.2 0.5 21.9 21.6 2.8 3.6 29.2 14.8 ...\n $ Data_Value_Footnote_Symbol: chr [1:43341] NA NA NA NA ...\n $ Data_Value_Footnote       : chr [1:43341] NA NA NA NA ...\n $ Data_Value_Std_Err        : num [1:43341] 2.3 NA NA 0.4 4.9 0.8 0.3 0.4 2.4 0.6 ...\n $ Low_Confidence_Limit      : num [1:43341] 25.4 NA NA 0 12.4 20 2.2 2.7 24.5 13.6 ...\n $ High_Confidence_Limit     : num [1:43341] 34.6 NA NA 1.2 31.4 23.2 3.4 4.5 33.9 16 ...\n $ Sample_Size               : num [1:43341] 692 NA NA 123 209 ...\n $ Gender                    : chr [1:43341] \"Male\" \"Overall\" \"Overall\" \"Overall\" ...\n $ Race                      : chr [1:43341] \"All Races\" \"All Races\" \"All Races\" \"Hispanic\" ...\n $ Age                       : chr [1:43341] \"All Ages\" \"All Ages\" \"All Ages\" \"All Ages\" ...\n $ Education                 : chr [1:43341] \"All Grades\" \"All Grades\" \"All Grades\" \"All Grades\" ...\n $ GeoLocation               : chr [1:43341] \"(13.444304, 144.793731)\" NA NA \"(13.444304, 144.793731)\" ...\n $ TopicTypeId               : chr [1:43341] \"BEH\" \"BEH\" \"BEH\" \"BEH\" ...\n $ TopicId                   : chr [1:43341] \"100BEH\" \"100BEH\" \"100BEH\" \"150BEH\" ...\n $ MeasureId                 : chr [1:43341] \"110CSA\" \"165SSA\" \"165SSA\" \"177SCU\" ...\n $ StratificationID1         : chr [1:43341] \"2GEN\" \"1GEN\" \"1GEN\" \"1GEN\" ...\n $ StratificationID2         : chr [1:43341] \"8AGE\" \"8AGE\" \"8AGE\" \"8AGE\" ...\n $ StratificationID3         : chr [1:43341] \"6RAC\" \"6RAC\" \"6RAC\" \"4RAC\" ...\n $ StratificationID4         : chr [1:43341] \"6EDU\" \"6EDU\" \"6EDU\" \"6EDU\" ...\n $ SubMeasureID              : chr [1:43341] \"BRF21\" \"BRF27\" \"BRF28\" \"BRF69\" ...\n $ DisplayOrder              : num [1:43341] 21 27 28 69 22 21 77 71 26 27 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   YEAR = col_character(),\n  ..   LocationAbbr = col_character(),\n  ..   LocationDesc = col_character(),\n  ..   TopicType = col_character(),\n  ..   TopicDesc = col_character(),\n  ..   MeasureDesc = col_character(),\n  ..   DataSource = col_character(),\n  ..   Response = col_character(),\n  ..   Data_Value_Unit = col_character(),\n  ..   Data_Value_Type = col_character(),\n  ..   Data_Value = col_double(),\n  ..   Data_Value_Footnote_Symbol = col_character(),\n  ..   Data_Value_Footnote = col_character(),\n  ..   Data_Value_Std_Err = col_double(),\n  ..   Low_Confidence_Limit = col_double(),\n  ..   High_Confidence_Limit = col_double(),\n  ..   Sample_Size = col_double(),\n  ..   Gender = col_character(),\n  ..   Race = col_character(),\n  ..   Age = col_character(),\n  ..   Education = col_character(),\n  ..   GeoLocation = col_character(),\n  ..   TopicTypeId = col_character(),\n  ..   TopicId = col_character(),\n  ..   MeasureId = col_character(),\n  ..   StratificationID1 = col_character(),\n  ..   StratificationID2 = col_character(),\n  ..   StratificationID3 = col_character(),\n  ..   StratificationID4 = col_character(),\n  ..   SubMeasureID = col_character(),\n  ..   DisplayOrder = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(rawdata) # summary statistics\n\n     YEAR           LocationAbbr       LocationDesc        TopicType        \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  TopicDesc         MeasureDesc         DataSource          Response        \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n Data_Value_Unit    Data_Value_Type      Data_Value   \n Length:43341       Length:43341       Min.   : 0.00  \n Class :character   Class :character   1st Qu.: 5.10  \n Mode  :character   Mode  :character   Median :17.10  \n                                       Mean   :25.36  \n                                       3rd Qu.:38.90  \n                                       Max.   :99.90  \n                                       NA's   :2117   \n Data_Value_Footnote_Symbol Data_Value_Footnote Data_Value_Std_Err\n Length:43341               Length:43341        Min.   : 0.000    \n Class :character           Class :character    1st Qu.: 0.600    \n Mode  :character           Mode  :character    Median : 1.100    \n                                                Mean   : 1.711    \n                                                3rd Qu.: 2.200    \n                                                Max.   :16.600    \n                                                NA's   :2195      \n Low_Confidence_Limit High_Confidence_Limit  Sample_Size       Gender         \n Min.   : 0.00        Min.   :  0.00        Min.   :   50   Length:43341      \n 1st Qu.: 3.20        1st Qu.:  6.90        1st Qu.:  480   Class :character  \n Median :14.30        Median : 19.80        Median : 1798   Mode  :character  \n Mean   :22.01        Mean   : 28.68        Mean   : 3050                     \n 3rd Qu.:30.50        3rd Qu.: 47.40        3rd Qu.: 4203                     \n Max.   :99.80        Max.   :100.00        Max.   :40726                     \n NA's   :2195         NA's   :2195          NA's   :2195                      \n     Race               Age             Education         GeoLocation       \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n TopicTypeId          TopicId           MeasureId         StratificationID1 \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n StratificationID2  StratificationID3  StratificationID4  SubMeasureID      \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  DisplayOrder  \n Min.   : 5.00  \n 1st Qu.:24.00  \n Median :51.00  \n Mean   :47.08  \n 3rd Qu.:71.00  \n Max.   :81.00  \n                \n\nskim(rawdata) # primarily to get completion rates\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n43341\n\n\nNumber of columns\n31\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n25\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nYEAR\n0\n1.00\n4\n9\n0\n17\n0\n\n\nLocationAbbr\n0\n1.00\n2\n2\n0\n54\n0\n\n\nLocationDesc\n0\n1.00\n4\n31\n0\n54\n0\n\n\nTopicType\n0\n1.00\n25\n25\n0\n1\n0\n\n\nTopicDesc\n0\n1.00\n18\n30\n0\n4\n0\n\n\nMeasureDesc\n0\n1.00\n11\n59\n0\n10\n0\n\n\nDataSource\n0\n1.00\n5\n5\n0\n1\n0\n\n\nResponse\n28323\n0.35\n5\n11\n0\n6\n0\n\n\nData_Value_Unit\n0\n1.00\n1\n1\n0\n1\n0\n\n\nData_Value_Type\n0\n1.00\n10\n10\n0\n1\n0\n\n\nData_Value_Footnote_Symbol\n41224\n0.05\n1\n1\n0\n1\n0\n\n\nData_Value_Footnote\n41224\n0.05\n71\n71\n0\n1\n0\n\n\nGender\n0\n1.00\n4\n7\n0\n3\n0\n\n\nRace\n0\n1.00\n5\n29\n0\n6\n0\n\n\nAge\n0\n1.00\n8\n18\n0\n8\n0\n\n\nEducation\n0\n1.00\n10\n12\n0\n4\n0\n\n\nGeoLocation\n78\n1.00\n23\n41\n0\n53\n0\n\n\nTopicTypeId\n0\n1.00\n3\n3\n0\n1\n0\n\n\nTopicId\n0\n1.00\n6\n6\n0\n4\n0\n\n\nMeasureId\n0\n1.00\n6\n6\n0\n14\n0\n\n\nStratificationID1\n0\n1.00\n4\n4\n0\n3\n0\n\n\nStratificationID2\n0\n1.00\n4\n4\n0\n8\n0\n\n\nStratificationID3\n0\n1.00\n4\n4\n0\n6\n0\n\n\nStratificationID4\n0\n1.00\n4\n4\n0\n4\n0\n\n\nSubMeasureID\n0\n1.00\n5\n5\n0\n53\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nData_Value\n2117\n0.95\n25.36\n25.00\n0\n5.1\n17.1\n38.9\n99.9\n▇▃▂▁▁\n\n\nData_Value_Std_Err\n2195\n0.95\n1.71\n1.72\n0\n0.6\n1.1\n2.2\n16.6\n▇▁▁▁▁\n\n\nLow_Confidence_Limit\n2195\n0.95\n22.01\n24.14\n0\n3.2\n14.3\n30.5\n99.8\n▇▂▂▁▁\n\n\nHigh_Confidence_Limit\n2195\n0.95\n28.68\n26.21\n0\n6.9\n19.8\n47.4\n100.0\n▇▃▂▂▁\n\n\nSample_Size\n2195\n0.95\n3050.17\n3798.87\n50\n480.0\n1798.0\n4203.0\n40726.0\n▇▁▁▁▁\n\n\nDisplayOrder\n0\n1.00\n47.08\n24.49\n5\n24.0\n51.0\n71.0\n81.0\n▂▇▁▂▇\n\n\n\n\n\n\nhead(rawdata, 20) # getting first 20 rows\n\n# A tibble: 20 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2017     GU           Guam         Tobacco … Cigarett… Current Sm… BRFSS     \n 2 2018     US           National Me… Tobacco … Cigarett… Smoking St… BRFSS     \n 3 2017     US           National Me… Tobacco … Cigarett… Smoking St… BRFSS     \n 4 2016     GU           Guam         Tobacco … Smokeles… Current Use BRFSS     \n 5 2014     GU           Guam         Tobacco … Cigarett… Current Sm… BRFSS     \n 6 2012     IN           Indiana      Tobacco … Cigarett… Current Sm… BRFSS     \n 7 2018-20… DE           Delaware     Tobacco … Smokeles… Current Us… BRFSS     \n 8 2011     MO           Missouri     Tobacco … Smokeles… Current Use BRFSS     \n 9 2013     ME           Maine        Tobacco … Cigarett… Smoking Fr… BRFSS     \n10 2017     WA           Washington   Tobacco … Cigarett… Smoking St… BRFSS     \n11 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n12 2011     MN           Minnesota    Tobacco … Smokeles… User Status BRFSS     \n13 2018     VA           Virginia     Tobacco … Cigarett… Smoking Fr… BRFSS     \n14 2014     MN           Minnesota    Tobacco … Cigarett… Smoking St… BRFSS     \n15 2017-20… WA           Washington   Tobacco … Cigarett… Current Sm… BRFSS     \n16 2017     UT           Utah         Tobacco … Smokeles… Current Use BRFSS     \n17 2012     CA           California   Tobacco … Smokeles… Frequency … BRFSS     \n18 2015     FL           Florida      Tobacco … Smokeles… Current Use BRFSS     \n19 2015     RI           Rhode Island Tobacco … Smokeles… Current Use BRFSS     \n20 2019     AZ           Arizona      Tobacco … Smokeles… Current Use BRFSS     \n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, StratificationID1 &lt;chr&gt;,\n#   StratificationID2 &lt;chr&gt;, StratificationID3 &lt;chr&gt;, …\n\n\nThe data set is not the most analyst friendly format. Generally the ideal format would be variables in the columns, observations in the rows. This data set has variables across several rows, and it is mostly aggregated. The questions are broken up by the different response options, which are spread across rows even though the values are proportions and therefore the samples sizes listed for the different response options for a single question come from the same audience. Aggregations grouped by each of the demographics, locations, and timeframes are also in the rows, similar to a pivot table, so getting things like summary statistics will require filtering these aggregated values. Some survey question variables have missing values, but with the data in this format it’s difficult to make much of them, but we will come back to that. There are a few variables that only serve to give information about the data set itself, like Data_Value_Unit, which only contains one value, “Percentage”, to inform that the Value column is a percentage. Variable like this (Zero or Near Zero Variance) can be removed, since they give no information about the individual observations.\nVariables to be removed: Zero Variance variables: Data_Value_Unit, Data_Value_Type, DataSource, TopicType, TopicTypeId NZV: Data_Value_Footnote_Symbol, Data_Value_Footnote\n\nrawdata %&gt;% filter(is.na(Data_Value)) #looking at nulls to determine why they are there\n\n# A tibble: 2,117 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n 2 2016     IA           Iowa         Tobacco … Smokeles… Frequency … BRFSS     \n 3 2014     ID           Idaho        Tobacco … Smokeles… Frequency … BRFSS     \n 4 2019     MI           Michigan     Tobacco … Smokeles… Frequency … BRFSS     \n 5 2016-20… MA           Massachuset… Tobacco … Smokeles… Current Us… BRFSS     \n 6 2013-20… DC           District of… Tobacco … Cigarett… Current Sm… BRFSS     \n 7 2013     ME           Maine        Tobacco … Cigarett… Current Sm… BRFSS     \n 8 2011     AZ           Arizona      Tobacco … Smokeles… Frequency … BRFSS     \n 9 2018     AL           Alabama      Tobacco … Smokeles… Frequency … BRFSS     \n10 2018     NC           North Carol… Tobacco … Smokeles… Frequency … BRFSS     \n# ℹ 2,107 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\nMissing values from the Data_Value column occur when the sample size is not large enough to report results. Some of these missing values may have the opportunity for imputation, and filtering these could cause some confusion in the actual structure of the data (complete and consistent combinations of categorical values) so we won’t filter anything out just yet. note this is evidenced by the Footnote column with the following message: “Data in these cells have been suppressed because of a small sample size.”\n\nunique(paste(rawdata$LocationDesc,rawdata$LocationAbbr)) # paste concatenates columns, unique to get distinct values.\n\n [1] \"Guam GU\"                            \"National Median (States and DC) US\"\n [3] \"Indiana IN\"                         \"Delaware DE\"                       \n [5] \"Missouri MO\"                        \"Maine ME\"                          \n [7] \"Washington WA\"                      \"Illinois IL\"                       \n [9] \"Minnesota MN\"                       \"Virginia VA\"                       \n[11] \"Utah UT\"                            \"California CA\"                     \n[13] \"Florida FL\"                         \"Rhode Island RI\"                   \n[15] \"Arizona AZ\"                         \"District of Columbia DC\"           \n[17] \"Kansas KS\"                          \"Nevada NV\"                         \n[19] \"Alabama AL\"                         \"West Virginia WV\"                  \n[21] \"Wisconsin WI\"                       \"Oklahoma OK\"                       \n[23] \"New York NY\"                        \"Iowa IA\"                           \n[25] \"Colorado CO\"                        \"Idaho ID\"                          \n[27] \"Alaska AK\"                          \"North Dakota ND\"                   \n[29] \"North Carolina NC\"                  \"New Hampshire NH\"                  \n[31] \"Vermont VT\"                         \"Texas TX\"                          \n[33] \"Nebraska NE\"                        \"Hawaii HI\"                         \n[35] \"Wyoming WY\"                         \"South Dakota SD\"                   \n[37] \"Louisiana LA\"                       \"Kentucky KY\"                       \n[39] \"Puerto Rico PR\"                     \"Arkansas AR\"                       \n[41] \"Maryland MD\"                        \"Ohio OH\"                           \n[43] \"South Carolina SC\"                  \"Massachusetts MA\"                  \n[45] \"Michigan MI\"                        \"Oregon OR\"                         \n[47] \"Montana MT\"                         \"Tennessee TN\"                      \n[49] \"Connecticut CT\"                     \"Mississippi MS\"                    \n[51] \"New Mexico NM\"                      \"Pennsylvania PA\"                   \n[53] \"New Jersey NJ\"                      \"Georgia GA\"                        \n\n\nTo understand the scope of the data set and check for things like inconsistencies, mismatched abbreviations, etc we look at all the unique values for LocationDesc and LocationAbbr. For simplicity sake I will remove the Abbreviated column in the future.\n\nunique(rawdata$YEAR) # seeing unique values of year\n\n [1] \"2017\"      \"2018\"      \"2016\"      \"2014\"      \"2012\"      \"2018-2019\"\n [7] \"2011\"      \"2013\"      \"2014-2015\" \"2017-2018\" \"2015\"      \"2019\"     \n[13] \"2016-2017\" \"2013-2014\" \"2015-2016\" \"2011-2012\" \"2012-2013\"\n\n\n\nrawdata %&gt;% filter(nchar(YEAR) &gt; 4) # confirming what the two-year values are\n\n# A tibble: 4,670 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2018-20… DE           Delaware     Tobacco … Smokeles… Current Us… BRFSS     \n 2 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n 3 2017-20… WA           Washington   Tobacco … Cigarett… Current Sm… BRFSS     \n 4 2018-20… DC           District of… Tobacco … Smokeles… Current Us… BRFSS     \n 5 2016-20… ID           Idaho        Tobacco … Cigarett… Current Sm… BRFSS     \n 6 2017-20… CO           Colorado     Tobacco … Smokeles… Current Us… BRFSS     \n 7 2018-20… TX           Texas        Tobacco … Cigarett… Current Sm… BRFSS     \n 8 2018-20… KS           Kansas       Tobacco … Cigarett… Current Sm… BRFSS     \n 9 2014-20… KS           Kansas       Tobacco … Smokeles… Current Us… BRFSS     \n10 2013-20… VT           Vermont      Tobacco … Cigarett… Current Sm… BRFSS     \n# ℹ 4,660 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\nAgain, understanding the scope of the data. Two things I’m noticing, first being that the year column is a character variable, and not a number. Second being that there are some two-year aggregations that are not necessary since we have every year between 2011 and 2019, these can likely be filtered out.\n\n# filtering down to one Locationa and a smaller year set, again to diagnose the two-year observations\nrawdata %&gt;% filter(LocationAbbr == 'IL', YEAR %in% c('2014-2015', '2014', '2015')) %&gt;% arrange(DisplayOrder)\n\n# A tibble: 152 × 31\n   YEAR  LocationAbbr LocationDesc TopicType    TopicDesc MeasureDesc DataSource\n   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 2 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 3 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 4 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 5 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 6 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 7 2014  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n 8 2015  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n 9 2015  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n10 2014  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n# ℹ 142 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\n\nrawdata %&gt;% filter(LocationAbbr == 'IL', YEAR %in% c('2014-2015', '2014', '2015')) %&gt;% \n  group_by(YEAR, Race, Gender, MeasureDesc) %&gt;% # groups picked to confirm aggregation hierarchy\n  summarize(\n    Samp = sum(Sample_Size, na.rm = TRUE) # sums sample size across the above groups\n  )\n\n`summarise()` has grouped output by 'YEAR', 'Race', 'Gender'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 78 × 5\n# Groups:   YEAR, Race, Gender [21]\n   YEAR  Race             Gender  MeasureDesc                               Samp\n   &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;                                    &lt;dbl&gt;\n 1 2014  African American Overall Current Smoking                            488\n 2 2014  African American Overall Current Use                                489\n 3 2014  All Races        Female  Current Smoking                           3459\n 4 2014  All Races        Female  Current Use                               3454\n 5 2014  All Races        Female  Frequency of Use                             0\n 6 2014  All Races        Female  Percent of Former Smokers Among Ever Sm…  1056\n 7 2014  All Races        Female  Quit Attempt in Past Year Among Every D…   225\n 8 2014  All Races        Female  Smoking Frequency                          686\n 9 2014  All Races        Female  Smoking Status                            8241\n10 2014  All Races        Female  User Status                               5486\n# ℹ 68 more rows\n\n\nThis was primarily to confirm that the two-year rows are in fact aggregations of the data we have in the single-year rows, which we can see that they are. Filtering to a specific location and timeframe also gives us an idea of data structure.\nNote: Values like Current Use and Current Smoking are intentionally different. ‘Use’ Corresponds to smokeless tobacco use, ‘Smoking’ refers to cigarrette/ non-electronic usage.\nThis is enough exploration to give me an idea of what I would want my final data set to look like after cleaning and processing.\nGoal Data Set Mapping:\n|Year|Location|Age|Race|Gender|Education|Value for Measure Desc combined with Values for Response, Surveyed then Freq\nThis would ultimately be a wider data set than we have now, and we will get there in steps.\n\nd1 &lt;- rawdata %&gt;% filter(nchar(YEAR) == 4) %&gt;% \n  mutate(\n  QA = paste(TopicDesc, MeasureDesc,Response), # concatenating all of the question-response related columns\n  Data_Value = as.numeric(Data_Value)/100, # transforming percentages into proper proportions\n  Year = as.numeric(YEAR) # Year variable was previously a string\n) %&gt;% select(Year, LocationDesc, Age, Race, Gender, Education, QA, Data_Value, Sample_Size) # reordering some but mostly getting rid of the original columns that were concatenated\n\nhead(d1)\n\n# A tibble: 6 × 9\n   Year LocationDesc   Age   Race  Gender Education QA    Data_Value Sample_Size\n  &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1  2017 Guam           All … All … Male   All Grad… Ciga…      0.3           692\n2  2018 National Medi… All … All … Overa… All Grad… Ciga…      0.161          NA\n3  2017 National Medi… All … All … Overa… All Grad… Ciga…      0.582          NA\n4  2016 Guam           All … Hisp… Overa… All Grad… Smok…      0.005         123\n5  2014 Guam           All … White Overa… All Grad… Ciga…      0.219         209\n6  2012 Indiana        All … All … Female All Grad… Ciga…      0.216        5165\n\n\nCombining Topic, Measures, and response effectively gives us a unique identifier for each answer choice for each question. Since the data set is already aggregated, we can use these combined question identifiers as variables and line them up with our categorical variables. This gets us one step closer to the rows as observations, or aggregations of observations in this case. Note that we essentially have two numeric values, the sample size and the observed proportion of interest. The sample size is not the same for each question or demographic, so we cant really solve for one without solving for both. To make this easier, I will split these up into two data sets then bring them back together at the end.\n\nd2 &lt;- d1 %&gt;% select(-Sample_Size) %&gt;% # removing sample size so the pivot over Data_Value works correctly\n  pivot_wider(names_from = QA, values_from = Data_Value) \n# transposes unique values of question-response identifier (QA) into columns, \n#essentially grouping by year, Location, and Demographics\n\nhead(d2)\n\n# A tibble: 6 × 25\n   Year LocationDesc         Age   Race  Gender Education Cigarette Use (Adult…¹\n  &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;dbl&gt;\n1  2017 Guam                 All … All … Male   All Grad…                  0.3  \n2  2018 National Median (St… All … All … Overa… All Grad…                 NA    \n3  2017 National Median (St… All … All … Overa… All Grad…                 NA    \n4  2016 Guam                 All … Hisp… Overa… All Grad…                  0.283\n5  2014 Guam                 All … White Overa… All Grad…                  0.219\n6  2012 Indiana              All … All … Female All Grad…                  0.216\n# ℹ abbreviated name: ¹​`Cigarette Use (Adults) Current Smoking NA`\n# ℹ 18 more variables: `Cigarette Use (Adults) Smoking Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Never` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) Current Use NA` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Frequency Some Days` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) User Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Former` &lt;dbl&gt;, …\n\n\n\nd3 &lt;- d1 %&gt;% select(-Data_Value) %&gt;% # removing Date_Value so the pivot over Sample_Size works correctly\n  pivot_wider(names_from = QA, values_from = Sample_Size)\n# transposes unique values of question-response identifier (QA) into columns, \n#essentially grouping by year, Location, and Demographics\n\nhead(d3)\n\n# A tibble: 6 × 25\n   Year LocationDesc         Age   Race  Gender Education Cigarette Use (Adult…¹\n  &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;dbl&gt;\n1  2017 Guam                 All … All … Male   All Grad…                    692\n2  2018 National Median (St… All … All … Overa… All Grad…                     NA\n3  2017 National Median (St… All … All … Overa… All Grad…                     NA\n4  2016 Guam                 All … Hisp… Overa… All Grad…                    123\n5  2014 Guam                 All … White Overa… All Grad…                    209\n6  2012 Indiana              All … All … Female All Grad…                   5165\n# ℹ abbreviated name: ¹​`Cigarette Use (Adults) Current Smoking NA`\n# ℹ 18 more variables: `Cigarette Use (Adults) Smoking Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Never` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) Current Use NA` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Frequency Some Days` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) User Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Former` &lt;dbl&gt;, …\n\n\n\nsum(d2[,1:6] != d3[,1:6]) # counts number of mismatches between categorical columns in the split dataset \n\n[1] 0\n\n\nThe pivot allows us to get each question-response identifier into a variable and line them up with categorical variables that are the same. Our d2 data frame is now our response frequency data set, and d3 is now our sample size data set. The last chunk is just checking that my categorical variables maintained the same structure after pivoting, which they should.\n\nd2 &lt;- d2 %&gt;%  select(c(names(d2[,1:6]),sort(names(d2[,7:25])))) %&gt;% \n  arrange(Year, LocationDesc, Age, Race)\n# Reorder the columns so like questions are next to each other, then reorders rows so like observations are next to each other\nd3 &lt;- d3 %&gt;%  select(c(names(d3[,1:6]),sort(names(d3[,7:25])))) %&gt;% \n  arrange(Year, LocationDesc, Age, Race)\n\nnames(d3)\n\n [1] \"Year\"                                                                             \n [2] \"LocationDesc\"                                                                     \n [3] \"Age\"                                                                              \n [4] \"Race\"                                                                             \n [5] \"Gender\"                                                                           \n [6] \"Education\"                                                                        \n [7] \"Cessation (Adults) Percent of Former Smokers Among Ever Smokers NA\"               \n [8] \"Cessation (Adults) Quit Attempt in Past Year Among Every Day Cigarette Smokers NA\"\n [9] \"Cigarette Use (Adults) Current Smoking NA\"                                        \n[10] \"Cigarette Use (Adults) Smoking Frequency Every Day\"                               \n[11] \"Cigarette Use (Adults) Smoking Frequency Some Days\"                               \n[12] \"Cigarette Use (Adults) Smoking Status Current\"                                    \n[13] \"Cigarette Use (Adults) Smoking Status Former\"                                     \n[14] \"Cigarette Use (Adults) Smoking Status Never\"                                      \n[15] \"E-Cigarette Use (Adults) Current Use NA\"                                          \n[16] \"E-Cigarette Use (Adults) Frequency of Use Every Day\"                              \n[17] \"E-Cigarette Use (Adults) Frequency of Use Some Days\"                              \n[18] \"E-Cigarette Use (Adults) User Status Current\"                                     \n[19] \"E-Cigarette Use (Adults) User Status Former\"                                      \n[20] \"E-Cigarette Use (Adults) User Status Never\"                                       \n[21] \"Smokeless Tobacco Use (Adults) Current Use NA\"                                    \n[22] \"Smokeless Tobacco Use (Adults) Frequency of Use Every Day\"                        \n[23] \"Smokeless Tobacco Use (Adults) Frequency of Use Some Days\"                        \n[24] \"Smokeless Tobacco Use (Adults) User Status Current\"                               \n[25] \"Smokeless Tobacco Use (Adults) User Status Not Current\"                           \n\n\nSince we lead off the question-response identifiers with the topic and Measure, we can get sort the variables alphabetically to get like questions next to each other. We do this for each data frame.\n\n# Manually renaming columns for conciseness\nnames(d3) &lt;- c(names(d3[,1:6]), \n               'QuitPctFrmr',\n               'QuitAttmpt',\n               \n               'CigCurrSmker',\n               'CigFreqDaily',\n               'CigFreqSome',\n               'CigStatCurr',\n               'CigStatFrmr',\n               'CigStatNvr',\n               \n               'EcigCurrUse',\n               'EcigFreqDaily',\n               'EcigFreqSome',\n               'EcigStatCurr',\n               'EcigStatFrmr',\n               'EcigStatNvr',\n               \n               'TobCurrUse',\n               'TobFreqDaily',\n               'TobFreqSome',\n               'TobStatCurr',\n               'TobStatNonCurr'\n               )\n\nredict &lt;- cbind(names(d3), names(d2)) # creates pseduo-dictionary\n\nnames(d2) &lt;- c(names(d3)) # copies new name convention from d3 to d2\n\n# concatenates the type of value identifier onto the columns, makes it easier to interpret\nnames(d2) &lt;- c(names(d2[,1:6]),paste0(names(d2[,7:25]), 'RespFreq')) \nnames(d3) &lt;- c(names(d3[,1:6]),paste0(names(d3[,7:25]), 'SrvCnt'))\n\nThere are shorter ways to make the column names usable, but I want column names that are also more concise. Here I have renamed each question-response identifier individually, to make them much easier to undrstand while I work with them. the redict part also creates a pseudo-data dictionary so I can remember what each renamed column corresponds to, if i forget. I rename the columns for d2 the lazy way, by copying the naming conventions from d3 since the structre is the same. Finally I add ‘RespFreq’ to the column names of my response frequency dataset, and ‘SrvCnt’ to the Survey Count sample size for my Sample Size dataset.\n\nsum(d2[,1:6] != d3[,1:6]) # one last check to make sure nothing got shuffled incorrectly\n\n[1] 0\n\nd4 &lt;- merge(d2,d3) # merging the two datasets into a master\n\nAgain, one last check to make sure everything is in the correct order, then merging the data sets into one master data set.\n\n# filter to narrow scope and see if transformations worked correctly and see what can be removed.\nd4 %&gt;% filter(Year == 2016, LocationDesc == 'Wyoming')\n\n   Year LocationDesc                Age                          Race  Gender\n1  2016      Wyoming     18 to 24 Years                     All Races Overall\n2  2016      Wyoming     18 to 44 Years                     All Races  Female\n3  2016      Wyoming     25 to 44 Years                     All Races Overall\n4  2016      Wyoming     45 to 64 Years                     All Races Overall\n5  2016      Wyoming 65 Years and Older                     All Races Overall\n6  2016      Wyoming   Age 20 and Older                     All Races Overall\n7  2016      Wyoming   Age 20 and Older                     All Races Overall\n8  2016      Wyoming   Age 20 and Older                     All Races Overall\n9  2016      Wyoming   Age 25 and Older                     All Races Overall\n10 2016      Wyoming   Age 25 and Older                     All Races Overall\n11 2016      Wyoming   Age 25 and Older                     All Races Overall\n12 2016      Wyoming           All Ages              African American Overall\n13 2016      Wyoming           All Ages                     All Races  Female\n14 2016      Wyoming           All Ages                     All Races    Male\n15 2016      Wyoming           All Ages                     All Races Overall\n16 2016      Wyoming           All Ages American Indian/Alaska Native Overall\n17 2016      Wyoming           All Ages        Asian/Pacific Islander Overall\n18 2016      Wyoming           All Ages                      Hispanic Overall\n19 2016      Wyoming           All Ages                         White Overall\n      Education QuitPctFrmrRespFreq QuitAttmptRespFreq CigCurrSmkerRespFreq\n1    All Grades                  NA                 NA                0.225\n2    All Grades                  NA                 NA                0.239\n3    All Grades                  NA                 NA                0.240\n4    All Grades                  NA                 NA                0.181\n5    All Grades                  NA                 NA                0.095\n6  &lt; 12th Grade                  NA                 NA                0.402\n7  &gt; 12th Grade                  NA                 NA                0.122\n8    12th Grade                  NA                 NA                0.267\n9  &lt; 12th Grade                  NA                 NA                0.386\n10 &gt; 12th Grade                  NA                 NA                0.122\n11   12th Grade                  NA                 NA                0.270\n12   All Grades                  NA                 NA                   NA\n13   All Grades               0.542              0.518                0.191\n14   All Grades               0.611              0.393                0.188\n15   All Grades               0.580              0.457                0.189\n16   All Grades                  NA                 NA                   NA\n17   All Grades                  NA                 NA                   NA\n18   All Grades                  NA                 NA                0.201\n19   All Grades                  NA                 NA                0.181\n   CigFreqDailyRespFreq CigFreqSomeRespFreq CigStatCurrRespFreq\n1                    NA                  NA                  NA\n2                    NA                  NA                  NA\n3                    NA                  NA                  NA\n4                    NA                  NA                  NA\n5                    NA                  NA                  NA\n6                    NA                  NA                  NA\n7                    NA                  NA                  NA\n8                    NA                  NA                  NA\n9                    NA                  NA                  NA\n10                   NA                  NA                  NA\n11                   NA                  NA                  NA\n12                   NA                  NA                  NA\n13                0.796               0.204               0.191\n14                0.745               0.255               0.188\n15                0.771               0.229               0.189\n16                   NA                  NA                  NA\n17                   NA                  NA                  NA\n18                   NA                  NA                  NA\n19                   NA                  NA                  NA\n   CigStatFrmrRespFreq CigStatNvrRespFreq EcigCurrUseRespFreq\n1                   NA                 NA               0.130\n2                   NA                 NA               0.075\n3                   NA                 NA               0.077\n4                   NA                 NA               0.033\n5                   NA                 NA               0.010\n6                   NA                 NA               0.100\n7                   NA                 NA               0.039\n8                   NA                 NA               0.066\n9                   NA                 NA               0.036\n10                  NA                 NA               0.042\n11                  NA                 NA               0.055\n12                  NA                 NA                  NA\n13               0.227              0.582               0.050\n14               0.295              0.518               0.061\n15               0.261              0.549               0.055\n16                  NA                 NA                  NA\n17                  NA                 NA                  NA\n18                  NA                 NA               0.033\n19                  NA                 NA               0.055\n   EcigFreqDailyRespFreq EcigFreqSomeRespFreq EcigStatCurrRespFreq\n1                     NA                   NA                   NA\n2                     NA                   NA                   NA\n3                     NA                   NA                   NA\n4                     NA                   NA                   NA\n5                     NA                   NA                   NA\n6                     NA                   NA                   NA\n7                     NA                   NA                   NA\n8                     NA                   NA                   NA\n9                     NA                   NA                   NA\n10                    NA                   NA                   NA\n11                    NA                   NA                   NA\n12                    NA                   NA                   NA\n13                 0.290                0.710                0.050\n14                 0.419                0.581                0.061\n15                 0.362                0.638                0.055\n16                    NA                   NA                   NA\n17                    NA                   NA                   NA\n18                    NA                   NA                   NA\n19                    NA                   NA                   NA\n   EcigStatFrmrRespFreq EcigStatNvrRespFreq TobCurrUseRespFreq\n1                    NA                  NA              0.175\n2                    NA                  NA              0.044\n3                    NA                  NA              0.139\n4                    NA                  NA              0.063\n5                    NA                  NA              0.038\n6                    NA                  NA              0.084\n7                    NA                  NA              0.079\n8                    NA                  NA              0.137\n9                    NA                  NA              0.086\n10                   NA                  NA              0.068\n11                   NA                  NA              0.127\n12                   NA                  NA                 NA\n13                0.157               0.793              0.023\n14                0.215               0.725              0.170\n15                0.186               0.758              0.098\n16                   NA                  NA                 NA\n17                   NA                  NA                 NA\n18                   NA                  NA              0.061\n19                   NA                  NA              0.097\n   TobFreqDailyRespFreq TobFreqSomeRespFreq TobStatCurrRespFreq\n1                    NA                  NA                  NA\n2                    NA                  NA                  NA\n3                    NA                  NA                  NA\n4                    NA                  NA                  NA\n5                    NA                  NA                  NA\n6                    NA                  NA                  NA\n7                    NA                  NA                  NA\n8                    NA                  NA                  NA\n9                    NA                  NA                  NA\n10                   NA                  NA                  NA\n11                   NA                  NA                  NA\n12                   NA                  NA                  NA\n13                   NA                  NA               0.023\n14                0.693               0.307               0.170\n15                0.672               0.328               0.098\n16                   NA                  NA                  NA\n17                   NA                  NA                  NA\n18                   NA                  NA                  NA\n19                   NA                  NA                  NA\n   TobStatNonCurrRespFreq QuitPctFrmrSrvCnt QuitAttmptSrvCnt CigCurrSmkerSrvCnt\n1                      NA                NA               NA                140\n2                      NA                NA               NA                469\n3                      NA                NA               NA                758\n4                      NA                NA               NA               1707\n5                      NA                NA               NA               1788\n6                      NA                NA               NA                211\n7                      NA                NA               NA               2896\n8                      NA                NA               NA               1231\n9                      NA                NA               NA                203\n10                     NA                NA               NA               2840\n11                     NA                NA               NA               1200\n12                     NA                NA               NA                 NA\n13                  0.977              1019              258               2492\n14                  0.830               951              217               1901\n15                  0.902              1970              475               4393\n16                     NA                NA               NA                 NA\n17                     NA                NA               NA                 NA\n18                     NA                NA               NA                213\n19                     NA                NA               NA               3961\n   CigFreqDailySrvCnt CigFreqSomeSrvCnt CigStatCurrSrvCnt CigStatFrmrSrvCnt\n1                  NA                NA                NA                NA\n2                  NA                NA                NA                NA\n3                  NA                NA                NA                NA\n4                  NA                NA                NA                NA\n5                  NA                NA                NA                NA\n6                  NA                NA                NA                NA\n7                  NA                NA                NA                NA\n8                  NA                NA                NA                NA\n9                  NA                NA                NA                NA\n10                 NA                NA                NA                NA\n11                 NA                NA                NA                NA\n12                 NA                NA                NA                NA\n13                339               339              2492              2492\n14                285               285              1901              1901\n15                624               624              4393              4393\n16                 NA                NA                NA                NA\n17                 NA                NA                NA                NA\n18                 NA                NA                NA                NA\n19                 NA                NA                NA                NA\n   CigStatNvrSrvCnt EcigCurrUseSrvCnt EcigFreqDailySrvCnt EcigFreqSomeSrvCnt\n1                NA               139                  NA                 NA\n2                NA               470                  NA                 NA\n3                NA               759                  NA                 NA\n4                NA              1713                  NA                 NA\n5                NA              1799                  NA                 NA\n6                NA               212                  NA                 NA\n7                NA              2902                  NA                 NA\n8                NA              1239                  NA                 NA\n9                NA               204                  NA                 NA\n10               NA              2847                  NA                 NA\n11               NA              1208                  NA                 NA\n12               NA                NA                  NA                 NA\n13             2492              2500                  76                 76\n14             1901              1910                  65                 65\n15             4393              4410                 141                141\n16               NA                NA                  NA                 NA\n17               NA                NA                  NA                 NA\n18               NA               213                  NA                 NA\n19               NA              3978                  NA                 NA\n   EcigStatCurrSrvCnt EcigStatFrmrSrvCnt EcigStatNvrSrvCnt TobCurrUseSrvCnt\n1                  NA                 NA                NA              140\n2                  NA                 NA                NA              470\n3                  NA                 NA                NA              756\n4                  NA                 NA                NA             1715\n5                  NA                 NA                NA             1799\n6                  NA                 NA                NA              213\n7                  NA                 NA                NA             2901\n8                  NA                 NA                NA             1239\n9                  NA                 NA                NA              205\n10                 NA                 NA                NA             2845\n11                 NA                 NA                NA             1208\n12                 NA                 NA                NA               NA\n13               2500               2500              2500             2503\n14               1910               1910              1910             1907\n15               4410               4410              4410             4410\n16                 NA                 NA                NA               NA\n17                 NA                 NA                NA               NA\n18                 NA                 NA                NA              214\n19                 NA                 NA                NA             3978\n   TobFreqDailySrvCnt TobFreqSomeSrvCnt TobStatCurrSrvCnt TobStatNonCurrSrvCnt\n1                  NA                NA                NA                   NA\n2                  NA                NA                NA                   NA\n3                  NA                NA                NA                   NA\n4                  NA                NA                NA                   NA\n5                  NA                NA                NA                   NA\n6                  NA                NA                NA                   NA\n7                  NA                NA                NA                   NA\n8                  NA                NA                NA                   NA\n9                  NA                NA                NA                   NA\n10                 NA                NA                NA                   NA\n11                 NA                NA                NA                   NA\n12                 NA                NA                NA                   NA\n13                 NA                NA              2503                 2503\n14                252               252              1907                 1907\n15                278               278              4410                 4410\n16                 NA                NA                NA                   NA\n17                 NA                NA                NA                   NA\n18                 NA                NA                NA                   NA\n19                 NA                NA                NA                   NA\n\n\nFiltering down to one Year and Location, so I can see how some of the aggregated values roll up and how they combine with other demographic variables. This gives me some insight into which questions were missing at what grains, and also allows me to check my merge to make sure it looks right. We finally have the data in a shape and layout that makes sense, and we see that its still kind of a mess. The intention was to maintain the separated demographic columns, and get each survey question into a column or columns. We accomplished that, however we can see that the demographics don’t stack in most cases. For example, I can see aggregated sample sizes for each Gender and the frequency for each question’s response, but I cannot see the age, education, or race makeup for each gender, or any combination of the demographic variables really. This was likely a deliberate choice for sample size constraints, nonetheless it complicates the next steps of this exercise. For a more in depth exercise, I would probably consider imputing the missing sample sizes or rates to get stratification by gender, age and race combined. However for the sake of this exercise, I will break each of the demographics into separate data sets since they have different questions considered. Once done, I will focus in on the Gender data set, since it looks like the gender demographic is the most complete across all questions.\nThe aggregated values by demographic aren’t necessary in the final version of any of these data sets, but we may need them to get the response volumes of some of the demographic values if there few responses. So, we can filter against all of the other demographic variables to only included their roll-up or overall value. First we will check for what those values are for each variable.\n\nunique(d4$Age) #'All Ages', also note some of the age bins are overlapping.\n\n[1] \"18 to 24 Years\"     \"18 to 44 Years\"     \"25 to 44 Years\"    \n[4] \"45 to 64 Years\"     \"65 Years and Older\" \"Age 20 and Older\"  \n[7] \"Age 25 and Older\"   \"All Ages\"          \n\nunique(d4$Gender) #'Overall'\n\n[1] \"Overall\" \"Female\"  \"Male\"   \n\nunique(d4$Race) #'All Races'\n\n[1] \"All Races\"                     \"African American\"             \n[3] \"American Indian/Alaska Native\" \"Asian/Pacific Islander\"       \n[5] \"Hispanic\"                      \"White\"                        \n\nunique(d4$Education) #'All Grades'\n\n[1] \"All Grades\"   \"&lt; 12th Grade\" \"&gt; 12th Grade\" \"12th Grade\"  \n\n\n\n# Age dataset, looking at rollup for all other variables except age, \n# then selecting relevant columns and filtering any last missing values\nAgeSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age != 'All Ages'\n              ,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\n# Gender Dataset. This time keeping the rollup value as well as the others,\n# but still filtering all other variables to their rollup\n# Also filtering out the national Median rows since its not the same grain as the others\n# finally removing last missing values. Will select relevant columns later.\nGndSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age == 'All Ages'\n              #,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  filter(LocationDesc != 'National Median (States and DC)',\n         !is.na(CigCurrSmkerRespFreq)\n         )\n\n# Race dataset, looking at rollup for all other variables except Race, \n# then selecting relevant columns and filtering any last missing values\nRaceSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age == 'All Ages'\n              ,Gender == 'Overall'\n              #,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\n# Education dataset, looking at rollup for all other variables except Education and Age,\n#since age actually is startified with education (interestingly, not vice-versa)\n# then selecting relevant columns and filtering any last missing values\nEdSrv &lt;- d4 %&gt;% filter(1==1\n              #,Age == 'All Ages'\n              ,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education != 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\nNow to focus in on the gender data set. I did not select specific columns earlier because the treatment was a bit different than the other data sets. This data set in particular has a lot more opportunity for imputing and other transformations, but for the simplicity of this exercise we will select only a few interesting and complete columns. Also one last note, the Gender variables has three values, Male, Female, and Overall. Since the Response frequency is the true variable of interest, I’m going to leave the “Overall” values in. Under other circumstances I would remove these and calculate overall values only when they apply, that way there aren’t “repeated” observations in the data set. However, the response frequency variables are pre-processed and rounded, so it would only unnecessarily complicate things to try and calculate response counts to get a proper aggregation.\n\n#Selecting relevant columns for the remainder of the exercise\nGndSrv &lt;- GndSrv %&gt;% \n  select(\n    Year\n    ,LocationDesc\n    ,Gender\n    ,QuitAttmptRespFreq\n    ,CigStatCurrRespFreq\n    ,CigStatFrmrRespFreq\n    ,CigStatNvrRespFreq\n    )\n\nI have decided to focus in on the questions for Cigarette usage, namely status and if they have attempted to quit in the last year. I also kept Year and Location since those variables were the least problematic, and of course Gender since this is the Gender-specific dataset.\n\nGndSrv %&gt;% group_by(Gender) %&gt;% #grouped by general to see potential differences in mean.\n  summarize( # getting mean values for each of my questions\n    AvgQuitAttempt = mean(QuitAttmptRespFreq)\n    ,AvgCurentSmokers = mean(CigStatCurrRespFreq)\n    ,AvgFormerSmokers = mean(CigStatFrmrRespFreq)\n    ,AvgNeverSmoked = mean(CigStatNvrRespFreq)\n  )\n\n# A tibble: 3 × 5\n  Gender  AvgQuitAttempt AvgCurentSmokers AvgFormerSmokers AvgNeverSmoked\n  &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1 Female           0.526            0.163            0.215          0.622\n2 Male             0.510            0.203            0.283          0.514\n3 Overall          0.518            0.183            0.248          0.569\n\n\nLet’s start by getting some quick descriptive statistics. Notice that this table will get the average Rate for each of the questions across all states and timeframes. The number of combinations we have between categorical variables, especially since the Location variable has so many categories, complicates our ability to get a closer look. Faceting and framing will help with this, as we will see in a moment.\n\nbox1o &lt;- GndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_boxplot() +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\nbox1o\n\n\n\n\n\n\n\nhist1o &lt;- GndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\nhist1o\n\n\n\n\n\n\n\n\nthe boxplots should show the distribution of the Percent of Current Smokers across the different locations. I have filter to 2011, the earliest year in the data set, and 2019, the latest year. Here I wanted to see if there would be a large difference from the beginning of the timeframe to the end, and for the most part we can see that there is. It appears as though we have a lower rate of “Current” Smokers from beginning to end. the distribution is not particularly clean, but it also looks to be shifting left while keeping some outliers on the higher end to make for a bit of a right skew.\n\nGndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=QuitAttmptRespFreq)) + # Quit Attempt Rate observed\n  geom_boxplot() +\n  labs(x = 'Percent of Quit Attempts\\nAmong Current Smokers') +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\n\n\n\n\n\n\n\nGndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=QuitAttmptRespFreq)) + # Quit Attempt Rate observed\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Quit Attempts\\nAmong Current Smokers', y = NULL) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\n\n\n\n\n\n\n\n\nGoing into the quit rate, it’s a bit of a different story here. There is maybe a little bit of a left shift in the Males and Overall, but it is far less pronounced than it was for the percent of current smokers. This makes sense, since it is the quit rate for current smokers, meaning we would expect it to either stay the same or potentially increase over time. This begs the question if the decrease in current smokers is coming from more people quitting or fewer people starting. The distribution is also all over the place, so there is a wider variance. This will be apparent in the next visual.\n\naniplot &lt;- GndSrv %&gt;% filter(Gender != 'Overall') %&gt;% # wanting Male and Female only\n  ggplot(aes(x=CigStatCurrRespFreq, y = QuitAttmptRespFreq)) + #plotting current smokers against quit attempt rate\n  geom_point(aes(frame = Year, color = Gender, ids = LocationDesc)) + #frame for animation, ids for hover text, color for comparison\n  labs(x = 'Percent of Current Smokers', y = 'Percent of Quit Attempts\\nAmong Current Smokers')\n\nWarning in geom_point(aes(frame = Year, color = Gender, ids = LocationDesc)):\nIgnoring unknown aesthetics: frame and ids\n\nggplotly(aniplot) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animaton, also slows down the slider when i click play\n\n\n\n\n\nThis was a pretty telling visual, so a few observations. First, there tend to be a higher rate of men who are current smokers over women. Second, the distribution is slowly but surely shifting left over time, it was not simply a fluke between 2011 and 2019 earlier. Finally, the wide variance in the percent of quit attempts is apparent, with the amount of vertical movement each point had in the visual. Still the distribution did not look to have any large shift up or down over time, which tells me the rate of people quitting is roughly the same, so more like the decrease in current smokers is happening because fewer people ever start smoking.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in later chunks\n  ggplot(aes(x = CigStatCurrRespFreq, y = fct_reorder(LocationDesc, CigStatCurrRespFreq), frame = Year)) + #fct_reorder orders location by Current smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of Current Smokers', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nMostly wanting to see if any outliers are more apparent here. Guam looks to have the highest rate of people who smoke, but they also have wide swings, likely due to a smaller sample size. Worth noting that the ranking of rates by state does not change significantly over time either.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in other chunks\n  ggplot(aes(x = CigStatFrmrRespFreq, y = fct_reorder(LocationDesc, CigStatFrmrRespFreq), frame = Year)) + #fct_reorder orders location by former smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of Former Smokers', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nThe rate of Former smokers does not look like it shares the trend. Again, if we saw an increase in the rates of people quitting we might expect the rate of Former Smokers to increase over time, but we don’t really see that much here.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in other chunks\n  ggplot(aes(x = CigStatNvrRespFreq, y = fct_reorder(LocationDesc, CigStatNvrRespFreq), frame = Year)) + #fct_reorder orders location by Never smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of People Who Have Never Smoked', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nHere we can see that increase in rate of people who have never smoked, though admittedly it still seems less pronounced than it was for the decrease in Current smokers. Still, it’s in line with the hypothesis that people are more likely are more likely to have never started smoking than they were in the past, and that is why we are seeing such a decrease in the rate of Current Smokers. Not to end on a darker note, but this is also likely a result of those who were Current Smokers in the earlier years dying off at a higher rate than non-smokers. This may suggest that younger people are much less likely to start smoking as well."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#generating-and-comparing-synthetic-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#generating-and-comparing-synthetic-data",
    "title": "CDC Data Exercise",
    "section": "Generating and comparing synthetic data",
    "text": "Generating and comparing synthetic data\n\nThis section contributed by Sean O’Sullivan\n\n# generating a reproducible set of synthetic data based on the characteristics\n# of the original GndSrv data frame\nset.seed(42)\nsynth &lt;- syn(GndSrv, method = c(\"\",\"\",\"\",\"norm\", \"norm\", \"norm\", \"norm\"))\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nsyn.GndSrv &lt;- as.data.frame(synth[3])\n\nIn order to generate some synthetic data that takes on the characteristics of the original data set we utilized the synthpop package’s syn() function and saved the returned data as a new data frame.\n\n# converting a few columns in both the original and synthetic data frames\n# to factor so that the summary function output can be better interpreted\nsyn.GndSrv_fac &lt;- as.data.frame(synth[3]) %&gt;%\n  mutate(syn.Year = as.factor(syn.Year),\n         syn.LocationDesc = as.factor(syn.LocationDesc),\n         syn.Gender = as.factor(syn.Gender))\nGndSrv_fac &lt;- GndSrv %&gt;%\n  mutate(Year = as.factor(Year),\n         LocationDesc = as.factor(LocationDesc),\n         Gender = as.factor(Gender))\n\n# outputting variable summaries for both data frames\nskim(GndSrv_fac)\n\n\nData summary\n\n\nName\nGndSrv_fac\n\n\nNumber of rows\n1428\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nYear\n0\n1\nFALSE\n9\n201: 159, 201: 159, 201: 159, 201: 159\n\n\nLocationDesc\n0\n1\nFALSE\n53\nAla: 27, Ala: 27, Ari: 27, Ark: 27\n\n\nGender\n0\n1\nFALSE\n3\nFem: 476, Mal: 476, Ove: 476\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nQuitAttmptRespFreq\n0\n1\n0.52\n0.05\n0.36\n0.48\n0.52\n0.54\n0.72\n▁▆▇▂▁\n\n\nCigStatCurrRespFreq\n0\n1\n0.18\n0.04\n0.06\n0.15\n0.18\n0.21\n0.36\n▁▇▇▂▁\n\n\nCigStatFrmrRespFreq\n0\n1\n0.25\n0.04\n0.10\n0.22\n0.25\n0.28\n0.35\n▁▂▆▇▂\n\n\nCigStatNvrRespFreq\n0\n1\n0.57\n0.07\n0.39\n0.52\n0.56\n0.61\n0.82\n▂▇▇▂▁\n\n\n\n\nskim(syn.GndSrv_fac)\n\n\nData summary\n\n\nName\nsyn.GndSrv_fac\n\n\nNumber of rows\n1428\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nsyn.Year\n0\n1\nFALSE\n9\n201: 159, 201: 159, 201: 159, 201: 159\n\n\nsyn.LocationDesc\n0\n1\nFALSE\n53\nAla: 27, Ala: 27, Ari: 27, Ark: 27\n\n\nsyn.Gender\n0\n1\nFALSE\n3\nFem: 476, Mal: 476, Ove: 476\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsyn.QuitAttmptRespFreq\n0\n1\n0.52\n0.05\n0.38\n0.48\n0.52\n0.55\n0.71\n▁▇▇▂▁\n\n\nsyn.CigStatCurrRespFreq\n0\n1\n0.18\n0.04\n0.04\n0.15\n0.18\n0.21\n0.33\n▁▅▇▃▁\n\n\nsyn.CigStatFrmrRespFreq\n0\n1\n0.25\n0.04\n0.11\n0.22\n0.25\n0.28\n0.36\n▁▂▇▆▁\n\n\nsyn.CigStatNvrRespFreq\n0\n1\n0.57\n0.07\n0.38\n0.52\n0.56\n0.62\n0.84\n▁▇▇▂▁\n\n\n\n\n# comparing overall distributions\n# subset just the numeric features\nGndSrv_num &lt;- GndSrv %&gt;%\n  select(where(is.numeric)) %&gt;% \n  select(-Year)\n\n# function for plotting multiple columns iteratively through the data frame\nhistfunco &lt;- function(colname) {\ncolname &lt;- sym(colname)\nplot &lt;- GndSrv_num %&gt;% \n  ggplot(aes(x = !!colname)) +\n  geom_histogram(aes(y = after_stat(density)), col =\"white\", fill = \"aquamarine2\", bins = 30) +\n  geom_density(col = \"aquamarine3\") +\n  ylab(NULL) +\n  theme(axis.text.y=element_blank(),\n  axis.ticks.y=element_blank())\n}\n\n# iterating through the numeric columns of the data frame with the above function and plotting the results\nhistso &lt;- lapply(colnames(GndSrv_num), FUN = histfunco)\n\n# subset just the numeric features\nsyn.GndSrv_num &lt;- syn.GndSrv %&gt;%\n  select(where(is.numeric)) %&gt;% \n  select(-syn.Year)\n\n# function for plotting multiple columns iteratively through the data frame\nhistfuncs &lt;- function(colname) {\ncolname &lt;- sym(colname)\nplot &lt;- syn.GndSrv_num %&gt;% \n  ggplot(aes(x = !!colname)) +\n  geom_histogram(aes(y = after_stat(density)), col =\"white\", fill = \"coral2\", bins = 30) +\n  geom_density(col = \"coral\") +\n  ylab(NULL) +\n  theme(axis.text.y=element_blank(),\n  axis.ticks.y=element_blank())\n}\n\n# iterating through the numeric columns of the data frame with the above function and plotting the results\nhistss &lt;- lapply(colnames(syn.GndSrv_num), FUN = histfuncs)\n\n#plot all plots\nwrap_plots(wrap_plots(histso, nrow = 1), wrap_plots(histss, nrow = 1), nrow = 2)\n\n\n\n\n\n\n\n\nComparing the overall distribution of the data in the original data frame and the new synthesized data frame we see that the overall shape of the data is very similar. Both data frames contain the same number of total observations, as well as the same composition of observations for each grouping within each factor (year, state, gender). Similarly we see nearly identical mean, standard deviation, and percentiles across each of the numeric variables. We can also see that the variable distributions when plotted are effectively visually indistinguishable overall.\n\n# comparing distribution plots for percent of current smokers by year and gender\nbox1s &lt;- syn.GndSrv %&gt;% filter(syn.Year %in% c(max(syn.Year), min(syn.Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=syn.CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_boxplot() +\n  labs(x = 'Percent of Current Smokers', title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(syn.Year), rows = vars(syn.Gender)) #want year and gender stratification\nwrap_plots(box1o, box1s, ncol = 1)\n\n\n\n\n\n\n\nhist1s &lt;- syn.GndSrv %&gt;% filter(syn.Year %in% c(max(syn.Year), min(syn.Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=syn.CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Synthetic Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(syn.Year), rows = vars(syn.Gender)) #want year and gender stratification\nwrap_plots(hist1o, hist1s, ncol = 1)\n\n\n\n\n\n\n\n\nHowever, when we examine the distributions of these features along combinations of Year and Gender we start to see some deviation from the underlying relationships at such a low level. They aren’t entirely dissimilar, but they differ significantly. This is perhaps because the synthpop package lacks enough data points for each combination of strata to create anything that isn’t simply a 1:1 copy of the original data.\nWhen attempting to use the syn.strata() function provided by the synthpop package we are able to better maintain relationships at a lower level by supplying a strata argument with the variable Year, but even the sample size for each strata of Year is too small for the function’s liking – as can be seen below.\n\n# generating a reproducible set of synthetic data based on the characteristics\n# of the original GndSrv data frame\nset.seed(42)\nsynth2 &lt;- syn.strata(GndSrv, strata = \"Year\", method = c(\"\",\"\",\"\",\"norm\", \"norm\", \"norm\", \"norm\"))\n\nNumber of observations in strata (original data):\n2011 2012 2013 2014 2015 2016 2017 2018 2019 \n 159  159  159  159  159  159  159  159  156 \nCAUTION: In the original data some strata (2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019) have limited numbers of observations.\nWe advise that there should be at least 170 observations (100 + 10 * no. of variables\nused in prediction).\n\nm = 1, strata = 2011\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 181 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2012\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 165 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2013\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 146 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2014\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 166 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2015\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 176 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2016\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 137 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2017\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 133 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2018\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2019\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (156) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 165 will be generated from original data of size 156.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nsyn.GndSrv2 &lt;- as.data.frame(synth2[3])\n\n# comparing distribution plots for percent of current smokers by year and gender\nbox2s &lt;- syn.GndSrv2 %&gt;% filter(syn.Year %in% c(max(syn.Year), min(syn.Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=syn.CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_boxplot() +\n  labs(x = 'Percent of Current Smokers', title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(syn.Year), rows = vars(syn.Gender)) #want year and gender stratification\nwrap_plots(box1o, box2s, ncol = 1)\n\n\n\n\n\n\n\nhist2s &lt;- syn.GndSrv2 %&gt;% filter(syn.Year %in% c(max(syn.Year), min(syn.Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=syn.CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Synthetic Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(syn.Year), rows = vars(syn.Gender)) #want year and gender stratification\nwrap_plots(hist1o, hist2s, ncol = 1)"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "My name is William Hyltin, I am currently a student in the Master of Science in Data Analytics program at the University of Texas at San Antonio. I have roughly 2 years of experience as an analyst, a year and a half as a Reporting Analyst for Omnichannel Analytics for a bank, and about 4 months of experience as a Business Strategy Analyst for the Actuary and Analytics organization of an insurance company. Professionally I’ve had the opportunity to use a number of different platforms, to include SQL (Snowflake, PROC SQL, and some SQL Server), SAS Enterprise Guide, Minitab, and Excel. Educationally I’ve had the opportunity to use R, Tableau, SAS (both Base and Enterprise Guide), and Python. Prior to working as an Analyst I worked for a call center as a phone representative, but my aspirations at the time had been to be come an Actuary, for which I was able to complete the first two exams (Probability and Financial Markets). It was ultimately several rejections for an entry level actuarial role that lead to me to my job as a Reporting Analyst, but I am finding that these more Data Analyst and adjacent roles are more enjoyable than the work I would have been doing as an Actuary.\n\n\n\n\n\n\n\nMy work as a reporting analyst was rotational, and had me on a team of several other Strategy Analysts. As a result I ended up doing a lot of strategy analytics work in addition to my reporting responsibilities. Some of my responsibilities in the role were reporting on several call center metrics, like satisfaction survey results, handle time of calls, and call volume. The more strategic work I got to do were things like root cause analysis for changes in performance metrics, estimating the impacts that future changes would have on those metrics, and setting goals for call center employees based on their current performance and skills. Most of the work I did here was done in Snowflake SQL, excel, and SAS. I would use statistical techniques like regression, ANOVA, t-test, and descriptive statistics, then utilize my findings to build reports, visualizations, and make recommendations.\n\n\n\n\n\n\n\nIt has often been said to me that analysts will spend 85% of their time massaging their data before they can start on the “real” analytic work. My role as a Business Strategy Analyst is intended to combat that. I work on a team whose purpose is to identify meaningful transactions or events that our customers are performing in complex and often disparate data, and translating this data into a consumable format so that Analysts can spend more time focusing on answering the questions at hand. This is similar to what a Data Engineer might do, but with a greater focus on how the business itself operates, and what an analyst would be looking for given a request from the business. This also means that analyses that we do are usually done in the name of accuracy over insight, and there is less of an opportunity for statistic methods outside of things like frequency-related measures and the occasional descriptive statistic.\n\n\n\n\n\n\n\nThe genesis of this site is from my Practicum II class for my Masters Program, in which I am excited about the opportunity to create a proper portfolio for myself. I see it as a means of displaying the kind of real work I am capable of, and as a way to inspire myself to take on more curiosity projects that go beyond the requirements of formal education and my profession. I’m also excited for the opportunity to use more R, since its not a language I get to use often but when I do I always enjoy it.\n\n\n\n\n\n\n\nBeyond my professional and educational experience, I have a number of other interests that I struggle to make time for. I play dungeons and dragons fairly regularly on the weekends with a few other friends. I enjoy reading, particularly horror and classic science fiction (think more Flowers for Algernon and Invisible Man than Ender’s Game). My girlfriend and I like going to concerts when we can, most recent standouts have been the reunion tour for the Postal Service and a Hot Mulligan show where one of the openers threw Burger Boy burgers into the crowd. I have two kids, a 6 year old aspiring princess/ doctor/ dancer/ singer/ chef, and a soon to be 14 year old son who is a trumpet and piano master with aspirations to become a music therapist."
  },
  {
    "objectID": "aboutme.html#a-man-who-needs-no-introduction-but-since-youre-here",
    "href": "aboutme.html#a-man-who-needs-no-introduction-but-since-youre-here",
    "title": "About me",
    "section": "",
    "text": "My name is William Hyltin, I am currently a student in the Master of Science in Data Analytics program at the University of Texas at San Antonio. I have roughly 2 years of experience as an analyst, a year and a half as a Reporting Analyst for Omnichannel Analytics for a bank, and about 4 months of experience as a Business Strategy Analyst for the Actuary and Analytics organization of an insurance company. Professionally I’ve had the opportunity to use a number of different platforms, to include SQL (Snowflake, PROC SQL, and some SQL Server), SAS Enterprise Guide, Minitab, and Excel. Educationally I’ve had the opportunity to use R, Tableau, SAS (both Base and Enterprise Guide), and Python. Prior to working as an Analyst I worked for a call center as a phone representative, but my aspirations at the time had been to be come an Actuary, for which I was able to complete the first two exams (Probability and Financial Markets). It was ultimately several rejections for an entry level actuarial role that lead to me to my job as a Reporting Analyst, but I am finding that these more Data Analyst and adjacent roles are more enjoyable than the work I would have been doing as an Actuary."
  },
  {
    "objectID": "aboutme.html#work-as-a-reporting-analyst",
    "href": "aboutme.html#work-as-a-reporting-analyst",
    "title": "About me",
    "section": "",
    "text": "My work as a reporting analyst was rotational, and had me on a team of several other Strategy Analysts. As a result I ended up doing a lot of strategy analytics work in addition to my reporting responsibilities. Some of my responsibilities in the role were reporting on several call center metrics, like satisfaction survey results, handle time of calls, and call volume. The more strategic work I got to do were things like root cause analysis for changes in performance metrics, estimating the impacts that future changes would have on those metrics, and setting goals for call center employees based on their current performance and skills. Most of the work I did here was done in Snowflake SQL, excel, and SAS. I would use statistical techniques like regression, ANOVA, t-test, and descriptive statistics, then utilize my findings to build reports, visualizations, and make recommendations."
  },
  {
    "objectID": "aboutme.html#work-as-a-business-strategy-analyst",
    "href": "aboutme.html#work-as-a-business-strategy-analyst",
    "title": "About me",
    "section": "",
    "text": "It has often been said to me that analysts will spend 85% of their time massaging their data before they can start on the “real” analytic work. My role as a Business Strategy Analyst is intended to combat that. I work on a team whose purpose is to identify meaningful transactions or events that our customers are performing in complex and often disparate data, and translating this data into a consumable format so that Analysts can spend more time focusing on answering the questions at hand. This is similar to what a Data Engineer might do, but with a greater focus on how the business itself operates, and what an analyst would be looking for given a request from the business. This also means that analyses that we do are usually done in the name of accuracy over insight, and there is less of an opportunity for statistic methods outside of things like frequency-related measures and the occasional descriptive statistic."
  },
  {
    "objectID": "aboutme.html#course-aspirations",
    "href": "aboutme.html#course-aspirations",
    "title": "About me",
    "section": "",
    "text": "The genesis of this site is from my Practicum II class for my Masters Program, in which I am excited about the opportunity to create a proper portfolio for myself. I see it as a means of displaying the kind of real work I am capable of, and as a way to inspire myself to take on more curiosity projects that go beyond the requirements of formal education and my profession. I’m also excited for the opportunity to use more R, since its not a language I get to use often but when I do I always enjoy it."
  },
  {
    "objectID": "aboutme.html#but-enough-about-me-lets-talk-about-yours-truly",
    "href": "aboutme.html#but-enough-about-me-lets-talk-about-yours-truly",
    "title": "About me",
    "section": "",
    "text": "Beyond my professional and educational experience, I have a number of other interests that I struggle to make time for. I play dungeons and dragons fairly regularly on the weekends with a few other friends. I enjoy reading, particularly horror and classic science fiction (think more Flowers for Algernon and Invisible Man than Ender’s Game). My girlfriend and I like going to concerts when we can, most recent standouts have been the reunion tour for the Postal Service and a Hot Mulligan show where one of the openers threw Burger Boy burgers into the crowd. I have two kids, a 6 year old aspiring princess/ doctor/ dancer/ singer/ chef, and a soon to be 14 year old son who is a trumpet and piano master with aspirations to become a music therapist."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "This project is intended to serve as R coding practice, both for familiarity with R language and for some practice documenting code. We will be using the gapminder health and income dataset.\nLoading necessary packages for the project, I like to use pacman::p_load() because it checks for installation of packages before loading the library and automatically installs ones that I don’t have, as well it allows me to load multiple in just one line. The only major downside is that it does require the installation of the pacman package to use.\npacman::p_load(dslabs, tidyverse, readxl)\n# I have the library versions commented out here for anyone needing to \n# replicate this code that doesnt have the pacman package installed.\n\n#library(dslabs)\n#library(tidyverse)\n#library(readxl)\nNow we can take a look at the gapminder dataset. We start by using the str() function to get the Structure of the data. Then use the summary() function to get a quick summary of each of the variables in the dataset. Finally we use the class() function to confirm what type of object the gapminder dataset is.\n#help(gapminder) #commented out for the sake of rendering later\nprint('-----Data Structure-----') #these are just to make the outputs a little more readable.\n\n[1] \"-----Data Structure-----\"\n\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\nprint('-----Data Summary-----')\n\n[1] \"-----Data Summary-----\"\n\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\nprint('-----Object Type-----')\n\n[1] \"-----Object Type-----\"\n\nclass(gapminder)\n\n[1] \"data.frame\"\nNow we can filter to the continent variable to just Africa. To do this I use the dplyr function filter() along with the pipe %&gt;%. I primarily do this for readability, although subsetting with base R syntax would be just as easy to do. After that I use str() and summary() again to get the data structure and summaries.\nafricadata &lt;- gapminder %&gt;% filter(continent == 'Africa')\n\nprint('-----Data Structure-----')\n\n[1] \"-----Data Structure-----\"\n\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nprint('-----Data Summary-----')\n\n[1] \"-----Data Summary-----\"\n\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0\nNow we will create two new objects, im.le and pop.le to isolate variables of interest.\nWe will accomplish this by using again the pipe operator %&gt;% and the select() function.\nim.le &lt;- africadata %&gt;% select(infant_mortality, life_expectancy) \n  # select() allows us to choose relevant columns for our new objects\npop.le &lt;- africadata %&gt;% select(population, life_expectancy)\n\nprint('-----Data Structure-----')\n\n[1] \"-----Data Structure-----\"\n\nprint('---Population x Life Expectancy---')\n\n[1] \"---Population x Life Expectancy---\"\n\nstr(im.le)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nprint('---Infant Mortality x Life Expectancy---')\n\n[1] \"---Infant Mortality x Life Expectancy---\"\n\nstr(pop.le)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nprint('-----Data Summary-----')\n\n[1] \"-----Data Summary-----\"\n\nprint('---Population x Life Expectancy---')\n\n[1] \"---Population x Life Expectancy---\"\n\nsummary(im.le)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\nprint('---Infant Mortality x Life Expectancy---')\n\n[1] \"---Infant Mortality x Life Expectancy---\"\n\nsummary(pop.le)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51\nim.le %&gt;% \n  ggplot(aes(x=infant_mortality, y=life_expectancy)) +\n  geom_point() +\n  labs(x='Infant Mortality', y='Life Expectancy', \n       title='Examination of Life Expectancy in African Countries', \n       subtitle='Relationship of Infant Mortality and Life Expectancy')\n\nWarning: Removed 226 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\npop.le %&gt;% \n  ggplot(aes(x=population, y=life_expectancy)) +\n  coord_trans(x='log2') +\n  geom_point() +\n  labs(x='Population', y='Life Expectancy', \n       title='Examination of Life Expectancy in African Countries', \n       subtitle='Relationship of Population and Life Expectancy')\n\nWarning: Removed 51 rows containing missing values (`geom_point()`).\nThe charts imply a negative correlation between infant mortality and life expectancy, as well as a positive correlation between population and life expectancy. However the “streaks” of points that we are seeing are likely due to the year variable from our original dataset. What we are seeing is the year over year change in each country’s life expectancy and infant mortality or population. We can isolate to one year in particular to avoid this. First we should determine which years have missing data for infant_mortality.\nafricadata %&gt;% \n  group_by(year) %&gt;% #group_by function will allow us to easily identify the year\n  summarize(\n    missing_im = sum(is.na(infant_mortality)) #takes advantage of sum function and logical values since TRUE==1\n  )\n\n# A tibble: 57 × 2\n    year missing_im\n   &lt;int&gt;      &lt;int&gt;\n 1  1960         10\n 2  1961         17\n 3  1962         16\n 4  1963         16\n 5  1964         15\n 6  1965         14\n 7  1966         13\n 8  1967         11\n 9  1968         11\n10  1969          7\n# ℹ 47 more rows\nAs we can see, there are missing values for infant mortality all the way up to 1981, and again in 2016. So we will just need to choose a year after 1981, but not 2016.\nWe will isolate to the year 2000 by using the dplyr filter() function again.\nim.le2000 &lt;- africadata %&gt;% \n  filter(year == 2000) %&gt;% \n  select(infant_mortality, life_expectancy)\n\npop.le2000 &lt;- africadata %&gt;% \n  filter(year == 2000) %&gt;% \n  select(population, life_expectancy)\n\nprint('-----Data Structure-----')\n\n[1] \"-----Data Structure-----\"\n\nprint('---Population x Life Expectancy---')\n\n[1] \"---Population x Life Expectancy---\"\n\nstr(im.le2000)\n\n'data.frame':   51 obs. of  2 variables:\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n\nprint('---Infant Mortality x Life Expectancy---')\n\n[1] \"---Infant Mortality x Life Expectancy---\"\n\nstr(pop.le2000)\n\n'data.frame':   51 obs. of  2 variables:\n $ population     : num  31183658 15058638 6949366 1736579 11607944 ...\n $ life_expectancy: num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n\nprint('-----Data Summary-----')\n\n[1] \"-----Data Summary-----\"\n\nprint('---Population x Life Expectancy---')\n\n[1] \"---Population x Life Expectancy---\"\n\nsummary(im.le2000)\n\n infant_mortality life_expectancy\n Min.   : 12.30   Min.   :37.60  \n 1st Qu.: 60.80   1st Qu.:51.75  \n Median : 80.30   Median :54.30  \n Mean   : 78.93   Mean   :56.36  \n 3rd Qu.:103.30   3rd Qu.:60.00  \n Max.   :143.30   Max.   :75.00  \n\nprint('---Infant Mortality x Life Expectancy---')\n\n[1] \"---Infant Mortality x Life Expectancy---\"\n\nsummary(pop.le2000)\n\n   population        life_expectancy\n Min.   :    81154   Min.   :37.60  \n 1st Qu.:  2304687   1st Qu.:51.75  \n Median :  8799165   Median :54.30  \n Mean   : 15659800   Mean   :56.36  \n 3rd Qu.: 17391242   3rd Qu.:60.00  \n Max.   :122876723   Max.   :75.00\nRepeating the process for our plots with the new data:\nim.le2000 %&gt;% \n  ggplot(aes(x=infant_mortality, y=life_expectancy)) +\n  geom_point() +\n  labs(x='Infant Mortality', y='Life Expectancy', \n       title='Examination of Life Expectancy in African Countries', \n       subtitle='Relationship of Infant Mortality and Life Expectancy During the Year 2000')\n\n\n\n\n\n\n\npop.le2000 %&gt;% \n  ggplot(aes(x=population, y=life_expectancy)) +\n  coord_trans(x='log2') +\n  geom_point() +\n  labs(x='Population', y='Life Expectancy', \n       title='Examination of Life Expectancy in African Countries', \n       subtitle='Relationship of Population and Life Expectancy During the Year 2000')\nWe see there is still likely a relationship between infant mortality and life expectancy, but the relationship between population and life expectancy is less apparent, if it is still there at all. To get more conclusive results, we can fit a linear model between the variables using the lm() function.\nfit1 &lt;- lm(life_expectancy~., data = im.le2000)\n#fits life expectancy as a function of infant mortality. \n#Infant mortality is not explicitly listed since it is the only other variable in the object.\n\nfit2 &lt;- lm(life_expectancy~., data = pop.le2000) \n#fits life expectancy as a function of population. \n#Population is not explicitly listed since it is the only other variable in the object.\n\nprint('-----Life Expectancy as a function of Infant Mortality-----')\n\n[1] \"-----Life Expectancy as a function of Infant Mortality-----\"\n\nsummary(fit1) #summary of each linear model fit to get results.\n\n\nCall:\nlm(formula = life_expectancy ~ ., data = im.le2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\nprint('-----Life Expectancy as a function of Population-----')\n\n[1] \"-----Life Expectancy as a function of Population-----\"\n\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ ., data = pop.le2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\nFor the fit of Life Expectancy as a function of Infant Mortality, we see a p-value of 2.826e-08. Against an alpha-level of 0.05, we would conclude that there is a significant linear relationship between Infant Mortality and Life Expectancy.\nFor the fit of Life Expectancy as a function of Population, we see a p-value of 0.6159. Against an alpha-level of 0.05, we would conclude that there is not a significant linear relationship between Population and Life Expectancy."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#the-following-section-is-contributed-by-zane-chumley.",
    "href": "coding-exercise/coding-exercise.html#the-following-section-is-contributed-by-zane-chumley.",
    "title": "R Coding Exercise",
    "section": "The following section is contributed by ZANE CHUMLEY.",
    "text": "The following section is contributed by ZANE CHUMLEY.\n\nPick a dataset\n\n# It's an election year, so let's look at the polls from the year Trump eventually won.\nZaneA03 &lt;- results_us_election_2016\n\n\n\nExplore the dataset\n\n# look at the data's type\nclass(ZaneA03)\n\n[1] \"data.frame\"\n\n# look at the data's structure\nstr(ZaneA03)\n\n'data.frame':   51 obs. of  5 variables:\n $ state          : chr  \"California\" \"Texas\" \"Florida\" \"New York\" ...\n $ electoral_votes: int  55 38 29 29 20 20 18 16 16 15 ...\n $ clinton        : num  61.7 43.2 47.8 59 55.8 47.9 43.5 45.9 47.3 46.2 ...\n $ trump          : num  31.6 52.2 49 36.5 38.8 48.6 51.7 51 47.5 49.8 ...\n $ others         : num  6.7 4.5 3.2 4.5 5.4 3.6 4.8 3.1 5.2 4 ...\n\n# look at a summary of the data\nsummary(ZaneA03)\n\n    state           electoral_votes    clinton          trump      \n Length:51          Min.   : 3.00   Min.   :21.90   Min.   : 4.10  \n Class :character   1st Qu.: 4.50   1st Qu.:36.00   1st Qu.:41.15  \n Mode  :character   Median : 8.00   Median :46.20   Median :48.70  \n                    Mean   :10.55   Mean   :44.79   Mean   :48.45  \n                    3rd Qu.:11.50   3rd Qu.:51.75   3rd Qu.:57.40  \n                    Max.   :55.00   Max.   :90.90   Max.   :68.60  \n     others      \n Min.   : 1.900  \n 1st Qu.: 4.650  \n Median : 5.800  \n Mean   : 6.767  \n 3rd Qu.: 7.450  \n Max.   :27.000  \n\n\nIt is worth noting that the dataset is significantly less detailed than described in https://cran.r-project.org/web/packages/dslabs/dslabs.pdf. While there are only 5 columns in the dataset, the description indicated many more columns would be provided:\n\nstate. State in which poll was taken. ’U.S‘ is for national polls.\nstartdate. Poll’s start date.\nenddate. Poll’s end date.\npollster. Pollster conducting the poll.\ngrade. Grade assigned by fivethirtyeight to pollster.\nsamplesize. Sample size.\npopulation. Type of population being polled.\nrawpoll_clinton. Percentage for Hillary Clinton.\nrawpoll_trump. Percentage for Donald Trump\nrawpoll_johnson. Percentage for Gary Johnson\nrawpoll_mcmullin. Percentage for Evan McMullin.\nadjpoll_clinton. Fivethirtyeight adjusted percentage for Hillary Clinton.\najdpoll_trump. Fivethirtyeight adjusted percentage for Donald Trump\nadjpoll_johnson. Fivethirtyeight adjusted percentage for Gary Johnson\nadjpoll_mcmullin. Fivethirtyeight adjusted percentage for Evan McMullin\n\n\n\nDo any processing/cleaning you want to do\nFrom the exploration above it does not appear there are any NA values in the data. Let’s check to be sure.\n\n# Any NA values in the state column?\nZaneA03.state.NAs &lt;- ZaneA03[ZaneA03$state==\"NA\",]\nstr(ZaneA03.state.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n# Any NA values in the electoral_votes column?\nZaneA03.electorals.NAs &lt;- ZaneA03[ZaneA03$electoral_votes==\"NA\",]\nstr(ZaneA03.electorals.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n# Any NA values in the clinton column?\nZaneA03.clinton.NAs &lt;- ZaneA03[ZaneA03$clinton==\"NA\",]\nstr(ZaneA03.clinton.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n# Any NA values in the trump column?\nZaneA03.trump.NAs &lt;- ZaneA03[ZaneA03$trump==\"NA\",]\nstr(ZaneA03.trump.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n# Any NA values in the others column?\nZaneA03.others.NAs &lt;- ZaneA03[ZaneA03$others==\"NA\",]\nstr(ZaneA03.others.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n\nThere are no objects in any of the five (5) datasets housing NA values. Therefore, no cleaning is warranted.\nBut are there any outliers?\n\n\nMake a few exploratory figures.\n\n# Let's use boxplots to see if there are any outliers in the four (4) columns containing numerical data\nboxplot(ZaneA03$electoral_votes\n        , main=\"Boxplot of Electorcal Votes\"\n        , xlab=\"\"\n        , horizontal=TRUE\n        )\n\n\n\n\n\n\n\nboxplot(ZaneA03$clinton\n        , main=\"Boxplot of Clinton poll readings\"\n        , xlab=\"\"\n        , horizontal=TRUE\n        )\n\n\n\n\n\n\n\nboxplot(ZaneA03$trump\n        , main=\"Boxplot of Trump poll readings\"\n        , xlab=\"\"\n        , horizontal=TRUE\n        )\n\n\n\n\n\n\n\nboxplot(ZaneA03$others\n        , main=\"Boxplot of pool readings for other candidates\"\n        , xlab=\"\"\n        , horizontal=TRUE\n        )\n\n\n\n\n\n\n\n\n\n\nOptionally, also some tables.\n\n# Let's display all the rows containing an outlier revealed by the boxplots above.\n\n# We'll sorting the rows by the values in each column into new datasets.\n# The sorting will be largest values first.\n# Then we will display the top and/or the bottom of the dataset corresponding to the upper and lower outliers, respectively.\nZaneA03.electorals.sorted &lt;- ZaneA03[order(-ZaneA03$electoral_votes),]\nhead(ZaneA03.electorals.sorted\n     , n=3\n     )\n\n       state electoral_votes clinton trump others\n1 California              55    61.7  31.6    6.7\n2      Texas              38    43.2  52.2    4.5\n3    Florida              29    47.8  49.0    3.2\n\nZaneA03.clinton.sorted &lt;- ZaneA03[order(-ZaneA03$clinton),]\nhead(ZaneA03.clinton.sorted\n     , n=1\n     )\n\n                  state electoral_votes clinton trump others\n51 District of Columbia               3    90.9   4.1      5\n\nZaneA03.trump.sorted &lt;- ZaneA03[order(-ZaneA03$trump),]\ntail(ZaneA03.trump.sorted\n     , n=1\n     )\n\n                  state electoral_votes clinton trump others\n51 District of Columbia               3    90.9   4.1      5\n\nZaneA03.others.sorted &lt;- ZaneA03[order(-ZaneA03$others),]\nhead(ZaneA03.others.sorted\n     , n=5\n     )\n\n        state electoral_votes clinton trump others\n35       Utah               6    27.5  45.5   27.0\n40      Idaho               4    27.5  59.3   13.2\n49    Vermont               3    56.7  30.3   13.1\n44     Alaska               3    36.6  51.3   12.2\n37 New Mexico               5    48.3  40.0   11.7\n\n\n\n\nRun some simple statistical model(s). Your choice.\nHow successful were the polls in predicting which candidate ultimately carried the state in the election? Well, we’ll need another dataset … the results from the voting.\n\n# Load actual votes from 2016 \nZaneA03.votedata &lt;- read_xlsx(\"1976-2020-president.xlsx\"\n                              , sheet=\"2016Flat\"\n                              , col_names = TRUE\n                              )\n# look at the data's type\nclass(ZaneA03.votedata)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# look at the data's structure\nstr(ZaneA03.votedata)\n\ntibble [51 × 9] (S3: tbl_df/tbl/data.frame)\n $ State       : chr [1:51] \"ALABAMA\" \"ALASKA\" \"ARIZONA\" \"ARKANSAS\" ...\n $ VotesClinton: num [1:51] 729547 116454 1161167 380494 8753788 ...\n $ VotesTrump  : num [1:51] 1318255 163387 1252401 684872 4483810 ...\n $ VotesOther  : num [1:51] 75570 38767 159597 65269 943997 ...\n $ TotalVotes  : num [1:51] 2123372 318608 2573165 1130635 14181595 ...\n $ ActClinton  : num [1:51] 34.4 36.6 45.1 33.7 61.7 ...\n $ ActTrump    : num [1:51] 62.1 51.3 48.7 60.6 31.6 ...\n $ ActOthers   : num [1:51] 3.56 12.17 6.2 5.77 6.66 ...\n $ TrumpWin    : logi [1:51] TRUE TRUE TRUE TRUE FALSE FALSE ...\n\n# look at a summary of the data\nsummary(ZaneA03.votedata)\n\n    State            VotesClinton       VotesTrump        VotesOther    \n Length:51          Min.   :  55973   Min.   :  12723   Min.   : 17022  \n Class :character   1st Qu.: 297584   1st Qu.: 377422   1st Qu.: 56876  \n Mode  :character   Median : 780154   Median : 949136   Median : 93418  \n                    Mean   :1291247   Mean   :1235001   Mean   :155854  \n                    3rd Qu.:1810340   3rd Qu.:1575898   3rd Qu.:225032  \n                    Max.   :8753788   Max.   :4685047   Max.   :943997  \n   TotalVotes         ActClinton       ActTrump       ActOthers     \n Min.   :  258788   Min.   :21.63   Min.   : 4.07   Min.   : 1.944  \n 1st Qu.:  758094   1st Qu.:35.99   1st Qu.:41.14   1st Qu.: 4.739  \n Median : 2001336   Median :46.17   Median :48.67   Median : 5.821  \n Mean   : 2682102   Mean   :44.61   Mean   :48.32   Mean   : 7.071  \n 3rd Qu.: 3347920   3rd Qu.:51.31   3rd Qu.:57.44   3rd Qu.: 8.611  \n Max.   :14181595   Max.   :90.48   Max.   :68.63   Max.   :26.998  \n  TrumpWin      \n Mode :logical  \n FALSE:21       \n TRUE :30       \n                \n                \n                \n\n\n\n# Are we fortunate enough that the dataset of polls and the dataset of votes are in the same order by state?   One way to check is through visual inspection.\n\nZaneA03.state.sorted &lt;- ZaneA03[order(ZaneA03$state),]\nZaneA03.sortcheck &lt;- rbind(ZaneA03.state.sorted$state\n                           , ZaneA03.votedata$State\n                            )\nhead(ZaneA03.sortcheck\n     , n=51\n     )\n\n     [,1]      [,2]     [,3]      [,4]       [,5]         [,6]      \n[1,] \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" \"California\" \"Colorado\"\n[2,] \"ALABAMA\" \"ALASKA\" \"ARIZONA\" \"ARKANSAS\" \"CALIFORNIA\" \"COLORADO\"\n     [,7]          [,8]       [,9]                   [,10]     [,11]    \n[1,] \"Connecticut\" \"Delaware\" \"District of Columbia\" \"Florida\" \"Georgia\"\n[2,] \"CONNECTICUT\" \"DELAWARE\" \"DISTRICT OF COLUMBIA\" \"FLORIDA\" \"GEORGIA\"\n     [,12]    [,13]   [,14]      [,15]     [,16]  [,17]    [,18]     \n[1,] \"Hawaii\" \"Idaho\" \"Illinois\" \"Indiana\" \"Iowa\" \"Kansas\" \"Kentucky\"\n[2,] \"HAWAII\" \"IDAHO\" \"ILLINOIS\" \"INDIANA\" \"IOWA\" \"KANSAS\" \"KENTUCKY\"\n     [,19]       [,20]   [,21]      [,22]           [,23]      [,24]      \n[1,] \"Louisiana\" \"Maine\" \"Maryland\" \"Massachusetts\" \"Michigan\" \"Minnesota\"\n[2,] \"LOUISIANA\" \"MAINE\" \"MARYLAND\" \"MASSACHUSETTS\" \"MICHIGAN\" \"MINNESOTA\"\n     [,25]         [,26]      [,27]     [,28]      [,29]    [,30]          \n[1,] \"Mississippi\" \"Missouri\" \"Montana\" \"Nebraska\" \"Nevada\" \"New Hampshire\"\n[2,] \"MISSISSIPPI\" \"MISSOURI\" \"MONTANA\" \"NEBRASKA\" \"NEVADA\" \"NEW HAMPSHIRE\"\n     [,31]        [,32]        [,33]      [,34]            [,35]         \n[1,] \"New Jersey\" \"New Mexico\" \"New York\" \"North Carolina\" \"North Dakota\"\n[2,] \"NEW JERSEY\" \"NEW MEXICO\" \"NEW YORK\" \"NORTH CAROLINA\" \"NORTH DAKOTA\"\n     [,36]  [,37]      [,38]    [,39]          [,40]          [,41]           \n[1,] \"Ohio\" \"Oklahoma\" \"Oregon\" \"Pennsylvania\" \"Rhode Island\" \"South Carolina\"\n[2,] \"OHIO\" \"OKLAHOMA\" \"OREGON\" \"PENNSYLVANIA\" \"RHODE ISLAND\" \"SOUTH CAROLINA\"\n     [,42]          [,43]       [,44]   [,45]  [,46]     [,47]     \n[1,] \"South Dakota\" \"Tennessee\" \"Texas\" \"Utah\" \"Vermont\" \"Virginia\"\n[2,] \"SOUTH DAKOTA\" \"TENNESSEE\" \"TEXAS\" \"UTAH\" \"VERMONT\" \"VIRGINIA\"\n     [,48]        [,49]           [,50]       [,51]    \n[1,] \"Washington\" \"West Virginia\" \"Wisconsin\" \"Wyoming\"\n[2,] \"WASHINGTON\" \"WEST VIRGINIA\" \"WISCONSIN\" \"WYOMING\"\n\n\nThe visual inspection reveals the data is aligned by state.\n\n# The visual inspection reveals the data is aligned by state. \n# So, polls and votes, by the power invested in me by R,\n# I pronounce your merged!\n\n# Build the list of datasets to merge\n\n# ZaneA03.datasetlist &lt;- c(\"ZaneA03.state.sorted\"\n#                         , \"ZaneA03.votedata\"\n#                         )\n# class(ZaneA03.datasetlist)\n\n\n# Go forth and merge!\n\n# Rest of this section commented out so you can see everything above in the render\n# ZaneA03.PollsAndVotes &lt;- rbind(ZaneA03.state.sorted\n#                               , ZaneA03.votedata)\n\n\n# \n#ZaneA03.PollsAndVotes &lt;- ZaneA03.state.sorted + ZaneA03.votedata"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website and data analysis profile",
    "section": "",
    "text": "Pardon the Dust\nThis site is intended to serve as a repository for different analytic exercises and assignments done over the course of my education in the Data Analytics Masters Program at UTSA and beyond. This means that this site will be perpetually under construction. Here you can expect to find:\n\nAnalyses\nVisualizations\nApplications of statistical methods\nThe slow spiral of a man drowning in the sea of his own imposter syndrome\nInsights, hopefully\n\nPlease use the Menu Bar at the top to peruse to your liking.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalytics is a combination of both art and science. Incidentally, robots are already better than us at both, but here’s to trying."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at C:/Users/Hylti/OneDrive/Documents/School/Practicum II/williamhyltin-P2-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  factor                   2     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique\n1 Gender                0             1 FALSE          3\n2 Generation            0             1 FALSE          4\n  top_counts                    \n1 M: 4, F: 3, O: 2              \n2 Gen: 5, Gen: 2, Bab: 1, Mil: 1\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n3 Salary                0             1  95.4 36.6  44  70  81 133  144 ▂▇▂▁▆\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\np5 = mydata %&gt;% ggplot(aes(x=Generation, y=Height)) + geom_boxplot()\nplot(p5)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-generation-boxplot.png\")\nggsave(filename = figure_file, plot=p5) \n\nSaving 7 x 5 in image\n\n\n\np6 = mydata %&gt;% ggplot(aes(x=Weight, y=Salary)) + geom_point()\nplot(p6)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"salary-weight-scatter.png\")\nggsave(filename = figure_file, plot=p6) \n\nSaving 7 x 5 in image\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(here) #to set paths\n\nhere() starts at C:/Users/Hylti/OneDrive/Documents/School/Practicum II/williamhyltin-P2-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  }
]