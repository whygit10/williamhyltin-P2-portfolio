[
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Placeholder file for the future Tidy Tuesday exercise."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "Antonio Flores contributed to this exercise.\nThe structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nIn addition to the standard variables of Height (in centimeters), Weight (in kilograms), and identified gender (Male, Female, Other), I have included twp additional variables: Generation and Salary. Generation represents the respondents age categorized into the self reported generation to which they belong, e.g. Gen Z are those with birth years between 1995 and 2012. Salary is the self reported annual salary in thousands of US dollars, e.g. a value of 70 would represent an annual salary of $70,000."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nfactor\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGeneration\n0\n1\nFALSE\n4\nGen: 5, Gen: 2, Bab: 1, Mil: 1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nSalary\n0\n1\nNA\nNA\nNA\n95.44444\n36.62005\n44\n70\n81\n133\n144\n▂▇▂▁▆"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(here) #to set paths\n\nhere() starts at C:/Users/Hylti/OneDrive/Documents/School/Practicum II/williamhyltin-P2-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                    `Allowed Values`     \n  &lt;chr&gt;           &lt;chr&gt;                                    &lt;chr&gt;                \n1 Height          height in centimeters                    numeric value &gt;0 or …\n2 Weight          weight in kilograms                      numeric value &gt;0 or …\n3 Gender          identified gender (male/female/other)    M/F/O/NA             \n4 Generation      Field representing a person's generation Gen Z/Millennial/Gen…\n5 Salary          Annual Salary in thousands of dollars    numeric value &gt;0 or …\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height     &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"15…\n$ Weight     &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender     &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\"…\n$ Generation &lt;chr&gt; \"Millennial\", \"Gen Z\", \"Baby Boomer\", \"Baby Boomer\", \"Baby …\n$ Salary     &lt;dbl&gt; 144, 77, 140, 68, 54, 133, 70, 44, 142, 77, 99, 144, 98, 81\n\nsummary(rawdata)\n\n    Height              Weight          Gender           Generation       \n Length:14          Min.   :  45.0   Length:14          Length:14         \n Class :character   1st Qu.:  55.0   Class :character   Class :character  \n Mode  :character   Median :  70.0   Mode  :character   Mode  :character  \n                    Mean   : 602.7                                        \n                    3rd Qu.:  90.0                                        \n                    Max.   :7000.0                                        \n                    NA's   :1                                             \n     Salary      \n Min.   : 44.00  \n 1st Qu.: 71.75  \n Median : 89.50  \n Mean   : 97.93  \n 3rd Qu.:138.25  \n Max.   :144.00  \n                 \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender Generation  Salary\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt;\n1 180        80 M      Millennial     144\n2 175        70 O      Gen Z           77\n3 sixty      60 F      Baby Boomer    140\n4 178        76 F      Baby Boomer     68\n5 192        90 NA     Baby Boomer     54\n6 6          55 F      Gen X          133\n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nGeneration\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70.0\n90.00\n7000\n▇▁▁▁▁\n\n\nSalary\n0\n1.00\n97.93\n36.02\n44\n71.75\n89.5\n138.25\n144\n▃▇▃▁▇\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nGeneration\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nSalary\n0\n1.00\n94.69\n35.31\n44\n70.00\n81\n133\n144\n▃▇▃▁▆\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nGeneration\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nSalary\n0\n1.00\n94.69\n35.31\n44\n70.00\n81\n133\n144\n▃▇▃▁▆\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nGeneration\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nSalary\n0\n1\n95.91\n38.22\n44\n69.0\n81\n137.5\n144\n▃▇▂▁▇\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\nWe will also change ‘Generation’ to a factor variable\n\nd3$Gender &lt;- as.factor(d3$Gender)\nd3$Generation &lt;- as.factor(d3$Generation)\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\nGeneration\n0\n1\nFALSE\n4\nGen: 6, Bab: 2, Gen: 2, Mil: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nSalary\n0\n1\n95.91\n38.22\n44\n69.0\n81\n137.5\n144\n▃▇▂▁▇\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\nGeneration\n0\n1\nFALSE\n4\nGen: 5, Gen: 2, Bab: 1, Mil: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nSalary\n0\n1\n95.44\n36.62\n44\n70\n81\n133\n144\n▂▇▂▁▆\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at C:/Users/Hylti/OneDrive/Documents/School/Practicum II/williamhyltin-P2-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "Placeholder file for the future data/results presentation exercise."
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "This exercise will load, process, and explore a text dataset that consists of employee reviews of their current and former employers on LinkedIn. The dataset can be found from Kaggle here.\nStarting with loading our packages, tidyverse for general cleaning, jsonlite to bring in our Json file, and here to make directory referencing easier.\n\npacman::p_load(tidyverse, jsonlite, here, stringr, superml)\n\nNow we will load our data. Json files are not generally square or in a data frame format, but the fromJSON function makes this tremendously easy.\n\nemp_rev &lt;- fromJSON(here('data-exercise', 'employer-reviews.json'))\nhead(emp_rev)\n\n                      ReviewTitle\n1                      Productive\n2                       Stressful\n3 Good Company for Every employee\n4                      Productive\n5                  Non productive\n6                            Good\n                                                                                                                                                                                                                                                                                                                    CompleteReview\n1                                                                                                                                                              Good company, cool workplace, work load little bit higher. Clean environment, disciplined, good cantin, big campus, systematic workflow, lenient but reliable firm.\n2 1. Need to work on boss's whims and fancies 2. Priorities keep changing 3. No regards for work life balance 4. Completing the work before time is stressed too much than completing it well. Involves lots of reworking, blame games; etc. 5. No job boundaries, you will be asked to do any work depending on the requirements.\n3                                                                                                                                                   Good company for every Engineers dream, Full Mediclaim for entired family, Free transport services from company location to home, Township culture for employees,job security.\n4                                                                                                                                                                         I am just pass out bsc in chemistry Typical day at work Mangement Work place good The most enjoyable part of the job I learn more information in company\n5                                                                                                                                                                     Not so fun at work just blame games  Target people and less target at work Paid less  No increment Make you feel low Too much stress  No one understands you\n6                                                                                                                                                                      I working as laboratory technician form last one year in covid19 staff but we are not appreciate of any about awards we also here for work work and work ..\n                                                        URL Rating\n1 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    3.0\n2 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    3.0\n3 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    5.0\n4 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    5.0\n5 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    1.0\n6 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    3.0\n                                            ReviewDetails\n1     (Current Employee)  -  Ghansoli  -  August 30, 2021\n2               (Former Employee)  -   -  August 26, 2021\n3               (Former Employee)  -   -  August 17, 2021\n4              (Current Employee)  -   -  August 17, 2021\n5                (Former Employee)  -   -  August 9, 2021\n6 (Current Employee)  -  Dahej, Gujarat  -  July 22, 2021\n\nstr(emp_rev)\n\n'data.frame':   145209 obs. of  5 variables:\n $ ReviewTitle   : chr  \"Productive\" \"Stressful\" \"Good Company for Every employee\" \"Productive\" ...\n $ CompleteReview: chr  \"Good company, cool workplace, work load little bit higher. Clean environment, disciplined, good cantin, big cam\"| __truncated__ \"1. Need to work on boss's whims and fancies 2. Priorities keep changing 3. No regards for work life balance 4. \"| __truncated__ \"Good company for every Engineers dream, Full Mediclaim for entired family, Free transport services from company\"| __truncated__ \"I am just pass out bsc in chemistry Typical day at work Mangement Work place good The most enjoyable part of th\"| __truncated__ ...\n $ URL           : chr  \"https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews\" \"https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews\" \"https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews\" \"https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews\" ...\n $ Rating        : chr  \"3.0\" \"3.0\" \"5.0\" \"5.0\" ...\n $ ReviewDetails : chr  \"(Current Employee)  -  Ghansoli  -  August 30, 2021\" \"(Former Employee)  -   -  August 26, 2021\" \"(Former Employee)  -   -  August 17, 2021\" \"(Current Employee)  -   -  August 17, 2021\" ...\n\nsummary(emp_rev)\n\n ReviewTitle        CompleteReview         URL               Rating         \n Length:145209      Length:145209      Length:145209      Length:145209     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ReviewDetails     \n Length:145209     \n Class :character  \n Mode  :character  \n\n\nLooking at the columns, we will want to do some cleanup on some of the more categorical ones. Starting with the URL, this may contain information about the employer, which we can extract. First i want to confirm that all the urls start the same way.\n\nsubstr(emp_rev$URL, 1, 26) %&gt;% unique() #substring extracts first 26 characters,\n\n[1] \"https://in.indeed.com/cmp/\"\n\n#unique tells us all of the unique values in the substring'd column\nlength(unique(emp_rev$URL)) #tells us the number of potential company names\n\n[1] 7286\n\n\nNext, I will use some substrings and regex to extract the company name after the above url portion.\n\nd1 &lt;- emp_rev %&gt;% mutate(\n  CompNm = (substr(URL, 27, nchar(URL)) %&gt;% str_extract('.*(?=/)') %&gt;% str_replace_all('-',' '))\n)\n#substring removes the first part of the url, since its always the same at 27 characters\n#str_extract looks for and extracts the first set of characters before the \"/\"\n#str_replace_all removes all of the dashes and replaces them with spaces\nd1$CompNm %&gt;% unique()\n\n [1] \"Reliance Industries Ltd\"         \"Mphasis\"                        \n [3] \"Kpmg\"                            \"Yes Bank\"                       \n [5] \"Sutherland\"                      \"Marriott International, Inc.\"   \n [7] \"DHL\"                             \"Jio\"                            \n [9] \"Vodafoneziggo\"                   \"HP\"                             \n[11] \"Maersk\"                          \"Ride.swiggy\"                    \n[13] \"Jll\"                             \"Alstom\"                         \n[15] \"UnitedHealth Group\"              \"Tata Consultancy Services (tcs)\"\n[17] \"Capgemini\"                       \"Teleperformance\"                \n[19] \"Cognizant Technology Solutions\"  \"Mahindra & Mahindra Ltd\"        \n[21] \"L&T Technology Services Ltd.\"    \"Bharti Airtel Limited\"          \n[23] \"Indeed\"                          \"Hyatt\"                          \n[25] \"Icici Prudential Life Insurance\" \"Accenture\"                      \n[27] \"Honeywell\"                       \"Standard Chartered Bank\"        \n[29] \"Nokia\"                           \"Apollo Hospitals\"               \n[31] \"Tata Aia Life\"                   \"Hdfc Bank\"                      \n[33] \"Bosch\"                           \"Deloitte\"                       \n[35] \"Ey\"                              \"Microsoft\"                      \n[37] \"Barclays\"                        \"JPMorgan Chase\"                 \n[39] \"Muthoot Finance\"                 \"Wns Global Services\"            \n[41] \"Kotak Mahindra Bank\"             \"Infosys\"                        \n[43] \"Oracle\"                          \"Byju's\"                         \n[45] \"Deutsche Bank\"                   \"Hinduja Global Solutions\"       \n[47] \"Ericsson\"                        \"Axis Bank\"                      \n[49] \"IBM\"                             \"Concentrix\"                     \n[51] \"Wells Fargo\"                     \"Google\"                         \n[53] \"Dell Technologies\"               \"Facebook\"                       \n[55] \"Amazon.com\"                      \"Flipkart.com\"                   \n[57] \"American Express\"                \"Citi\"                           \n[59] \"HSBC\"                           \n\n\nThis is cheating a little bit, because I counted the number of characters in the first part of the URL manually, meaning this is not the most robust way to identify the company name, but observing our values it does not look like it caused any problems.\nNext let’s look at the Rating. When the file was read in it looks like it was read as a string, but it would be more useful to us as a number. We’ll start with a quick summary and completeness check.\n\nd1$Rating %&gt;% summary() #summary of variable, understand scope\n\n   Length     Class      Mode \n   145209 character character \n\nsum(d1$Rating=='') + sum(is.na(d1$Rating)) #counts empty and missing values\n\n[1] 0\n\nd1$Rating %&gt;% unique()\n\n[1] \"3.0\" \"5.0\" \"1.0\" \"4.0\" \"2.0\"\n\n\nSo the variable is a string, but does not contain any missing or null values. All of the values fall under the five-point scale, so it should be safe to convert to a number.\n\nd2 &lt;- d1 %&gt;% mutate(\n  Rating = as.numeric(Rating)\n) # converts the Rating variable to numeric and saves it to the same variable.\n\nOne last variable to look at, ReviewDetails. This looks to have three parts to it. The status of the employee, the location, and the date the review was done. I’m most interested in the status for this exercise, but let’s see if we can get all three\n\n#str_split() breaks up the column by the dashes\n#simplify = TRUE turns it into a matrix\n# dim() gives us the number of rows and columns of the matrix, expecting 3 cols\nstr_split(emp_rev$ReviewDetails, '-', simplify = TRUE) %&gt;% dim()\n\n[1] 145209      5\n\n\nThe intention was to split the variable by dashes to create three columns, however it looks like there are some values that contain a dash themselves. This causes two additional columns to appear, so we will have to make some adjustments.\n\n#check for number of columns\nstr_split(emp_rev$ReviewDetails, ' -  ', simplify = TRUE) %&gt;% dim() \n\n[1] 145209      3\n\n#check for number of unique employee statuses\nstr_split(emp_rev$ReviewDetails, ' -  ', simplify = TRUE)[,1] %&gt;% unique() %&gt;% head(10)\n\n [1] \"(Current Employee) \"                                   \n [2] \"(Former Employee) \"                                    \n [3] \"Training   (Former Employee) \"                         \n [4] \"Officer   (Former Employee) \"                          \n [5] \"Leader   (Current Employee) \"                          \n [6] \"health care   (Current Employee) \"                     \n [7] \"Good team worker   (Former Employee) \"                 \n [8] \"Officer   (Current Employee) \"                         \n [9] \"Sr.G.M.Engineering and projects .   (Former Employee) \"\n[10] \"Hospitality   (Former Employee) \"                      \n\n#check for number of empty values\nifelse(str_split(emp_rev$ReviewDetails, ' -  ', simplify = TRUE)[,3] =='',1,0) %&gt;% sum()\n\n[1] 0\n\n\nFortunately, the solution is easier than it first appeared. Originally I was going to approach the split by splitting from the left and the right for Employee Status and Review Date, then removing everything thats in the left and right for location. However, the dashes that split the different details would have two additional spaces after each, so if we include that in the split function we can get the result we are looking for.\nThe Employee Status section fo the Details field looks to have more than just the status for some observations. A quick check might be worth it to see if Employee Title would be worth pursuing.\n\n# splits the columns then checks the first column for the employee status values,\n#then counts those that don't fall into the status value only\nemp_rev %&gt;%\n  mutate(\n    Stat = str_split(ReviewDetails, ' -  ', simplify = TRUE)[,1],\n    Stat = ifelse(Stat %in% c('(Current Employee) ', '(Former Employee) '), 0, 1)\n      ) %&gt;% select(Stat) %&gt;% sum()\n\n[1] 523\n\n\nWith only 523 observations that fall outside of the Current or Former employee status, it’s relatively safe to ignore that part of the ReviewDetails field.\n\n#saves the split column into three new variables.\n#Review Date is tranformed into date format\n#Employee status uses str_extract to get the status vales only\n#Location uses trimws() to remove extrenuous blanks\nd3 &lt;- d2 %&gt;% mutate(\n  ReviewDate = (str_split(ReviewDetails, ' -  ', simplify = TRUE)[,3] %&gt;% \n                  parse_date_time('0m d, y')),\n  EmployeeStatus = (str_split(ReviewDetails, ' -  ', simplify = TRUE)[,1] %&gt;% \n                      str_extract('(Current Employee)|(Former Employee)')),\n  Location = (str_split(ReviewDetails, ' -  ', simplify = TRUE)[,2] %&gt;% \n                trimws())\n  )\n# checks for how many values actually have a location. \n#Primarily to check if the column is worth using\nd3$Location %&gt;% unique() %&gt;% length()\n\n[1] 3780\n\n#Checks to make sure only two values are in the status\nd3$EmployeeStatus %&gt;% unique() %&gt;% length()\n\n[1] 2\n\n\n\n#Null and empty checks for new columns. \n#DatNull does not check for empties because of date format limitation\nd3 %&gt;% summarize(\n  StatNull = sum(EmployeeStatus == '') + sum(is.na(EmployeeStatus)),\n  DatNull = sum(is.na(ReviewDate)),\n  LocNull = sum(Location=='') + sum(is.na(Location))\n)\n\n  StatNull DatNull LocNull\n1        0       0  129942\n\n\nLocation is a pretty empty field, so it can largely be ignored, otherwise our other two variables look great. From here we can move on to the Review text itself.\n\n#lower cases the full review text\nd3 &lt;- d3 %&gt;% mutate(\n  CompleteReview = tolower(CompleteReview)\n)\n\nBefore we do anything we do anything with the reviews, the dataset is huge, and since the next step involves creating a bag of words it would probably be a good idea to filter the dataset. We will pick two companies to filter to as our companies of interest. First let’s look at the number of reviews by company.\n\n#checks the count of reviews by company name\nd3 %&gt;% group_by(CompNm) %&gt;% \n  summarize(\n    cnt = n()\n  ) %&gt;% arrange(-cnt)\n\n# A tibble: 59 × 2\n   CompNm                            cnt\n   &lt;chr&gt;                           &lt;int&gt;\n 1 Tata Consultancy Services (tcs) 14441\n 2 IBM                             10820\n 3 Infosys                         10696\n 4 Accenture                       10137\n 5 Cognizant Technology Solutions   9626\n 6 Hdfc Bank                        6749\n 7 Capgemini                        5248\n 8 Amazon.com                       3385\n 9 L&T Technology Services Ltd.     3226\n10 Concentrix                       3162\n# ℹ 49 more rows\n\n\nLooking at the size, HP and Dell Technologies look pretty reasonable, so we can filter to those two and compare.\n\n#filters to reviews for HP and Dell Technologies, saves to new df\nd4 &lt;- d3 %&gt;% filter(CompNm %in% c('Dell Technologies', 'HP'))\n\nThe next step will create a ‘bag of words’ commonly used for machine learning, but we’re going to use it this time for to get summary information about scores based on the appearance of words.\n\n#initializes the class for CountVectorizer. \n#Only looking at top 100 most frequently used words\ncfv &lt;- CountVectorizer$new(max_features = 100)\n#Transforms the occurence of each word across all reviews into a vector\ncf_mat &lt;- cfv$fit_transform(d4$CompleteReview)\n#transposed for readability\nhead(cf_mat) %&gt;% t()\n\n              [,1] [,2] [,3] [,4] [,5] [,6]\nwork             1    1    1    1    1    0\ngood             1    0    0    4    1    0\nmanagement       0    0    0    1    0    0\ncompany          2    0    1    2    0    0\nplace            0    1    0    0    0    2\nteam             0    0    0    0    0    0\ngreat            0    1    0    0    0    0\nhp               0    1    2    0    1    0\nworking          0    0    0    0    0    1\ndell             0    0    0    0    0    0\njob              0    0    1    0    0    0\nculture          0    0    0    0    0    0\nlife             0    0    0    0    0    0\nenvironment      0    0    0    0    1    0\nlot              0    0    0    0    1    0\npart             0    0    0    0    0    0\nlearn            0    0    0    0    0    2\nbalance          0    0    0    0    0    0\nnew              0    0    0    0    0    0\nfun              0    0    0    0    1    0\nday              0    0    0    0    0    2\nexperience       0    0    0    0    1    1\nbest             0    0    0    0    0    1\ntime             0    0    0    0    0    0\nlearned          0    0    0    0    0    0\nco               0    0    0    0    0    0\nfriendly         1    0    0    2    0    0\nlearning         0    0    0    0    2    0\nemployees        0    0    0    1    0    0\npeople           0    0    0    0    0    0\nemployee         0    0    0    1    0    0\nprocess          0    0    0    0    0    0\nthings           0    0    0    0    0    0\ncan              0    0    0    2    0    0\nworkers          0    0    0    0    0    0\nnice             0    0    0    1    0    0\ns                0    1    0    0    0    0\ncustomer         1    0    0    0    0    1\nwill             0    0    0    1    0    0\nsupport          0    0    0    0    0    1\nalso             0    0    0    0    0    0\none              0    0    0    0    0    0\ncareer           0    1    0    0    0    0\nlike             0    0    0    1    0    0\nget              0    0    2    0    0    0\nmany             0    0    0    0    0    0\ngrowth           0    0    1    0    0    1\nworked           0    0    0    0    1    0\nwell             0    0    0    0    0    0\nskills           0    0    1    0    0    0\nlearnt           0    0    0    0    0    0\nsalary           1    0    0    0    0    0\nevery            0    0    0    0    1    0\nenjoyable        0    0    0    0    0    0\ntraining         0    0    0    0    0    0\nknowledge        0    0    1    0    0    1\ntechnical        0    0    1    0    0    0\nalways           0    0    0    0    0    0\ncustomers        0    0    0    0    0    0\nopportunities    0    0    0    0    0    0\nhardest          0    0    0    0    0    0\nsupportive       0    0    0    0    0    0\nexcellent        0    0    0    0    0    0\nbusiness         0    0    0    0    0    0\nopportunity      0    0    1    0    0    0\nreally           0    1    0    1    0    0\nyears            0    0    0    0    0    0\ndifferent        0    0    0    0    0    0\ngrow             0    0    0    0    0    0\nmanagers         0    0    0    0    0    1\nissues           0    0    0    0    0    0\nactivities       0    0    0    0    1    0\nmuch             0    0    0    0    0    0\ngot              1    0    0    0    0    0\nproject          0    0    0    1    0    0\nhelp             0    0    0    0    0    0\nfirst            0    0    0    0    0    0\nmanager          0    0    0    0    0    0\nclient           0    0    0    0    0    0\npressure         0    0    0    0    0    0\nprofessional     0    0    0    0    0    0\nflexible         0    0    0    0    0    0\nus               0    0    0    0    0    0\ntechnologies     0    0    0    0    0    0\nhelpful          0    0    0    0    0    0\norganization     0    0    0    0    0    0\nenjoyed          0    0    0    0    0    0\nhandling         0    0    0    0    0    0\nworkplace        0    0    0    0    0    0\nbenefits         1    0    0    0    0    0\ntechnology       0    0    0    0    0    0\nsales            0    0    0    0    0    0\noverall          0    0    0    0    1    0\nprojects         0    0    0    0    0    0\nservice          0    0    0    0    0    0\ntypical          0    0    0    0    0    0\namazing          0    0    0    0    0    0\nneed             0    0    0    0    0    0\nhome             0    0    0    0    0    0\nservices         0    0    0    0    0    0\n\n\nNow we combine the bag of words matrix to the dataframe to make summarizing a bit easier\n\n#combines bag of words with orginal data frame\nd5 &lt;- cbind(d4, cf_mat)\n\nWe can take a look at the average score for each word for both companies. Note that the average score is weighted by the number of appearances of a word, that is to say that if a word appears multiple times in a review the score will have a greater weight.\n\n#Multiples the rating by the appearance of each word, then sums that up for each word\n#Then it divides by the total number of appearances of that word\n((d5$Rating * d5[,10:109]) %&gt;% colSums())/(colSums(d5[,10:109]))\n\n         work          good    management       company         place \n     4.239728      4.152515      4.140500      4.208420      4.260095 \n         team         great            hp       working          dell \n     4.221481      4.352092      4.276878      4.249440      4.292874 \n          job       culture          life   environment           lot \n     4.180314      4.321168      4.286114      4.289959      4.220779 \n         part         learn       balance           new           fun \n     4.174620      4.230942      4.242068      4.264916      4.288538 \n          day    experience          best          time       learned \n     4.173973      4.273743      4.497076      4.168182      4.176380 \n           co      friendly      learning     employees        people \n     4.150769      4.305772      4.239370      4.270799      4.191736 \n     employee       process        things           can       workers \n     4.270408      4.169811      4.235741      4.199620      4.158397 \n         nice             s      customer          will       support \n     4.212644      4.127413      4.300797      4.062000      4.235772 \n         also           one        career          like           get \n     4.271967      4.256356      4.267094      4.221729      4.159251 \n         many        growth        worked          well        skills \n     4.176755      4.106436      4.270471      4.308458      4.300752 \n       learnt        salary         every     enjoyable      training \n     4.251256      3.877863      4.318421      4.171123      4.329640 \n    knowledge     technical        always     customers opportunities \n     4.189944      4.247887      4.219373      4.272206      4.262537 \n      hardest    supportive     excellent      business   opportunity \n     4.088496      4.281899      4.489552      4.247678      4.263492 \n       really         years     different          grow      managers \n     4.290657      4.163763      4.184397      4.370107      4.306859 \n       issues    activities          much           got       project \n     4.408397      4.310345      3.923077      4.315175      4.003922 \n         help         first       manager        client      pressure \n     4.360324      4.258621      4.210526      4.136564      4.053097 \n professional      flexible            us  technologies       helpful \n     4.355556      4.247748      4.325792      4.239819      4.303167 \n organization       enjoyed      handling     workplace      benefits \n     4.319444      4.280374      4.202830      4.241706      4.142180 \n   technology         sales       overall      projects       service \n     4.204762      4.328502      4.082927      4.113300      4.336634 \n      typical       amazing          need          home      services \n     4.080808      4.520202      3.953846      4.226804      4.300518 \n\n\nNow lets split up the data by company and see if there are any differences.\n\n#Standard filters saved as new data frames\nDell &lt;- d5 %&gt;% filter(CompNm == 'Dell Technologies')\nHP &lt;- d5 %&gt;% filter(CompNm == 'HP')\n\nWe can use the same logic as before to get the average score by word, but for each of our new dataframes for each company.\n\n#Finds the average score by word for Dell. Saves as dataframe\nDell_scores &lt;- ((Dell$Rating * Dell[,10:109]) %&gt;%\n                  colSums())/ (colSums(Dell[,10:109])) %&gt;% \n  as.data.frame()\n\n#Finds the average score by word for HP. Saves as dataframe\nHP_scores &lt;- ((HP$Rating * HP[,10:109]) %&gt;% \n     colSums())/(colSums(HP[,10:109])) %&gt;% \n  as.data.frame()\n\nDell_scores %&gt;% head()\n\n                  .\nwork       4.210822\ngood       4.118130\nmanagement 4.078498\ncompany    4.172018\nplace      4.226161\nteam       4.212121\n\nHP_scores %&gt;% head()\n\n                  .\nwork       4.264388\ngood       4.181271\nmanagement 4.189117\ncompany    4.238593\nplace      4.292148\nteam       4.229529\n\n\nNow we use these new data frames to check the difference in score for each word. This could be a potential way of identifying certain areas that employees of one company like or dislike more than employees of the other company.\n\n#determines the difference in score each word\nword_scores &lt;- Dell_scores - HP_scores\n#updates the name of the score difference so it can be referenced\ncolnames(word_scores) &lt;- c('score_diff')\n#sorts by the word score\nword_scores &lt;- word_scores %&gt;% arrange(-score_diff)\n#creates a new variable called word, easier to reference than row names\nword_scores$word &lt;- rownames(word_scores)\n\nhead(word_scores)\n\n          score_diff      word\ndell       0.7941176      dell\nfirst      0.2670235     first\nhp         0.2234513        hp\ndifferent  0.1577160 different\nhome       0.1491707      home\nalso       0.1454323      also\n\n\nUltimately we find the scores to not be so different, but we can still see words that tend to “lead” to higher scores than others. Interestingly, a review that includes either of the words Dell or HP would tend to be higher for Dell than HP,\n\n#plots the difference in score by word.\nword_scores %&gt;% head(sum(word_scores$score_diff&gt;0)) %&gt;% ggplot() +\n  geom_col(aes(y=fct_reorder(word, score_diff),x=score_diff)) +\n  labs(y='Word', x='Score Difference', title = 'Dell-HP Score Difference by Word') +\n  theme(axis.text.y = element_text(size=6, angle = 25))\n\n\n\n\n\n\n\n\nThe methodology used here was not terribly robust, so certainly more research could be done. For example using n-grams might get an idea if certain short phrases are more telling, or grouping known “problem” words for one company or another to see if they are mentioned in each others surveys. Other options would be filtering by a score range and seeing what words appear for more positive or negative results."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "My name is William Hyltin, I am currently a student in the Master of Science in Data Analytics program at the University of Texas at San Antonio. I have roughly 2 years of experience as an analyst, a year and a half as a Reporting Analyst for Omnichannel Analytics for a bank, and about 4 months of experience as a Business Strategy Analyst for the Actuary and Analytics organization of an insurance company. Professionally I’ve had the opportunity to use a number of different platforms, to include SQL (Snowflake, PROC SQL, and some SQL Server), SAS Enterprise Guide, Minitab, and Excel. Educationally I’ve had the opportunity to use R, Tableau, SAS (both Base and Enterprise Guide), and Python. Prior to working as an Analyst I worked for a call center as a phone representative, but my aspirations at the time had been to be come an Actuary, for which I was able to complete the first two exams (Probability and Financial Markets). It was ultimately several rejections for an entry level actuarial role that lead to me to my job as a Reporting Analyst, but I am finding that these more Data Analyst and adjacent roles are more enjoyable than the work I would have been doing as an Actuary.\n\n\n\n\n\n\n\nMy work as a reporting analyst was rotational, and had me on a team of several other Strategy Analysts. As a result I ended up doing a lot of strategy analytics work in addition to my reporting responsibilities. Some of my responsibilities in the role were reporting on several call center metrics, like satisfaction survey results, handle time of calls, and call volume. The more strategic work I got to do were things like root cause analysis for changes in performance metrics, estimating the impacts that future changes would have on those metrics, and setting goals for call center employees based on their current performance and skills. Most of the work I did here was done in Snowflake SQL, excel, and SAS. I would use statistical techniques like regression, ANOVA, t-test, and descriptive statistics, then utilize my findings to build reports, visualizations, and make recommendations.\n\n\n\n\n\n\n\nIt has often been said to me that analysts will spend 68% of their time massaging their data before they can start on the “real” analytic work. My role as a Business Strategy Analyst is intended to combat that. I work on a team whose purpose is to identify meaningful transactions or events that our customers are performing in complex and often disparate data, and translating this data into a consumable format so that Analysts can spend more time focusing on answering the questions at hand. This is similar to what a Data Engineer might do, but with a greater focus on how the business itself operates, and what an analyst would be looking for given a request from the business. This also means that analyses that we do are usually done in the name of accuracy over insight, and there is less of an opportunity for statistic methods outside of things like frequency-related measures and the occasional descriptive statistic.\n\n\n\n\n\n\n\nThe genesis of this site is from my Practicum II class for my Masters Program, in which I am excited about the opportunity to create a proper portfolio for myself. I see it as a means of displaying the kind of real work I am capable of, and as a way to inspire myself to take on more curiosity projects that go beyond the requirements of formal education and my profession. I’m also excited for the opportunity to use more R, since its not a language I get to use often but when I do I always enjoy it.\n\n\n\n\n\n\n\nBeyond my professional and educational experience, I have a number of other interests that I struggle to make time for. I play dungeons and dragons fairly regularly on the weekends with a few other friends. I enjoy reading, particularly horror and classic science fiction (think more Flowers for Algernon and Invisible Man than Ender’s Game). My girlfriend and I like going to concerts when we can, most recent standouts have been the reunion tour for the Postal Service and a Hot Mulligan show where one of the openers threw Burger Boy burgers into the crowd. I have two kids, a 6 year old aspiring princess/ doctor/ dancer/ singer/ chef, and a soon to be 14 year old son who is a trumpet and piano master with aspirations to become a music therapist."
  },
  {
    "objectID": "aboutme.html#a-man-who-needs-no-introduction-but-since-youre-here",
    "href": "aboutme.html#a-man-who-needs-no-introduction-but-since-youre-here",
    "title": "About me",
    "section": "",
    "text": "My name is William Hyltin, I am currently a student in the Master of Science in Data Analytics program at the University of Texas at San Antonio. I have roughly 2 years of experience as an analyst, a year and a half as a Reporting Analyst for Omnichannel Analytics for a bank, and about 4 months of experience as a Business Strategy Analyst for the Actuary and Analytics organization of an insurance company. Professionally I’ve had the opportunity to use a number of different platforms, to include SQL (Snowflake, PROC SQL, and some SQL Server), SAS Enterprise Guide, Minitab, and Excel. Educationally I’ve had the opportunity to use R, Tableau, SAS (both Base and Enterprise Guide), and Python. Prior to working as an Analyst I worked for a call center as a phone representative, but my aspirations at the time had been to be come an Actuary, for which I was able to complete the first two exams (Probability and Financial Markets). It was ultimately several rejections for an entry level actuarial role that lead to me to my job as a Reporting Analyst, but I am finding that these more Data Analyst and adjacent roles are more enjoyable than the work I would have been doing as an Actuary."
  },
  {
    "objectID": "aboutme.html#work-as-a-reporting-analyst",
    "href": "aboutme.html#work-as-a-reporting-analyst",
    "title": "About me",
    "section": "",
    "text": "My work as a reporting analyst was rotational, and had me on a team of several other Strategy Analysts. As a result I ended up doing a lot of strategy analytics work in addition to my reporting responsibilities. Some of my responsibilities in the role were reporting on several call center metrics, like satisfaction survey results, handle time of calls, and call volume. The more strategic work I got to do were things like root cause analysis for changes in performance metrics, estimating the impacts that future changes would have on those metrics, and setting goals for call center employees based on their current performance and skills. Most of the work I did here was done in Snowflake SQL, excel, and SAS. I would use statistical techniques like regression, ANOVA, t-test, and descriptive statistics, then utilize my findings to build reports, visualizations, and make recommendations."
  },
  {
    "objectID": "aboutme.html#work-as-a-business-strategy-analyst",
    "href": "aboutme.html#work-as-a-business-strategy-analyst",
    "title": "About me",
    "section": "",
    "text": "It has often been said to me that analysts will spend 68% of their time massaging their data before they can start on the “real” analytic work. My role as a Business Strategy Analyst is intended to combat that. I work on a team whose purpose is to identify meaningful transactions or events that our customers are performing in complex and often disparate data, and translating this data into a consumable format so that Analysts can spend more time focusing on answering the questions at hand. This is similar to what a Data Engineer might do, but with a greater focus on how the business itself operates, and what an analyst would be looking for given a request from the business. This also means that analyses that we do are usually done in the name of accuracy over insight, and there is less of an opportunity for statistic methods outside of things like frequency-related measures and the occasional descriptive statistic."
  },
  {
    "objectID": "aboutme.html#course-aspirations",
    "href": "aboutme.html#course-aspirations",
    "title": "About me",
    "section": "",
    "text": "The genesis of this site is from my Practicum II class for my Masters Program, in which I am excited about the opportunity to create a proper portfolio for myself. I see it as a means of displaying the kind of real work I am capable of, and as a way to inspire myself to take on more curiosity projects that go beyond the requirements of formal education and my profession. I’m also excited for the opportunity to use more R, since its not a language I get to use often but when I do I always enjoy it."
  },
  {
    "objectID": "aboutme.html#but-enough-about-me-lets-talk-about-yours-truly",
    "href": "aboutme.html#but-enough-about-me-lets-talk-about-yours-truly",
    "title": "About me",
    "section": "",
    "text": "Beyond my professional and educational experience, I have a number of other interests that I struggle to make time for. I play dungeons and dragons fairly regularly on the weekends with a few other friends. I enjoy reading, particularly horror and classic science fiction (think more Flowers for Algernon and Invisible Man than Ender’s Game). My girlfriend and I like going to concerts when we can, most recent standouts have been the reunion tour for the Postal Service and a Hot Mulligan show where one of the openers threw Burger Boy burgers into the crowd. I have two kids, a 6 year old aspiring princess/ doctor/ dancer/ singer/ chef, and a soon to be 14 year old son who is a trumpet and piano master with aspirations to become a music therapist."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "This project is intended to serve as R coding practice, both for familiarity with R language and for some practice documenting code. We will be using the gapminder health and income dataset.\nLoading necessary packages for the project, I like to use pacman::p_load() because it checks for installation of packages before loading the library and automatically installs ones that I don’t have, as well it allows me to load multiple in just one line. The only major downside is that it does require the installation of the pacman package to use.\npacman::p_load(dslabs, tidyverse, readxl)\n# I have the library versions commented out here for anyone needing to \n# replicate this code that doesnt have the pacman package installed.\n\n#library(dslabs)\n#library(tidyverse)\n#library(readxl)\nNow we can take a look at the gapminder dataset. We start by using the str() function to get the Structure of the data. Then use the summary() function to get a quick summary of each of the variables in the dataset. Finally we use the class() function to confirm what type of object the gapminder dataset is.\n#help(gapminder) #commented out for the sake of rendering later\nprint('-----Data Structure-----') #these are just to make the outputs a little more readable.\n\n[1] \"-----Data Structure-----\"\n\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\nprint('-----Data Summary-----')\n\n[1] \"-----Data Summary-----\"\n\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\nprint('-----Object Type-----')\n\n[1] \"-----Object Type-----\"\n\nclass(gapminder)\n\n[1] \"data.frame\"\nNow we can filter to the continent variable to just Africa. To do this I use the dplyr function filter() along with the pipe %&gt;%. I primarily do this for readability, although subsetting with base R syntax would be just as easy to do. After that I use str() and summary() again to get the data structure and summaries.\nafricadata &lt;- gapminder %&gt;% filter(continent == 'Africa')\n\nprint('-----Data Structure-----')\n\n[1] \"-----Data Structure-----\"\n\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nprint('-----Data Summary-----')\n\n[1] \"-----Data Summary-----\"\n\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0\nNow we will create two new objects, im.le and pop.le to isolate variables of interest.\nWe will accomplish this by using again the pipe operator %&gt;% and the select() function.\nim.le &lt;- africadata %&gt;% select(infant_mortality, life_expectancy) \n  # select() allows us to choose relevant columns for our new objects\npop.le &lt;- africadata %&gt;% select(population, life_expectancy)\n\nprint('-----Data Structure-----')\n\n[1] \"-----Data Structure-----\"\n\nprint('---Population x Life Expectancy---')\n\n[1] \"---Population x Life Expectancy---\"\n\nstr(im.le)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nprint('---Infant Mortality x Life Expectancy---')\n\n[1] \"---Infant Mortality x Life Expectancy---\"\n\nstr(pop.le)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nprint('-----Data Summary-----')\n\n[1] \"-----Data Summary-----\"\n\nprint('---Population x Life Expectancy---')\n\n[1] \"---Population x Life Expectancy---\"\n\nsummary(im.le)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\nprint('---Infant Mortality x Life Expectancy---')\n\n[1] \"---Infant Mortality x Life Expectancy---\"\n\nsummary(pop.le)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51\nim.le %&gt;% \n  ggplot(aes(x=infant_mortality, y=life_expectancy)) +\n  geom_point() +\n  labs(x='Infant Mortality', y='Life Expectancy', \n       title='Examination of Life Expectancy in African Countries', \n       subtitle='Relationship of Infant Mortality and Life Expectancy')\n\nWarning: Removed 226 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\npop.le %&gt;% \n  ggplot(aes(x=population, y=life_expectancy)) +\n  coord_trans(x='log2') +\n  geom_point() +\n  labs(x='Population', y='Life Expectancy', \n       title='Examination of Life Expectancy in African Countries', \n       subtitle='Relationship of Population and Life Expectancy')\n\nWarning: Removed 51 rows containing missing values (`geom_point()`).\nThe charts imply a negative correlation between infant mortality and life expectancy, as well as a positive correlation between population and life expectancy. However the “streaks” of points that we are seeing are likely due to the year variable from our original dataset. What we are seeing is the year over year change in each country’s life expectancy and infant mortality or population. We can isolate to one year in particular to avoid this. First we should determine which years have missing data for infant_mortality.\nafricadata %&gt;% \n  group_by(year) %&gt;% #group_by function will allow us to easily identify the year\n  summarize(\n    missing_im = sum(is.na(infant_mortality)) #takes advantage of sum function and logical values since TRUE==1\n  )\n\n# A tibble: 57 × 2\n    year missing_im\n   &lt;int&gt;      &lt;int&gt;\n 1  1960         10\n 2  1961         17\n 3  1962         16\n 4  1963         16\n 5  1964         15\n 6  1965         14\n 7  1966         13\n 8  1967         11\n 9  1968         11\n10  1969          7\n# ℹ 47 more rows\nAs we can see, there are missing values for infant mortality all the way up to 1981, and again in 2016. So we will just need to choose a year after 1981, but not 2016.\nWe will isolate to the year 2000 by using the dplyr filter() function again.\nim.le2000 &lt;- africadata %&gt;% \n  filter(year == 2000) %&gt;% \n  select(infant_mortality, life_expectancy)\n\npop.le2000 &lt;- africadata %&gt;% \n  filter(year == 2000) %&gt;% \n  select(population, life_expectancy)\n\nprint('-----Data Structure-----')\n\n[1] \"-----Data Structure-----\"\n\nprint('---Population x Life Expectancy---')\n\n[1] \"---Population x Life Expectancy---\"\n\nstr(im.le2000)\n\n'data.frame':   51 obs. of  2 variables:\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n\nprint('---Infant Mortality x Life Expectancy---')\n\n[1] \"---Infant Mortality x Life Expectancy---\"\n\nstr(pop.le2000)\n\n'data.frame':   51 obs. of  2 variables:\n $ population     : num  31183658 15058638 6949366 1736579 11607944 ...\n $ life_expectancy: num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n\nprint('-----Data Summary-----')\n\n[1] \"-----Data Summary-----\"\n\nprint('---Population x Life Expectancy---')\n\n[1] \"---Population x Life Expectancy---\"\n\nsummary(im.le2000)\n\n infant_mortality life_expectancy\n Min.   : 12.30   Min.   :37.60  \n 1st Qu.: 60.80   1st Qu.:51.75  \n Median : 80.30   Median :54.30  \n Mean   : 78.93   Mean   :56.36  \n 3rd Qu.:103.30   3rd Qu.:60.00  \n Max.   :143.30   Max.   :75.00  \n\nprint('---Infant Mortality x Life Expectancy---')\n\n[1] \"---Infant Mortality x Life Expectancy---\"\n\nsummary(pop.le2000)\n\n   population        life_expectancy\n Min.   :    81154   Min.   :37.60  \n 1st Qu.:  2304687   1st Qu.:51.75  \n Median :  8799165   Median :54.30  \n Mean   : 15659800   Mean   :56.36  \n 3rd Qu.: 17391242   3rd Qu.:60.00  \n Max.   :122876723   Max.   :75.00\nRepeating the process for our plots with the new data:\nim.le2000 %&gt;% \n  ggplot(aes(x=infant_mortality, y=life_expectancy)) +\n  geom_point() +\n  labs(x='Infant Mortality', y='Life Expectancy', \n       title='Examination of Life Expectancy in African Countries', \n       subtitle='Relationship of Infant Mortality and Life Expectancy During the Year 2000')\n\n\n\n\n\n\n\npop.le2000 %&gt;% \n  ggplot(aes(x=population, y=life_expectancy)) +\n  coord_trans(x='log2') +\n  geom_point() +\n  labs(x='Population', y='Life Expectancy', \n       title='Examination of Life Expectancy in African Countries', \n       subtitle='Relationship of Population and Life Expectancy During the Year 2000')\nWe see there is still likely a relationship between infant mortality and life expectancy, but the relationship between population and life expectancy is less apparent, if it is still there at all. To get more conclusive results, we can fit a linear model between the variables using the lm() function.\nfit1 &lt;- lm(life_expectancy~., data = im.le2000)\n#fits life expectancy as a function of infant mortality. \n#Infant mortality is not explicitly listed since it is the only other variable in the object.\n\nfit2 &lt;- lm(life_expectancy~., data = pop.le2000) \n#fits life expectancy as a function of population. \n#Population is not explicitly listed since it is the only other variable in the object.\n\nprint('-----Life Expectancy as a function of Infant Mortality-----')\n\n[1] \"-----Life Expectancy as a function of Infant Mortality-----\"\n\nsummary(fit1) #summary of each linear model fit to get results.\n\n\nCall:\nlm(formula = life_expectancy ~ ., data = im.le2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\nprint('-----Life Expectancy as a function of Population-----')\n\n[1] \"-----Life Expectancy as a function of Population-----\"\n\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ ., data = pop.le2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\nFor the fit of Life Expectancy as a function of Infant Mortality, we see a p-value of 2.826e-08. Against an alpha-level of 0.05, we would conclude that there is a significant linear relationship between Infant Mortality and Life Expectancy.\nFor the fit of Life Expectancy as a function of Population, we see a p-value of 0.6159. Against an alpha-level of 0.05, we would conclude that there is not a significant linear relationship between Population and Life Expectancy."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#the-following-section-is-contributed-by-zane-chumley.",
    "href": "coding-exercise/coding-exercise.html#the-following-section-is-contributed-by-zane-chumley.",
    "title": "R Coding Exercise",
    "section": "The following section is contributed by ZANE CHUMLEY.",
    "text": "The following section is contributed by ZANE CHUMLEY.\n\nPick a dataset\n\n# It's an election year, so let's look at the polls from the year Trump eventually won.\nZaneA03 &lt;- results_us_election_2016\n\n\n\nExplore the dataset\n\n# look at the data's type\nclass(ZaneA03)\n\n[1] \"data.frame\"\n\n# look at the data's structure\nstr(ZaneA03)\n\n'data.frame':   51 obs. of  5 variables:\n $ state          : chr  \"California\" \"Texas\" \"Florida\" \"New York\" ...\n $ electoral_votes: int  55 38 29 29 20 20 18 16 16 15 ...\n $ clinton        : num  61.7 43.2 47.8 59 55.8 47.9 43.5 45.9 47.3 46.2 ...\n $ trump          : num  31.6 52.2 49 36.5 38.8 48.6 51.7 51 47.5 49.8 ...\n $ others         : num  6.7 4.5 3.2 4.5 5.4 3.6 4.8 3.1 5.2 4 ...\n\n# look at a summary of the data\nsummary(ZaneA03)\n\n    state           electoral_votes    clinton          trump      \n Length:51          Min.   : 3.00   Min.   :21.90   Min.   : 4.10  \n Class :character   1st Qu.: 4.50   1st Qu.:36.00   1st Qu.:41.15  \n Mode  :character   Median : 8.00   Median :46.20   Median :48.70  \n                    Mean   :10.55   Mean   :44.79   Mean   :48.45  \n                    3rd Qu.:11.50   3rd Qu.:51.75   3rd Qu.:57.40  \n                    Max.   :55.00   Max.   :90.90   Max.   :68.60  \n     others      \n Min.   : 1.900  \n 1st Qu.: 4.650  \n Median : 5.800  \n Mean   : 6.767  \n 3rd Qu.: 7.450  \n Max.   :27.000  \n\n\nIt is worth noting that the dataset is significantly less detailed than described in https://cran.r-project.org/web/packages/dslabs/dslabs.pdf. While there are only 5 columns in the dataset, the description indicated many more columns would be provided:\n\nstate. State in which poll was taken. ’U.S‘ is for national polls.\nstartdate. Poll’s start date.\nenddate. Poll’s end date.\npollster. Pollster conducting the poll.\ngrade. Grade assigned by fivethirtyeight to pollster.\nsamplesize. Sample size.\npopulation. Type of population being polled.\nrawpoll_clinton. Percentage for Hillary Clinton.\nrawpoll_trump. Percentage for Donald Trump\nrawpoll_johnson. Percentage for Gary Johnson\nrawpoll_mcmullin. Percentage for Evan McMullin.\nadjpoll_clinton. Fivethirtyeight adjusted percentage for Hillary Clinton.\najdpoll_trump. Fivethirtyeight adjusted percentage for Donald Trump\nadjpoll_johnson. Fivethirtyeight adjusted percentage for Gary Johnson\nadjpoll_mcmullin. Fivethirtyeight adjusted percentage for Evan McMullin\n\n\n\nDo any processing/cleaning you want to do\nFrom the exploration above it does not appear there are any NA values in the data. Let’s check to be sure.\n\n# Any NA values in the state column?\nZaneA03.state.NAs &lt;- ZaneA03[ZaneA03$state==\"NA\",]\nstr(ZaneA03.state.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n# Any NA values in the electoral_votes column?\nZaneA03.electorals.NAs &lt;- ZaneA03[ZaneA03$electoral_votes==\"NA\",]\nstr(ZaneA03.electorals.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n# Any NA values in the clinton column?\nZaneA03.clinton.NAs &lt;- ZaneA03[ZaneA03$clinton==\"NA\",]\nstr(ZaneA03.clinton.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n# Any NA values in the trump column?\nZaneA03.trump.NAs &lt;- ZaneA03[ZaneA03$trump==\"NA\",]\nstr(ZaneA03.trump.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n# Any NA values in the others column?\nZaneA03.others.NAs &lt;- ZaneA03[ZaneA03$others==\"NA\",]\nstr(ZaneA03.others.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n\nThere are no objects in any of the five (5) datasets housing NA values. Therefore, no cleaning is warranted.\nBut are there any outliers?\n\n\nMake a few exploratory figures.\n\n# Let's use boxplots to see if there are any outliers in the four (4) columns containing numerical data\nboxplot(ZaneA03$electoral_votes\n        , main=\"Boxplot of Electorcal Votes\"\n        , xlab=\"\"\n        , horizontal=TRUE\n        )\n\n\n\n\n\n\n\nboxplot(ZaneA03$clinton\n        , main=\"Boxplot of Clinton poll readings\"\n        , xlab=\"\"\n        , horizontal=TRUE\n        )\n\n\n\n\n\n\n\nboxplot(ZaneA03$trump\n        , main=\"Boxplot of Trump poll readings\"\n        , xlab=\"\"\n        , horizontal=TRUE\n        )\n\n\n\n\n\n\n\nboxplot(ZaneA03$others\n        , main=\"Boxplot of pool readings for other candidates\"\n        , xlab=\"\"\n        , horizontal=TRUE\n        )\n\n\n\n\n\n\n\n\n\n\nOptionally, also some tables.\n\n# Let's display all the rows containing an outlier revealed by the boxplots above.\n\n# We'll sorting the rows by the values in each column into new datasets.\n# The sorting will be largest values first.\n# Then we will display the top and/or the bottom of the dataset corresponding to the upper and lower outliers, respectively.\nZaneA03.electorals.sorted &lt;- ZaneA03[order(-ZaneA03$electoral_votes),]\nhead(ZaneA03.electorals.sorted\n     , n=3\n     )\n\n       state electoral_votes clinton trump others\n1 California              55    61.7  31.6    6.7\n2      Texas              38    43.2  52.2    4.5\n3    Florida              29    47.8  49.0    3.2\n\nZaneA03.clinton.sorted &lt;- ZaneA03[order(-ZaneA03$clinton),]\nhead(ZaneA03.clinton.sorted\n     , n=1\n     )\n\n                  state electoral_votes clinton trump others\n51 District of Columbia               3    90.9   4.1      5\n\nZaneA03.trump.sorted &lt;- ZaneA03[order(-ZaneA03$trump),]\ntail(ZaneA03.trump.sorted\n     , n=1\n     )\n\n                  state electoral_votes clinton trump others\n51 District of Columbia               3    90.9   4.1      5\n\nZaneA03.others.sorted &lt;- ZaneA03[order(-ZaneA03$others),]\nhead(ZaneA03.others.sorted\n     , n=5\n     )\n\n        state electoral_votes clinton trump others\n35       Utah               6    27.5  45.5   27.0\n40      Idaho               4    27.5  59.3   13.2\n49    Vermont               3    56.7  30.3   13.1\n44     Alaska               3    36.6  51.3   12.2\n37 New Mexico               5    48.3  40.0   11.7\n\n\n\n\nRun some simple statistical model(s). Your choice.\nHow successful were the polls in predicting which candidate ultimately carried the state in the election? Well, we’ll need another dataset … the results from the voting.\n\n# Load actual votes from 2016 \nZaneA03.votedata &lt;- read_xlsx(\"1976-2020-president.xlsx\"\n                              , sheet=\"2016Flat\"\n                              , col_names = TRUE\n                              )\n# look at the data's type\nclass(ZaneA03.votedata)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# look at the data's structure\nstr(ZaneA03.votedata)\n\ntibble [51 × 9] (S3: tbl_df/tbl/data.frame)\n $ State       : chr [1:51] \"ALABAMA\" \"ALASKA\" \"ARIZONA\" \"ARKANSAS\" ...\n $ VotesClinton: num [1:51] 729547 116454 1161167 380494 8753788 ...\n $ VotesTrump  : num [1:51] 1318255 163387 1252401 684872 4483810 ...\n $ VotesOther  : num [1:51] 75570 38767 159597 65269 943997 ...\n $ TotalVotes  : num [1:51] 2123372 318608 2573165 1130635 14181595 ...\n $ ActClinton  : num [1:51] 34.4 36.6 45.1 33.7 61.7 ...\n $ ActTrump    : num [1:51] 62.1 51.3 48.7 60.6 31.6 ...\n $ ActOthers   : num [1:51] 3.56 12.17 6.2 5.77 6.66 ...\n $ TrumpWin    : logi [1:51] TRUE TRUE TRUE TRUE FALSE FALSE ...\n\n# look at a summary of the data\nsummary(ZaneA03.votedata)\n\n    State            VotesClinton       VotesTrump        VotesOther    \n Length:51          Min.   :  55973   Min.   :  12723   Min.   : 17022  \n Class :character   1st Qu.: 297584   1st Qu.: 377422   1st Qu.: 56876  \n Mode  :character   Median : 780154   Median : 949136   Median : 93418  \n                    Mean   :1291247   Mean   :1235001   Mean   :155854  \n                    3rd Qu.:1810340   3rd Qu.:1575898   3rd Qu.:225032  \n                    Max.   :8753788   Max.   :4685047   Max.   :943997  \n   TotalVotes         ActClinton       ActTrump       ActOthers     \n Min.   :  258788   Min.   :21.63   Min.   : 4.07   Min.   : 1.944  \n 1st Qu.:  758094   1st Qu.:35.99   1st Qu.:41.14   1st Qu.: 4.739  \n Median : 2001336   Median :46.17   Median :48.67   Median : 5.821  \n Mean   : 2682102   Mean   :44.61   Mean   :48.32   Mean   : 7.071  \n 3rd Qu.: 3347920   3rd Qu.:51.31   3rd Qu.:57.44   3rd Qu.: 8.611  \n Max.   :14181595   Max.   :90.48   Max.   :68.63   Max.   :26.998  \n  TrumpWin      \n Mode :logical  \n FALSE:21       \n TRUE :30       \n                \n                \n                \n\n\n\n# Are we fortunate enough that the dataset of polls and the dataset of votes are in the same order by state?   One way to check is through visual inspection.\n\nZaneA03.state.sorted &lt;- ZaneA03[order(ZaneA03$state),]\nZaneA03.sortcheck &lt;- rbind(ZaneA03.state.sorted$state\n                           , ZaneA03.votedata$State\n                            )\nhead(ZaneA03.sortcheck\n     , n=51\n     )\n\n     [,1]      [,2]     [,3]      [,4]       [,5]         [,6]      \n[1,] \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" \"California\" \"Colorado\"\n[2,] \"ALABAMA\" \"ALASKA\" \"ARIZONA\" \"ARKANSAS\" \"CALIFORNIA\" \"COLORADO\"\n     [,7]          [,8]       [,9]                   [,10]     [,11]    \n[1,] \"Connecticut\" \"Delaware\" \"District of Columbia\" \"Florida\" \"Georgia\"\n[2,] \"CONNECTICUT\" \"DELAWARE\" \"DISTRICT OF COLUMBIA\" \"FLORIDA\" \"GEORGIA\"\n     [,12]    [,13]   [,14]      [,15]     [,16]  [,17]    [,18]     \n[1,] \"Hawaii\" \"Idaho\" \"Illinois\" \"Indiana\" \"Iowa\" \"Kansas\" \"Kentucky\"\n[2,] \"HAWAII\" \"IDAHO\" \"ILLINOIS\" \"INDIANA\" \"IOWA\" \"KANSAS\" \"KENTUCKY\"\n     [,19]       [,20]   [,21]      [,22]           [,23]      [,24]      \n[1,] \"Louisiana\" \"Maine\" \"Maryland\" \"Massachusetts\" \"Michigan\" \"Minnesota\"\n[2,] \"LOUISIANA\" \"MAINE\" \"MARYLAND\" \"MASSACHUSETTS\" \"MICHIGAN\" \"MINNESOTA\"\n     [,25]         [,26]      [,27]     [,28]      [,29]    [,30]          \n[1,] \"Mississippi\" \"Missouri\" \"Montana\" \"Nebraska\" \"Nevada\" \"New Hampshire\"\n[2,] \"MISSISSIPPI\" \"MISSOURI\" \"MONTANA\" \"NEBRASKA\" \"NEVADA\" \"NEW HAMPSHIRE\"\n     [,31]        [,32]        [,33]      [,34]            [,35]         \n[1,] \"New Jersey\" \"New Mexico\" \"New York\" \"North Carolina\" \"North Dakota\"\n[2,] \"NEW JERSEY\" \"NEW MEXICO\" \"NEW YORK\" \"NORTH CAROLINA\" \"NORTH DAKOTA\"\n     [,36]  [,37]      [,38]    [,39]          [,40]          [,41]           \n[1,] \"Ohio\" \"Oklahoma\" \"Oregon\" \"Pennsylvania\" \"Rhode Island\" \"South Carolina\"\n[2,] \"OHIO\" \"OKLAHOMA\" \"OREGON\" \"PENNSYLVANIA\" \"RHODE ISLAND\" \"SOUTH CAROLINA\"\n     [,42]          [,43]       [,44]   [,45]  [,46]     [,47]     \n[1,] \"South Dakota\" \"Tennessee\" \"Texas\" \"Utah\" \"Vermont\" \"Virginia\"\n[2,] \"SOUTH DAKOTA\" \"TENNESSEE\" \"TEXAS\" \"UTAH\" \"VERMONT\" \"VIRGINIA\"\n     [,48]        [,49]           [,50]       [,51]    \n[1,] \"Washington\" \"West Virginia\" \"Wisconsin\" \"Wyoming\"\n[2,] \"WASHINGTON\" \"WEST VIRGINIA\" \"WISCONSIN\" \"WYOMING\"\n\n\nThe visual inspection reveals the data is aligned by state.\n\n# The visual inspection reveals the data is aligned by state. \n# So, polls and votes, by the power invested in me by R,\n# I pronounce your merged!\n\n# Build the list of datasets to merge\n\n# ZaneA03.datasetlist &lt;- c(\"ZaneA03.state.sorted\"\n#                         , \"ZaneA03.votedata\"\n#                         )\n# class(ZaneA03.datasetlist)\n\n\n# Go forth and merge!\n\n# Rest of this section commented out so you can see everything above in the render\n# ZaneA03.PollsAndVotes &lt;- rbind(ZaneA03.state.sorted\n#                               , ZaneA03.votedata)\n\n\n# \n#ZaneA03.PollsAndVotes &lt;- ZaneA03.state.sorted + ZaneA03.votedata"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website and data analysis profile",
    "section": "",
    "text": "Pardon the Dust\nThis site is intended to serve as a repository for different analytic exercises and assignments done over the course of my education in the Data Analytics Masters Program at UTSA and beyond. This means that this site will be perpetually under construction. Here you can expect to find:\n\nAnalyses\nVisualizations\nApplications of statistical methods\nThe slow spiral of a man drowning in the sea of his own imposter syndrome\nInsights, hopefully\n\nPlease use the Menu Bar at the top to peruse to your liking.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalytics is a combination of both art and science. Incidentally, robots are already better than us at both, but here’s to trying."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at C:/Users/Hylti/OneDrive/Documents/School/Practicum II/williamhyltin-P2-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  factor                   2     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique\n1 Gender                0             1 FALSE          3\n2 Generation            0             1 FALSE          4\n  top_counts                    \n1 M: 4, F: 3, O: 2              \n2 Gen: 5, Gen: 2, Bab: 1, Mil: 1\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n3 Salary                0             1  95.4 36.6  44  70  81 133  144 ▂▇▂▁▆\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\np5 = mydata %&gt;% ggplot(aes(x=Generation, y=Height)) + geom_boxplot()\nplot(p5)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-generation-boxplot.png\")\nggsave(filename = figure_file, plot=p5) \n\nSaving 7 x 5 in image\n\n\n\np6 = mydata %&gt;% ggplot(aes(x=Weight, y=Salary)) + geom_point()\nplot(p6)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"salary-weight-scatter.png\")\nggsave(filename = figure_file, plot=p6) \n\nSaving 7 x 5 in image\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(here) #to set paths\n\nhere() starts at C:/Users/Hylti/OneDrive/Documents/School/Practicum II/williamhyltin-P2-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  }
]