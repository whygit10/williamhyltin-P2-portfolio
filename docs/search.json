[
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "For this exercise we are tasked with performing an analysis on the latest Tidy Tuesday dataset. Given that the exercise is intended to give us an opportunity for an end to end analysis, I want to use this as an opportunity to show my thought process when it comes to an analysis. Likely this means there will be plenty of rambling and word vomit, but my intended focus is the work flow itself and less the end result. I will save the polished up manuscripts and final results for larger projects.\nThis week’s dataset is one on American Idol data, scraped from Wikipedia tables as I understand. I don’t have a lot of familiarity with the show since I’ve never been a fan myself, so one way or another this will absolutely be a learning experience.\nWe start by loading the dataset.\n\n#loading packages\npacman::p_load(tidyverse, here, tidytuesdayR, tidymodels, parsnip, skimr, viridis, caret, recipes, workflows, yardstick, kknn, earth, vip)\n\nInstalling package into 'C:/Users/Hylti/AppData/Local/R/win-library/4.3'\n(as 'lib' is unspecified)\n\n\nalso installing the dependency 'cli'\n\n\npackage 'cli' successfully unpacked and MD5 sums checked\npackage 'tidymodels' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\Hylti\\AppData\\Local\\Temp\\RtmpKcPdv4\\downloaded_packages\n\n\n\ntidymodels installed\n\n#code below is taken from the tidy tuesday github\ntuesdata &lt;- tidytuesdayR::tt_load('2024-07-23')\n\n--- Compiling #TidyTuesday Information for 2024-07-23 ----\n\n\n--- There are 6 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 6: `auditions.csv`\n    Downloading file 2 of 6: `eliminations.csv`\n    Downloading file 3 of 6: `finalists.csv`\n    Downloading file 4 of 6: `ratings.csv`\n    Downloading file 5 of 6: `seasons.csv`\n    Downloading file 6 of 6: `songs.csv`\n\n\n--- Download complete ---\n\nauditions &lt;- tuesdata$auditions\neliminations &lt;- tuesdata$eliminations\nfinalists &lt;- tuesdata$finalists\nratings &lt;- tuesdata$ratings\nseasons &lt;- tuesdata$seasons\nsongs &lt;- tuesdata$songs\n\n\nThen looking at the raw data.\n\n#lots of tables, so I'm using lapply to save myself some typing\nlapply(list(auditions, eliminations, finalists, ratings, seasons, songs), head)\n\n[[1]]\n# A tibble: 6 × 12\n  season audition_date_start audition_date_end audition_city      audition_venue\n   &lt;dbl&gt; &lt;date&gt;              &lt;date&gt;            &lt;chr&gt;              &lt;chr&gt;         \n1      1 2002-04-20          2002-04-22        Los Angeles, Cali… Westin Bonave…\n2      1 2002-04-23          2002-04-25        Seattle, Washingt… Hyatt Regency…\n3      1 2002-04-26          2002-04-28        Chicago, Illinois  Congress Plaz…\n4      1 2002-04-29          2002-05-01        New York City, Ne… Millenium Hil…\n5      1 2002-05-03          2002-05-05        Atlanta, Georgia   AmericasMart/…\n6      1 2002-05-05          2002-05-07        Dallas, Texas      Wyndham Anato…\n# ℹ 7 more variables: episodes &lt;chr&gt;, episode_air_date &lt;chr&gt;,\n#   callback_venue &lt;chr&gt;, callback_date_start &lt;date&gt;, callback_date_end &lt;date&gt;,\n#   tickets_to_hollywood &lt;dbl&gt;, guest_judge &lt;chr&gt;\n\n[[2]]\n# A tibble: 6 × 46\n  season place gender contestant        top_36 top_36_2 top_36_3 top_36_4 top_32\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; \n1      1 1     Female Kelly Clarkson    &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n2      1 2     Male   Justin Guarini    &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n3      1 3     Female Nikki McKibbin    &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n4      1 4     Female Tamyra Gray       &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n5      1 5     Male   R. J. Helton      &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n6      1 6     Female Christina Christ… &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n# ℹ 37 more variables: top_32_2 &lt;chr&gt;, top_32_3 &lt;chr&gt;, top_32_4 &lt;chr&gt;,\n#   top_30 &lt;chr&gt;, top_30_2 &lt;chr&gt;, top_30_3 &lt;chr&gt;, top_25 &lt;chr&gt;, top_25_2 &lt;chr&gt;,\n#   top_25_3 &lt;chr&gt;, top_24 &lt;chr&gt;, top_24_2 &lt;chr&gt;, top_24_3 &lt;chr&gt;, top_20 &lt;chr&gt;,\n#   top_20_2 &lt;chr&gt;, top_16 &lt;chr&gt;, top_14 &lt;chr&gt;, top_13 &lt;chr&gt;, top_12 &lt;chr&gt;,\n#   top_11 &lt;chr&gt;, top_11_2 &lt;chr&gt;, wildcard &lt;chr&gt;, comeback &lt;lgl&gt;, top_10 &lt;chr&gt;,\n#   top_9 &lt;chr&gt;, top_9_2 &lt;chr&gt;, top_8 &lt;chr&gt;, top_8_2 &lt;chr&gt;, top_7 &lt;chr&gt;,\n#   top_7_2 &lt;chr&gt;, top_6 &lt;chr&gt;, top_6_2 &lt;chr&gt;, top_5 &lt;chr&gt;, top_5_2 &lt;chr&gt;, …\n\n[[3]]\n# A tibble: 6 × 6\n  Contestant          Birthday  Birthplace           Hometown Description Season\n  &lt;chr&gt;               &lt;chr&gt;     &lt;chr&gt;                &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;\n1 Kelly Clarkson      24-Apr-82 Fort Worth, Texas    Burleso… \"She perfo…      1\n2 Justin Guarini      28-Oct-78 Columbus, Georgia    Doylest… \"He perfor…      1\n3 Nikki McKibbin      28-Sep-78 Grand Prairie, Texas &lt;NA&gt;     \"She had p…      1\n4 Tamyra Gray         26-Jul-79 Takoma Park, Maryla… Atlanta… \"She had a…      1\n5 R. J. Helton        17-May-81 Pasadena, Texas      Cumming… \"J. Helton…      1\n6 Christina Christian 21-Jun-81 Brooklyn, New York   &lt;NA&gt;     \".Christin…      1\n\n[[4]]\n# A tibble: 6 × 17\n  season show_number episode    airdate `18_49_rating_share` viewers_in_millions\n   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;                              &lt;dbl&gt;\n1      1           1 Auditions  June 1… 4.8                                 9.85\n2      1           2 Hollywood… June 1… 5.2                                11.2 \n3      1           3 Top 30: G… June 1… 5.2                                10.3 \n4      1           4 Top 30: G… June 1… 4.7                                 9.47\n5      1           5 Top 30: G… June 2… 4.5                                 9.08\n6      1           6 Top 30: G… June 2… 4.2                                 8.53\n# ℹ 11 more variables: timeslot_et &lt;chr&gt;, dvr_18_49 &lt;chr&gt;,\n#   dvr_viewers_millions &lt;chr&gt;, total_18_49 &lt;chr&gt;,\n#   total_viewers_millions &lt;chr&gt;, weekrank &lt;chr&gt;, ref &lt;lgl&gt;, share &lt;chr&gt;,\n#   nightlyrank &lt;dbl&gt;, rating_share_households &lt;chr&gt;, rating_share &lt;chr&gt;\n\n[[5]]\n# A tibble: 6 × 10\n  season winner     runner_up original_release original_network hosted_by judges\n   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;            &lt;chr&gt;     &lt;chr&gt; \n1      1 Kelly Cla… Justin G… June 11 (2002-0… Fox              Ryan Sea… Paula…\n2      2 Ruben Stu… Clay Aik… January 21 (200… Fox              Ryan Sea… Paula…\n3      3 Fantasia … Diana De… January 19 (200… Fox              Ryan Sea… Paula…\n4      4 Carrie Un… Bo Bice   January 18 (200… Fox              Ryan Sea… Paula…\n5      5 Taylor Hi… Katharin… January 17 (200… Fox              Ryan Sea… Paula…\n6      6 Jordin Sp… Blake Le… January 16 (200… Fox              Ryan Sea… Paula…\n# ℹ 3 more variables: no_of_episodes &lt;dbl&gt;, finals_venue &lt;chr&gt;, mentor &lt;chr&gt;\n\n[[6]]\n# A tibble: 6 × 8\n  season    week                 order contestant song  artist song_theme result\n  &lt;chr&gt;     &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt; \n1 Season_01 20020618_top_30_gro…     1 Tamyra Gr… And … Jenni… &lt;NA&gt;       Advan…\n2 Season_01 20020618_top_30_gro…     2 Jim Verra… When… Doris… &lt;NA&gt;       Advan…\n3 Season_01 20020618_top_30_gro…     3 Adriel He… I'll… Edwin… &lt;NA&gt;       Elimi…\n4 Season_01 20020618_top_30_gro…     4 Rodesia E… Dayd… The M… &lt;NA&gt;       Elimi…\n5 Season_01 20020618_top_30_gro…     5 Natalie B… Crazy Patsy… &lt;NA&gt;       Elimi…\n6 Season_01 20020618_top_30_gro…     6 Brad Estr… Just… James… &lt;NA&gt;       Elimi…\n\n\n\nlapply(list(auditions, eliminations, finalists, ratings, seasons, songs), skim)\n\nWarning: There was 1 warning in `dplyr::summarize()`.\nℹ In argument: `dplyr::across(tidyselect::any_of(variable_names),\n  mangled_skimmers$funs)`.\nℹ In group 0: .\nCaused by warning:\n! There were 20 warnings in `dplyr::summarize()`.\nThe first warning was:\nℹ In argument: `dplyr::across(tidyselect::any_of(variable_names),\n  mangled_skimmers$funs)`.\nCaused by warning in `grepl()`:\n! unable to translate 'Jos&lt;8e&gt; \"Sway\" Penala' to a wide string\nℹ Run `dplyr::last_dplyr_warnings()` to see the 19 remaining warnings.\n\n\n[[1]]\n── Data Summary ────────────────────────\n                           Values\nName                       X[[i]]\nNumber of rows             142   \nNumber of columns          12    \n_______________________          \nColumn type frequency:           \n  character                6     \n  Date                     4     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable    n_missing complete_rate min max empty n_unique whitespace\n1 audition_city            0        1       11  32     0       66          0\n2 audition_venue           0        1        7  50     0      115          0\n3 episodes               126        0.113    1  13     0        9          0\n4 episode_air_date        42        0.704    8  20     0       81          0\n5 callback_venue          43        0.697    7  51     0       84          0\n6 guest_judge            133        0.0634   9  19     0        8          0\n\n── Variable type: Date ─────────────────────────────────────────────────────────\n  skim_variable       n_missing complete_rate min        max        median    \n1 audition_date_start         0         1     2002-04-20 2019-09-21 2010-09-05\n2 audition_date_end           0         1     2002-04-22 2019-09-21 2010-09-05\n3 callback_date_start        13         0.908 2002-02-06 2019-09-21 2010-11-09\n4 callback_date_end          13         0.908 2002-02-06 2019-09-21 2010-11-10\n  n_unique\n1      131\n2      131\n3      118\n4      118\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable        n_missing complete_rate mean    sd p0 p25 p50 p75 p100\n1 season                       0         1     10.4  5.53  1   6  10  15   18\n2 tickets_to_hollywood        48         0.662 41.8 76.9   6  20  29  37  561\n  hist \n1 ▅▅▆▅▇\n2 ▇▁▁▁▁\n\n[[2]]\n── Data Summary ────────────────────────\n                           Values\nName                       X[[i]]\nNumber of rows             456   \nNumber of columns          46    \n_______________________          \nColumn type frequency:           \n  character                44    \n  logical                  1     \n  numeric                  1     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n   skim_variable n_missing complete_rate min max empty n_unique whitespace\n 1 place                 1       0.998     1   5     0       44          0\n 2 gender                1       0.998     4   6     0        2          0\n 3 contestant            1       0.998     3  21     0      454          0\n 4 top_36              384       0.158     3  15     0        7          0\n 5 top_36_2            394       0.136     3  15     0        7          0\n 6 top_36_3            404       0.114     3  16     0        7          0\n 7 top_36_4            435       0.0461    3  16     0        6          0\n 8 top_32              424       0.0702    3  15     0        6          0\n 9 top_32_2            427       0.0636    3  16     0        5          0\n10 top_32_3            433       0.0504    3  15     0        6          0\n11 top_32_4            436       0.0439    3  15     0        6          0\n12 top_30              426       0.0658    3  10     0        6          0\n13 top_30_2            431       0.0548    3  10     0        6          0\n14 top_30_3            436       0.0439    3  10     0        6          0\n15 top_25              431       0.0548    3  10     0        4          0\n16 top_25_2            436       0.0439    3  10     0        4          0\n17 top_25_3            440       0.0351    4  10     0        3          0\n18 top_24              240       0.474     3  10     0        5          0\n19 top_24_2            278       0.390     3  12     0        6          0\n20 top_24_3            360       0.211     4  13     0        4          0\n21 top_20              376       0.175     3  10     0        4          0\n22 top_20_2            424       0.0702    3  10     0        4          0\n23 top_16              440       0.0351    4  10     0        2          0\n24 top_14              428       0.0614    4  10     0        2          0\n25 top_13              404       0.114     4  14     0        5          0\n26 top_12              335       0.265     4  12     0        7          0\n27 top_11              312       0.316     4  15     0        8          0\n28 top_11_2            434       0.0482    4  12     0        3          0\n29 wildcard            350       0.232     3  21     0       11          0\n30 top_10              306       0.329     3  12     0       13          0\n31 top_9               348       0.237     4  15     0        9          0\n32 top_9_2             447       0.0197    4  10     0        2          0\n33 top_8               335       0.265     4  15     0        9          0\n34 top_8_2             440       0.0351    4  12     0        5          0\n35 top_7               344       0.246     4  15     0       10          0\n36 top_7_2             442       0.0307    4  12     0        3          0\n37 top_6               366       0.197     4  14     0       10          0\n38 top_6_2             450       0.0132    4  10     0        2          0\n39 top_5               381       0.164     4  12     0        5          0\n40 top_5_2             451       0.0110    4  13     0        2          0\n41 top_4               396       0.132     4  10     0        4          0\n42 top_4_2             452       0.00877   4  10     0        2          0\n43 top_3               411       0.0987    4  10     0        2          0\n44 finale              417       0.0855    6   9     0        6          0\n\n── Variable type: logical ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate mean count\n1 comeback            456             0  NaN \": \" \n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate mean   sd p0 p25 p50 p75 p100 hist \n1 season                0             1 8.86 5.21  1   4   8  13   18 ▇▅▇▅▆\n\n[[3]]\n── Data Summary ────────────────────────\n                           Values\nName                       X[[i]]\nNumber of rows             190   \nNumber of columns          6     \n_______________________          \nColumn type frequency:           \n  character                5     \n  numeric                  1     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min  max empty n_unique whitespace\n1 Contestant            0         1       3   25     0      190          0\n2 Birthday              1         0.995   8    9     0      186          0\n3 Birthplace            6         0.968  11   30     0      163          0\n4 Hometown             88         0.537  11   26     0       96          0\n5 Description          12         0.937  26 1206     0      178          0\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate mean   sd p0 p25 p50 p75 p100 hist \n1 Season                0             1 8.86 4.86  1   5   9  13   17 ▇▆▅▆▇\n\n[[4]]\n── Data Summary ────────────────────────\n                           Values\nName                       X[[i]]\nNumber of rows             593   \nNumber of columns          17    \n_______________________          \nColumn type frequency:           \n  character                12    \n  logical                  1     \n  numeric                  4     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n   skim_variable           n_missing complete_rate min max empty n_unique\n 1 episode                         0        1        6  62     0      313\n 2 airdate                         0        1        5  17     0      590\n 3 18_49_rating_share              0        1        1   9     0      373\n 4 timeslot_et                   515        0.132   16  19     0        8\n 5 dvr_18_49                     539        0.0911   1   3     0        5\n 6 dvr_viewers_millions          539        0.0911   1   4     0       45\n 7 total_18_49                   539        0.0911   1   3     0       16\n 8 total_viewers_millions        539        0.0911   1   5     0       49\n 9 weekrank                      101        0.830    1   4     0       28\n10 share                         449        0.243    1   3     0       21\n11 rating_share_households       515        0.132    7   9     0       55\n12 rating_share                  284        0.521    1   9     0      195\n   whitespace\n 1          0\n 2          0\n 3          0\n 4          0\n 5          0\n 6          0\n 7          0\n 8          0\n 9          0\n10          0\n11          0\n12          0\n\n── Variable type: logical ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate mean count\n1 ref                 593             0  NaN \": \" \n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable       n_missing complete_rate  mean     sd   p0  p25  p50  p75\n1 season                      0        1       8.30  4.63  1     4    8   12  \n2 show_number                 0        1      19.2  11.7   1     9   18   29  \n3 viewers_in_millions         3        0.995  19.9   7.76  5.38 12.6 21.8 26.1\n4 nightlyrank               569        0.0405  2.08  0.929 1     1    2    3  \n  p100 hist \n1 18   ▇▆▇▃▃\n2 44   ▇▇▆▆▃\n3 38.1 ▆▃▇▆▁\n4  4   ▆▇▁▃▂\n\n[[5]]\n── Data Summary ────────────────────────\n                           Values\nName                       X[[i]]\nNumber of rows             18    \nNumber of columns          10    \n_______________________          \nColumn type frequency:           \n  character                8     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable    n_missing complete_rate min max empty n_unique whitespace\n1 winner                   0         1       8  16     0       18          0\n2 runner_up                0         1       7  20     0       18          0\n3 original_release         0         1      47  53     0       18          0\n4 original_network         0         1       3   3     0        2          0\n5 hosted_by                0         1      13  30     0        2          0\n6 judges                   0         1      37  64     0        8          0\n7 finals_venue             3         0.833  13  23     0        4          0\n8 mentor                  16         0.111  12  15     0        2          0\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable  n_missing complete_rate mean   sd p0   p25  p50  p75 p100 hist \n1 season                 0         1      9.5 5.34  1  5.25  9.5 13.8   18 ▇▆▇▆▇\n2 no_of_episodes        14         0.222 19.5 3.32 16 18.2  19   20.2   24 ▃▇▁▁▃\n\n[[6]]\n── Data Summary ────────────────────────\n                           Values\nName                       X[[i]]\nNumber of rows             2429  \nNumber of columns          8     \n_______________________          \nColumn type frequency:           \n  character                7     \n  numeric                  1     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 season                0         1       9   9     0       18          0\n2 week                  0         1       8  54     0      186          0\n3 contestant            0         1       1  51     0      565          0\n4 song                  0         1       1  64     0     1512          0\n5 artist                0         1       2 513     0      921          0\n6 song_theme         1656         0.318   4  33     0      157          0\n7 result               30         0.988   1  25     0       53          0\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate mean   sd p0 p25 p50 p75 p100 hist \n1 order                 0             1 5.93 4.21  1   3   5   8   40 ▇▂▁▁▁\n\n\n\nA lot of missing values across several different variables and tables. I want to start with the contestants, namely the finalists. The finalists dataset has Hometown and Birthplace, with many missing hometown but few missing birthplace. Looking at the tables on Wikipedia, it appears that many of the people with missing Hometowns in the dataset have their birthplace listed as their hometown. I check around 10 of them across different seasons, which may not be a lot but for the sake of this exercise is enough to make me comfortable to impute the hometown with their birthplace when it’s missing. I think Hometown is a more meaningful variable, so if I do something like determining the impact of location on advancement I think hometown is the more reasonable method. Last thing to note, season 18 is included in the eliminations and song datasets but not the finalists dataset.\n\n# logical if_else statement to impute birthplace only when hometown is missing\nfinalists2 &lt;- finalists %&gt;% mutate(\n  Hometown = if_else(is.na(Hometown), Birthplace, Hometown)\n)\n\n\nThere are only a few with missing hometowns now, and I’m tempted to impute these manually by referencing the material found in Wikipedia since it looks like it’s mostly available there. I will resist the urge for now, moving on to joining some of the dataframes for a master finalist dataset.\nEliminations has the placement of the contestants, as well as their gender, so I want to bring those in. There’s much more in eliminations, but the format of the show is inconsistent across seasons so there would be a lot of cleaning to do to make the data more analysis-friendly. For example, I’d be interested in getting the episode someone is eliminated to then bring in things like ratings and viewership of said episodes, but there aren’t common keys across ratings, songs, or eliminations datasets, which would have the data necessary to accomplish that. For now, I think the placement can serve as the contestants success variable and ratings and viewership will have to be a little more disjointed.\n\n# left join to keep everything from finalists\nfinalists3 &lt;- finalists2 %&gt;% \n  left_join(eliminations,\n            join_by(Contestant == contestant, Season == season)\n            ) %&gt;% \n  select(c(names(finalists2), place, gender))\nfinalists3 %&gt;% head()\n\n# A tibble: 6 × 8\n  Contestant        Birthday Birthplace Hometown Description Season place gender\n  &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Kelly Clarkson    24-Apr-… Fort Wort… Burleso… \"She perfo…      1 1     Female\n2 Justin Guarini    28-Oct-… Columbus,… Doylest… \"He perfor…      1 2     Male  \n3 Nikki McKibbin    28-Sep-… Grand Pra… Grand P… \"She had p…      1 3     Female\n4 Tamyra Gray       26-Jul-… Takoma Pa… Atlanta… \"She had a…      1 4     Female\n5 R. J. Helton      17-May-… Pasadena,… Cumming… \"J. Helton…      1 5     Male  \n6 Christina Christ… 21-Jun-… Brooklyn,… Brookly… \".Christin…      1 6     Female\n\nfinalists3 %&gt;% filter(is.na(place))\n\n# A tibble: 6 × 8\n  Contestant        Birthday Birthplace Hometown Description Season place gender\n  &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 \"Rebecca \\\"Becky… 13-Jul-… Dobbs Fer… Dobbs F… \"Her origi…      5 &lt;NA&gt;  &lt;NA&gt;  \n2 \"William \\\"Will\\… 2-Mar-89 The Woodl… The Woo… \"In high s…      5 &lt;NA&gt;  &lt;NA&gt;  \n3 \"Jos\\x8e \\\"Sway\\… 23-Jan-… &lt;NA&gt;       &lt;NA&gt;     \"He was th…      5 &lt;NA&gt;  &lt;NA&gt;  \n4 \"Bobby Bennett, … 4-Jun-86 Denver, C… Denver,… \"(born Jun…      5 &lt;NA&gt;  &lt;NA&gt;  \n5 \"Chikezie Eze\"    11-Sep-… Inglewood… Inglewo… \"During th…      7 &lt;NA&gt;  &lt;NA&gt;  \n6 \"Uch\\x8e\"         15-Jul-… Sugarland… Sugarla… \"As a chil…     17 &lt;NA&gt;  &lt;NA&gt;  \n\n\n\nA few contestants have some foreign characters characters in their names, and the way the tables were read looks like there is some discrepancy that caused different values across our datasets. I will have to correct this first and then join the datasets.\n\n# Some simple string replacements and conditionals to fix names prior to join\nfinalists4 &lt;- finalists2 %&gt;% mutate(\n  Contestant = str_replace_all(Contestant, '\\x8e', 'é'),\n  Contestant = if_else(Contestant == 'Chikezie Eze', 'Chikezie', Contestant),\n  Nickname = trimws(str_extract(Contestant, '\".*?\"'), whitespace = '\"'),\n  Contestant = if_else(!is.na(Nickname) & Nickname != 'Sway', paste(Nickname, word(Contestant, -1)), Contestant),\n  Contestant = if_else(Contestant == 'Bobby Bennett, Jr.', 'Bobby Bennett', Contestant)\n) %&gt;% \n  select(-Nickname) %&gt;% \n  left_join(eliminations,\n            join_by(Contestant == contestant, Season == season)\n            ) %&gt;% \n  select(c(names(finalists2), place, gender)) # only want two variables from eliminations\n\nfinalists4 %&gt;% head() # checking new variables look right\n\n# A tibble: 6 × 8\n  Contestant        Birthday Birthplace Hometown Description Season place gender\n  &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Kelly Clarkson    24-Apr-… Fort Wort… Burleso… \"She perfo…      1 1     Female\n2 Justin Guarini    28-Oct-… Columbus,… Doylest… \"He perfor…      1 2     Male  \n3 Nikki McKibbin    28-Sep-… Grand Pra… Grand P… \"She had p…      1 3     Female\n4 Tamyra Gray       26-Jul-… Takoma Pa… Atlanta… \"She had a…      1 4     Female\n5 R. J. Helton      17-May-… Pasadena,… Cumming… \"J. Helton…      1 5     Male  \n6 Christina Christ… 21-Jun-… Brooklyn,… Brookly… \".Christin…      1 6     Female\n\nfinalists4 %&gt;% filter(is.na(place)) # checking for any nulls from poor join\n\n# A tibble: 0 × 8\n# ℹ 8 variables: Contestant &lt;chr&gt;, Birthday &lt;chr&gt;, Birthplace &lt;chr&gt;,\n#   Hometown &lt;chr&gt;, Description &lt;chr&gt;, Season &lt;dbl&gt;, place &lt;chr&gt;, gender &lt;chr&gt;\n\n\n\nNote that I had to manually override Bobby Bennett’s name because there are other contestants with a “Jr.” suffix that were able to join across the tables.\nFrom the eliminations dataset we brought over the place that each finalist ultimately ended the show in. However, in several instances multiple contestants are eliminated at a time, so they’re placements are listed in the table as a range, e.g. 9-10. We can arguably say these contestants tied for the higher place in the range, so in the 9-10 example two contestants tied for 9th place. So let’s recode those placements accordingly.\n\n# regex to extract only the first number from range placements\nfinalists5 &lt;- finalists4 %&gt;% mutate(\n  numplace = as.numeric(str_extract(place,'(^[0-9]+)'))\n  )\n\nNow the new variable is coded to have only one value and is numeric. At this point it’s a little unclear how useful that will really be, since it’s an integer scale and for many regression problems wouldn’t be appropriate, but we can come back to that later.\n\n# splits up Hometown into city and state, since State will be a smaller category\nfinalists6 &lt;- finalists5 %&gt;% mutate(\n  HomeState = trimws(str_split_fixed(Hometown, ',',2)[,2]),\n  HomeCity = str_split_fixed(Hometown, ',',2)[,1]\n)\n\nfinalists6$HomeState %&gt;% unique()\n\n [1] \"Texas\"             \"Pennsylvania\"      \"Georgia\"          \n [4] \"New York\"          \"California\"        \"Washington\"       \n [7] \"Illinois\"          \"Alabama\"           \"North Carolina\"   \n[10] \"Tennessee\"         \"Utah\"              \"Connecticut\"      \n[13] \"Ohio\"              \"Hawaii\"            \"\"                 \n[16] \"Idaho\"             \"Maui\"              \"Canada\"           \n[19] \"Oklahoma\"          \"New Jersey\"        \"Florida\"          \n[22] \"Nevada\"            \"Louisiana\"         \"Massachusetts\"    \n[25] \"Arkansas\"          \"Colorado\"          \"Arizona\"          \n[28] \"Michigan\"          \"Virginia\"          \"South Carolina\"   \n[31] \"Ireland\"           \"Oregon\"            \"Western Australia\"\n[34] \"Indiana\"           \"Wisconsin\"         \"Merseyside\"       \n[37] \"Mississippi\"       \"Rhode Island\"      \"Karen Carpenter\"  \n[40] \"New Hampshire\"     \"Iowa\"              \"Maryland\"         \n\nfinalists6 %&gt;% filter(HomeState == 'Karen Carpenter')\n\n# A tibble: 1 × 11\n  Contestant    Birthday  Birthplace    Hometown Description Season place gender\n  &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Amber Holcomb 17-Mar-94 e Dion, Kare… e Dion,… \"She origi…     12 4     Female\n# ℹ 3 more variables: numplace &lt;dbl&gt;, HomeState &lt;chr&gt;, HomeCity &lt;chr&gt;\n\n\n\nThere’s a stray ‘Karen Carpenter’ in one value for the new State variable, and looking at the value it appears to be an issue with the original Birthplace variable. We can take care of this easily by recoding the variable manually.\n\n# Manually correcting incorrect location variables for the one contestant\nfinalists7 &lt;- finalists6 %&gt;% mutate(\n  Birthplace = if_else(Birthplace == 'e Dion, Karen Carpenter', NA, Birthplace),\n  Hometown = if_else(Hometown == 'e Dion, Karen Carpenter', NA, Hometown),\n  HomeState = if_else(HomeState == 'Karen Carpenter', NA, HomeState),\n  HomeCity = if_else(HomeCity == 'e Dion', NA, HomeCity)\n)\nfinalists7 %&gt;% filter(HomeState == 'Karen Carpenter'|\n                        Birthplace == 'e Dion, Karen Carpenter'| \n                        Hometown == 'e Dion, Karen Carpenter' | \n                        HomeState == 'Karen Carpenter')\n\n# A tibble: 0 × 11\n# ℹ 11 variables: Contestant &lt;chr&gt;, Birthday &lt;chr&gt;, Birthplace &lt;chr&gt;,\n#   Hometown &lt;chr&gt;, Description &lt;chr&gt;, Season &lt;dbl&gt;, place &lt;chr&gt;, gender &lt;chr&gt;,\n#   numplace &lt;dbl&gt;, HomeState &lt;chr&gt;, HomeCity &lt;chr&gt;\n\n\nAnother variable that might be interesting is age. We have birthdate, however since we have multiple seasons across different years it’s not valuable to us like it is. To remedy this, I will use the auditions dataset to extract a contestant’s age at the time of audition start.\n\n# simplifies Audition data to just get needed dates\nAuditionStarts &lt;- auditions %&gt;% \n  group_by(season) %&gt;% \n  summarize(\n    AudStart = min(audition_date_start)\n  )\n#joins simplified audition data, converts Birthday to date, \n#imputes missing contest birthday, and calculates Age at start of auditions\nfinalists8 &lt;- finalists7 %&gt;% \n  left_join(AuditionStarts, join_by(Season == season)) %&gt;% \n  mutate(Birthday = dmy(Birthday),\n         Birthday = if_else(Contestant == 'Jax', ymd('1996-05-05'), Birthday),\n         Age = floor(as.numeric(interval(Birthday, AudStart), 'years'))\n         )\nfinalists8 %&gt;% filter(is.na(Birthday))\n\n# A tibble: 0 × 13\n# ℹ 13 variables: Contestant &lt;chr&gt;, Birthday &lt;date&gt;, Birthplace &lt;chr&gt;,\n#   Hometown &lt;chr&gt;, Description &lt;chr&gt;, Season &lt;dbl&gt;, place &lt;chr&gt;, gender &lt;chr&gt;,\n#   numplace &lt;dbl&gt;, HomeState &lt;chr&gt;, HomeCity &lt;chr&gt;, AudStart &lt;date&gt;, Age &lt;dbl&gt;\n\n\n\nThis gets us the Age for every finalist at the start of their respective season’s audition start. I could have tried to extract the location of their audition from the Description column and then used that to match up the audition date instead, but this is quicker, quite frankly. One person did not have a value in the Birthday column, and in this case I imputed it manually because the information is available from Wikipedia quickly, and missing numeric values are a much greater problem for model building than missing categorical variables are.\nThat should have the values fixed and additional variables added. Now let’s do a little exploring.\n\n# group bar chart of placement counts by gender\nfinalists8 %&gt;% \n  filter(numplace &lt;= 10) %&gt;% \n  group_by(numplace, gender) %&gt;% \n  summarize(cnt = n()) %&gt;%\n  ggplot() +\n  geom_bar(aes(fill = gender, y = cnt, x=numplace), position = 'dodge', stat = 'identity') +\n  theme_minimal() +\n  scale_fill_manual(values = c('#D81561', '#1379BF')) +\n  labs(title = 'Distribution of Top 10 Male and Female Finalists', x = 'Placement', y = 'Count', fill = 'Gender') +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(color = 'gray')) +\n  scale_y_continuous(expand = c(0,0),limits = c(0, NA)) +\n  scale_x_continuous(breaks = seq(1,10))\n\n`summarise()` has grouped output by 'numplace'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\n# creates a simplified dataset of Homestate contestant frequencies\nHSOrder &lt;- finalists8 %&gt;%\n  filter(numplace &lt;= 10) %&gt;%\n  group_by(HomeState) %&gt;% \n  summarize(\n    ordercnt = n()\n  )\n#joins above HomeState data so that it can used to reorder HomeState variable by\n#total frequency. Horizontal bar chart of location color-coded by placement\nfinalists8 %&gt;% \n  filter(numplace &lt;= 10) %&gt;%\n  group_by(numplace, HomeState) %&gt;% \n  summarize(cnt = n()) %&gt;%\n  left_join(HSOrder, join_by(HomeState==HomeState)) %&gt;% \n  ggplot() +\n  geom_bar(aes(fill = numplace, y = fct_reorder(HomeState, ordercnt), x=cnt), position = 'stack', stat = 'identity') +\n  theme_minimal() +\n  scale_fill_viridis(option = 'mako') +\n  labs(title = 'Count and Placement of Finalists by State', x = 'Count', y = 'Home State', fill = 'Placement') +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(color = 'gray'), axis.text.y = element_text(size = 7)) +\n  scale_x_continuous(expand = c(0,0),limits = c(0, NA))\n\n`summarise()` has grouped output by 'numplace'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\n# Scatterplot of Age by Placement, stratified by Gender. Jitter used for readability\nfinalists8 %&gt;% \n  ggplot() +\n  geom_jitter(aes(color = gender, y = numplace, x=Age)) +\n  theme_minimal() +\n  labs(title = 'Placement vs Age, Stratified by Gender', x = 'Age', y = 'Placement', color = 'Gender') +\n  theme(panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank(), \n        axis.line = element_line(color = 'gray'))\n\n\n\n\n\n\n\n\n\nThe exploratory charts were made to see if there was a relationship between a contestant’s Home State, their gender, or Age and their placement in the competition. Truthfully looking at these charts nothing is really jumping out at me, which surprises me at least a little, especially for Age and Gender. Despite seeing no obvious relationship, I think this is a good opportunity to see if a model would tell us any differently. On paper it looks like the competition is fairly non-discriminatory, at least with the people that make it a far as a finalist, so if a more rigorous statistical method like a model says otherwise then that would be interesting. If not, well that’s good for American Idol right?.\nFirst we’ll need to prepare the data a bit:\n\n# Preparing data for only needed predictors\nmodeldata &lt;- finalists8 %&gt;% \n  mutate(\n    top3 = as.factor(if_else(numplace &lt;= 3, 1,0))\n      ) %&gt;% \n  select(gender, HomeState, Age, top3)\n\n\nWe now have only the variables of interest for our models in the dataset. It’s only four predictors, but given that the number of observations is also relatively small, a simple model is probably for the best. Let’s start with a logistic regression model:\n\nLogistic Regression\n\nlogreg &lt;- logistic_reg() %&gt;% \n  set_engine('glm')\n\nset.seed(42)\ntrainpart &lt;- createDataPartition(modeldata$top3, p=.7)[[1]]\nmodeltrain &lt;- modeldata[trainpart,]\nmodeltest &lt;- modeldata[-trainpart,]\n\nsimprecipe &lt;- recipe(top3 ~ ., data = modeltrain) %&gt;% \n  step_interact(terms = ~ Age:gender) %&gt;%\n  step_novel(HomeState) %&gt;% \n  step_unknown(HomeState) %&gt;% \n  step_dummy(HomeState,gender)\n\n\nlogrecipe &lt;- simprecipe %&gt;% \n  step_normalize(Age)\n  \n\nset.seed(42)\n\nlogreg_wflow &lt;- workflow() %&gt;% \n  add_model(logreg) %&gt;% \n  add_recipe(logrecipe)\n\nlogreg_fit &lt;- logreg_wflow %&gt;% fit(data = modeltrain)\nlogpreds_results &lt;- logreg_fit %&gt;% extract_fit_parsnip() %&gt;% tidy()\nlogpreds_results\n\n# A tibble: 44 × 5\n   term                  estimate std.error  statistic p.value\n   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)            -19.5    5328.    -0.00365    0.997 \n 2 Age                     -0.717     0.441 -1.62       0.104 \n 3 Age_x_genderMale         0.261     0.154  1.70       0.0899\n 4 HomeState_Alabama       18.3    5328.     0.00343    0.997 \n 5 HomeState_Arizona       18.3    5328.     0.00343    0.997 \n 6 HomeState_Arkansas      39.3   12002.     0.00328    0.997 \n 7 HomeState_California    16.7    5328.     0.00313    0.998 \n 8 HomeState_Colorado      -0.646 12002.    -0.0000538  1.00  \n 9 HomeState_Connecticut   -1.03  12002.    -0.0000857  1.00  \n10 HomeState_Florida       -0.441  6231.    -0.0000708  1.00  \n# ℹ 34 more rows\n\n\n\ntestpreds &lt;- bind_cols(\n  top3 = modeltest$top3,\n  predict(logreg_fit, modeltest),\n  predict(logreg_fit, modeltest, type = 'prob')\n)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n\nnames(testpreds) &lt;- c('top3', 'logpreds', 'log0', 'log1')\n\nmodelperf &lt;- bind_rows(\n  acc_log = accuracy(testpreds, top3, logpreds, event_level = 'second'),\n  prec_log = precision(testpreds, top3, logpreds, event_level = 'second'),\n  rec_log = recall(testpreds, top3, logpreds, event_level = 'second'),\n  spec_log = specificity(testpreds, top3, logpreds, event_level = 'second')\n)\nnames(modelperf) &lt;- c('metric', 'estimator', 'log_reg')\nmodelperf\n\n# A tibble: 4 × 3\n  metric      estimator log_reg\n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;\n1 accuracy    binary      0.696\n2 precision   binary      0.364\n3 recall      binary      0.286\n4 specificity binary      0.833\n\n\n\nThe logistic regression model is pretty terrible, but there are a few things to glean here. First off, originally I had run this model without the interaction between Age and Gender, and the p-values for all predictors were well above any rejection threshold. However, including the interaction drastically changes the results for Age and Gender. Age alone is still not significant against any alpha level with a p-value of 0.1042617, but against an alpha level of 0.1 the interaction of Age and Gender is significant with a p-value of 0.0899354. Similarly Gender alone is significant at an alpha level of 0.1 with a p-value of 0.0690929. one could interpret these results as two finalists of the same age but different gender have different odds of making it to the top 3. An alpha level of 0.1 may not be the most stringent or typical value, but this example was not intended to be the most rigorous so I feel comfortable taking the liberty. The location variables are all basically worthless, though surprisingly taking them out causes the model to guess every contestant as Negative, that they would not make it to the top 3. That likely means some feature selection would be necessary if there was any intention of taking this model any further.\nPrediction performance for the model is what truly reveals the quality. The accuracy at first glance seems not totally terrible; with a value of 0.6964286, it’s better than flipping a coin at least. However, it is important to remember this is whether a finalist makes it to the top 3 or not. One season has more than 20 finalists, so in that instance there is a 98.5% chance of randomly choosing someone who is not in the top 3. In fact, among the finalists in our dataset only 25.2631579% of contestants make the top 3, meaning if the model were to guess that nobody made the top 3 it would have an accuracy value of 74.7368421%. This is also made clear by the poor Recall value of 0.2857143, essentially saying that out of the actual positive examples, few are predicted correctly.\nDespite the poor predictive performance this model suggests a relationship may exist, and warrants some more exploration. I will try a few more models to see if I get different results.\nThe scatterplot from earlier makes me think there may be some possibility of groupings based on the predictor variables. If you look at the bottom part of the scatter plot, where the top three would be, there seem to be a possible difference in Age and Gender amongst who was in the top 3. this leads me to think a KNN model might be possible, though this will largely depend on if those differences amongst the top 3 are also apparent for those not in the top 3.\n\n\nK-Nearest Neighbors\n\nknnmodel &lt;- nearest_neighbor() %&gt;% \n  set_engine('kknn') %&gt;% \n  set_mode('classification')\n\n\nknnrecipe &lt;- simprecipe %&gt;% \n  step_normalize(Age)\n  \n\nset.seed(42)\n\nknn_wflow &lt;-\n  workflow() %&gt;% \n  add_model(knnmodel) %&gt;% \n  add_recipe(knnrecipe)\n\nknn_fit &lt;- knn_wflow %&gt;% fit(data = modeltrain)\nknnpreds_results &lt;- knn_fit %&gt;% extract_fit_parsnip()\nknnpreds_results\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(5,     data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.3283582\nBest kernel: optimal\nBest k: 5\n\n\n\ntestpreds &lt;- bind_cols(testpreds,\n  predict(knn_fit, modeltest),\n  predict(knn_fit, modeltest, type = 'prob')\n)\n\nnames(testpreds) &lt;- c('top3', 'logpreds', 'log0', 'log1', 'knnpreds', 'knn0', 'knn1')\n\nmodelperf &lt;- bind_cols(modelperf,\n                       bind_rows(\n                         accuracy(testpreds, top3, knnpreds, event_level = 'second'),\n                         precision(testpreds, top3, knnpreds, event_level = 'second'),\n                         recall(testpreds, top3, knnpreds, event_level = 'second'),\n                         specificity(testpreds, top3, knnpreds, event_level = 'second')\n                         )[,3]\n                       )\nnames(modelperf) &lt;- c('metric', 'estimator', 'log_reg', 'KNN')\nmodelperf\n\n# A tibble: 4 × 4\n  metric      estimator log_reg   KNN\n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 accuracy    binary      0.696 0.661\n2 precision   binary      0.364 0.308\n3 recall      binary      0.286 0.286\n4 specificity binary      0.833 0.786\n\n\n\nThe KNN results look remarkably similar to the logistic regression results, though it does in fact perform worse. The recall is, somewhat surprisingly, the same at 0.2857143, but all other model performance metrics are worse. From this we can tell that the model guessed the same proportion of contestants who actually did make the top 3, but incorrectly guessed contestants made the top 3 more times. The KNN model doesn’t give us information about model predictor importance or significance, so there’s less to say about this one, but ultimately it offers nothing to us that the logistic regression does not.\nFor the last model I want to try something with variable selection/ reduction. I think the abundance of location variables could have a negative impact on my results, so reducing variables should help that.\n\n\nMARS\n\nmarsmodel &lt;- mars(prod_degree = 2, prune_method = \"backward\") %&gt;% \n  set_engine('earth') %&gt;% \n  set_mode('classification') %&gt;% \n  translate()\n\n\nset.seed(42)\n\nmars_wflow &lt;-\n  workflow() %&gt;% \n  add_model(marsmodel) %&gt;% \n  add_recipe(simprecipe)  #MARS does not require much preprocessing beyond what was already done\n\nmars_fit &lt;- mars_wflow %&gt;% fit(data = modeltrain)\nmarspreds_results &lt;- mars_fit %&gt;% extract_fit_parsnip()\nmarspreds_results\n\nparsnip model object\n\nGLM (family binomial, link logit):\n nulldev  df       dev  df   devratio     AIC iters converged\n 151.794 133   146.219 132     0.0367   150.2    14         1\n\nEarth selected 2 of 45 terms, and 2 of 43 predictors\nTermination condition: GRSq -10 at 45 terms\nImportance: HomeState_North.Carolina, gender_Male, Age-unused, ...\nNumber of terms at each degree of interaction: 1 0 1\nEarth GCV 0.1907479    RSS 24.24242    GRSq 0.007605869    RSq 0.04456328\n\nmars_fit %&gt;% vip()\n\n\n\n\n\n\n\n\n\ntestpreds &lt;- bind_cols(testpreds,\n  predict(mars_fit, modeltest),\n  predict(mars_fit, modeltest, type = 'prob')\n)\n\nnames(testpreds) &lt;- c('top3', 'logpreds', 'log0', 'log1', 'knnpreds', 'knn0', 'knn1', 'marspreds', 'mars0', 'mars1')\n\nmodelperf &lt;- bind_cols(modelperf,\n                       bind_rows(\n                         accuracy(testpreds, top3, marspreds, event_level = 'second'),\n                         precision(testpreds, top3, marspreds, event_level = 'second'),\n                         recall(testpreds, top3, marspreds, event_level = 'second'),\n                         specificity(testpreds, top3, marspreds, event_level = 'second')\n                         )[,3]\n                       )\nnames(modelperf) &lt;- c('metric', 'estimator', 'log_reg', 'KNN', 'MARS')\nmodelperf\n\n# A tibble: 4 × 5\n  metric      estimator log_reg   KNN   MARS\n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 accuracy    binary      0.696 0.661 0.75  \n2 precision   binary      0.364 0.308 0.5   \n3 recall      binary      0.286 0.286 0.0714\n4 specificity binary      0.833 0.786 0.976 \n\n\n\nThe MARS models have the advantage of variable selection, and my thinking is if it removed several of the unimportant location variables but kept some that were informative we have seen a difference in model performance. However, the variable selection only chose 2 variables, HomeState_North.Carolina (dummy variable) and gender_Male (dummy variable, but for a binary variable, so essentially just gender). The Gender variable being selected is unsurprising considering what we saw earlier with the logistic regression, but the North Carolina variable is somewhat surprising. Though, looking back at the location bar chart from earlier the frequency that finalists that come from North Carolina place high is noteworthy, so it does make some sense.\nOur predictive power is ultimately worse than both of the previous models. Specificity, precision, and accuracy all went up, but at a huge cost to recall, which had a value of 0.0714286. This is because the Model only predicted 2 contestants to be in the top 3, one of which it admittedly got correct, but this was too conservative to be of any actual value. Given the variables selected, it likely assumes any finalist who is Male from North Carolina will make the top 3.\nUltimately the last two models were too complex to appropriately predict simple data like this one, which is likely why the simpler model performed the best. Still, the predictors were just not enough to make a worthwhile prediction, So including other predictors for future work could be useful. Possibly something like finalists who sing songs by certain artists, or even just lumping more of the location responses together so there is not so much noise caused by the large number of dummy variables. That said, while there may be some underlying relationship between age and gender, it is subtle enough to argue that the American Idol contestants likely stand a fair chance, regardless of their background."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "Antonio Flores contributed to this exercise.\nThe structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats.\nWarning: package 'knitr' was built under R version 4.3.3"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nIn addition to the standard variables of Height (in centimeters), Weight (in kilograms), and identified gender (Male, Female, Other), I have included twp additional variables: Generation and Salary. Generation represents the respondents age categorized into the self reported generation to which they belong, e.g. Gen Z are those with birth years between 1995 and 2012. Salary is the self reported annual salary in thousands of US dollars, e.g. a value of 70 would represent an annual salary of $70,000."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nfactor\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGeneration\n0\n1\nFALSE\n4\nGen: 5, Gen: 2, Bab: 1, Mil: 1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nSalary\n0\n1\nNA\nNA\nNA\n95.44444\n36.62005\n44\n70\n81\n133\n144\n▂▇▂▁▆"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(here) #to set paths\n\nhere() starts at C:/Users/Hylti/OneDrive/Documents/School/Practicum II/williamhyltin-P2-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                    `Allowed Values`     \n  &lt;chr&gt;           &lt;chr&gt;                                    &lt;chr&gt;                \n1 Height          height in centimeters                    numeric value &gt;0 or …\n2 Weight          weight in kilograms                      numeric value &gt;0 or …\n3 Gender          identified gender (male/female/other)    M/F/O/NA             \n4 Generation      Field representing a person's generation Gen Z/Millennial/Gen…\n5 Salary          Annual Salary in thousands of dollars    numeric value &gt;0 or …\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height     &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"15…\n$ Weight     &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender     &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\"…\n$ Generation &lt;chr&gt; \"Millennial\", \"Gen Z\", \"Baby Boomer\", \"Baby Boomer\", \"Baby …\n$ Salary     &lt;dbl&gt; 144, 77, 140, 68, 54, 133, 70, 44, 142, 77, 99, 144, 98, 81\n\nsummary(rawdata)\n\n    Height              Weight          Gender           Generation       \n Length:14          Min.   :  45.0   Length:14          Length:14         \n Class :character   1st Qu.:  55.0   Class :character   Class :character  \n Mode  :character   Median :  70.0   Mode  :character   Mode  :character  \n                    Mean   : 602.7                                        \n                    3rd Qu.:  90.0                                        \n                    Max.   :7000.0                                        \n                    NA's   :1                                             \n     Salary      \n Min.   : 44.00  \n 1st Qu.: 71.75  \n Median : 89.50  \n Mean   : 97.93  \n 3rd Qu.:138.25  \n Max.   :144.00  \n                 \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender Generation  Salary\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt;\n1 180        80 M      Millennial     144\n2 175        70 O      Gen Z           77\n3 sixty      60 F      Baby Boomer    140\n4 178        76 F      Baby Boomer     68\n5 192        90 NA     Baby Boomer     54\n6 6          55 F      Gen X          133\n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nGeneration\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70.0\n90.00\n7000\n▇▁▁▁▁\n\n\nSalary\n0\n1.00\n97.93\n36.02\n44\n71.75\n89.5\n138.25\n144\n▃▇▃▁▇\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nGeneration\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nSalary\n0\n1.00\n94.69\n35.31\n44\n70.00\n81\n133\n144\n▃▇▃▁▆\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nGeneration\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nSalary\n0\n1.00\n94.69\n35.31\n44\n70.00\n81\n133\n144\n▃▇▃▁▆\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nGeneration\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nSalary\n0\n1\n95.91\n38.22\n44\n69.0\n81\n137.5\n144\n▃▇▂▁▇\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\nWe will also change ‘Generation’ to a factor variable\n\nd3$Gender &lt;- as.factor(d3$Gender)\nd3$Generation &lt;- as.factor(d3$Generation)\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\nGeneration\n0\n1\nFALSE\n4\nGen: 6, Bab: 2, Gen: 2, Mil: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nSalary\n0\n1\n95.91\n38.22\n44\n69.0\n81\n137.5\n144\n▃▇▂▁▇\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\nGeneration\n0\n1\nFALSE\n4\nGen: 5, Gen: 2, Bab: 1, Mil: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nSalary\n0\n1\n95.44\n36.62\n44\n70\n81\n133\n144\n▂▇▂▁▆\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at C:/Users/Hylti/OneDrive/Documents/School/Practicum II/williamhyltin-P2-portfolio\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "For this exercise we are tasked with recreating a high-quality publication level chart found online using R and various AI tools. I have chosen to recreate the area chart on this page. Since this is coming from FiveThirtyEight and the original plot is interactive I have decided to recreate the plot with Highcharter.\nFirst we are tasked with requesting AI (in this case I have chosen to use ChatGPT) to attempt to recreate the plot, which I do with my initial prompt here:\n  Can you provide code using Highcharter in R that would recreate the area plot titled “Baby boomers are the biggest generation in Congress today” on this page https://fivethirtyeight.com/features/aging-congress-boomers/\n  raw data can be found here: https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\nThe first round of code had a handful of problems, primarily with processing the data, but it came pretty close in terms of structuring the overall request. I provided the column names in the raw data to help with processing, as well as correcting things like unnecessary filters. One of the outputs from ChatGPT ended up with pivoted data, which was not as easy to use for what we were doing, but the unpivoted data was still pretty close so I modified it myself and fed that back as a prompt.\nI’m still having problems with the highcharter section of code. I’ve modified the processing code slightly to adjust the pct values as well, because it did not look like it was previously giving the correct values for pct. I’ve also created a year variable to match what is done in the chart. Here is the processing code after my modifications: data1 &lt;- data %&gt;% mutate(year = as.integer(str_sub(start_date, 1, 4))) %&gt;% group_by(year, generation) %&gt;% summarise(count = n()) %&gt;% ungroup() %&gt;% group_by(year) %&gt;% mutate(pct = count / sum(count) * 100) Can you utilize this dataframe without pivoting to create the chart in Highcharter?\nThe output from this prompt got me most of the way there. The code runs, but some of the generations were out of order, and there were several tweaks needed to get the chart to look like the one on the page. Still the bones were there, so I took over from this point on, aside from some one off prompts to get exactly what I was looking for, with an example below.\nCan you adjust the tooltip in the chart to have the following format: Generation Year Percent (in percent format 00.0%) So for example: Missionary 1937 44.9%\nThe code and output below is the final product after the combined efforts of ChatGPT and myself.\n\n# Load libraries\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(highcharter)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n# Read the data from the URL\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\"\ndata &lt;- read_csv(url)\n\nRows: 29120 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): chamber, state_abbrev, bioname, bioguide_id, generation\ndbl  (6): congress, party_code, cmltv_cong, cmltv_chamber, age_days, age_years\ndate (2): start_date, birthday\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Calculate percentage of each generation in each congress\ndata1 &lt;- data %&gt;%\n  mutate(year = as.integer(str_sub(start_date, 1, 4))) %&gt;% \n  group_by(year, generation) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(year) %&gt;% \n  mutate(pct = count / sum(count) * 100)\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\ngen_order &lt;- data %&gt;% mutate(byear = as.integer(str_sub(birthday, 1, 4))) %&gt;% group_by(generation) %&gt;% summarize(oldyr = min(byear)) %&gt;% arrange(by = oldyr) %&gt;% select(generation)\n\n\ndata1$generation &lt;- fct_rev(fct_relevel(data1$generation, as.vector(gen_order)))\n\nWarning: Outer names are only allowed for unnamed scalar atomic inputs\n\n\n\ngeneration_colors &lt;- c(\n  \"#D26E8C\",\n  \"#92DCE0\",\n  \"#A593E9\",\n  \"#FD867E\",\n  \"#FDE384\",\n  \"#86D09D\",\n  \"#8B8887\",\n  \"#E98CCA\",\n  \"#FFE7E5\",\n  \"#E9E9E9\"\n)\n\n\nhighchart(type = \"chart\") %&gt;%\n  hc_chart(type = \"area\") %&gt;%\n  hc_title(text = \"&lt;b&gt;Baby boomers are the biggest generation in Congress today&lt;/b&gt;\",\n           align = 'left',\n           style = list(fontSize = \"24px\",\n                        fontFamily = 'Calibri')) %&gt;%\n  hc_subtitle(text = 'Share of members in Congress from each generation, 1919 to 2023',\n              align = 'left',\n              style = list(fontSize = \"20px\",\n                           fontFamily = 'Calibri')) %&gt;% \n  hc_xAxis(gridLineWidth = 0,\n    labels = list(\n    style = list(fontSize = \"16px\")\n    )\n    ) %&gt;%\n  hc_yAxis(max = 100,\n           labels = list(\n      formatter = JS(\"function() {\n                      if (this.value == 100) {\n                        return this.value + '%';\n                      } else {\n                        return this.value;\n                      }\n                    }\"),\n      style = list(fontSize = \"16px\")\n           )\n      ) %&gt;%\n  hc_add_series(data1, \"area\", hcaes(x = year, y = pct, group = generation),\n                color = generation_colors) %&gt;%\n  hc_legend(align = \"left\",\n            verticalAlign = \"top\",\n            layout = \"horizontal\",\n            itemStyle = list(fontFamily = 'Copperplate Gothic Light', fontSize = \"12px\")) %&gt;%\n  hc_tooltip(\n    headerFormat = \"&lt;b&gt;{series.name}&lt;/b&gt;&lt;br/&gt;\",\n    pointFormat = \"{point.x}&lt;br/&gt;{point.y:.1f}%\",\n    crosshairs = TRUE\n  ) %&gt;%\n  hc_plotOptions(\n    area = list(\n      stacking = \"normal\",\n      marker = list(enabled = FALSE,\n                    states = list(\n                      hover = list(\n                        enabled = FALSE\n                        )\n                      )\n                    )  # Disable markers\n      ),\n    series = list(\n      lineColor = '#EDF6E9',\n      trackByArea = TRUE,\n      stickyTracking = FALSE\n    )\n  ) %&gt;% hc_add_theme(hc_theme_538())\n\n\n\n\n\nI believe my output is pretty close to the original, with the exception of a few things like fonts, and the hover animations. I could not figure out how to get the crosshair for the tooltip to stay inside of the relevant generation group, nor could I figure out how to change the shape of the color indicators in the legend from circular to square, but even without those changes it still like it was high quality and close enough."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#publication-quality-chart",
    "href": "presentation-exercise/presentation-exercise.html#publication-quality-chart",
    "title": "Presentation Exercise",
    "section": "",
    "text": "For this exercise we are tasked with recreating a high-quality publication level chart found online using R and various AI tools. I have chosen to recreate the area chart on this page. Since this is coming from FiveThirtyEight and the original plot is interactive I have decided to recreate the plot with Highcharter.\nFirst we are tasked with requesting AI (in this case I have chosen to use ChatGPT) to attempt to recreate the plot, which I do with my initial prompt here:\n  Can you provide code using Highcharter in R that would recreate the area plot titled “Baby boomers are the biggest generation in Congress today” on this page https://fivethirtyeight.com/features/aging-congress-boomers/\n  raw data can be found here: https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\nThe first round of code had a handful of problems, primarily with processing the data, but it came pretty close in terms of structuring the overall request. I provided the column names in the raw data to help with processing, as well as correcting things like unnecessary filters. One of the outputs from ChatGPT ended up with pivoted data, which was not as easy to use for what we were doing, but the unpivoted data was still pretty close so I modified it myself and fed that back as a prompt.\nI’m still having problems with the highcharter section of code. I’ve modified the processing code slightly to adjust the pct values as well, because it did not look like it was previously giving the correct values for pct. I’ve also created a year variable to match what is done in the chart. Here is the processing code after my modifications: data1 &lt;- data %&gt;% mutate(year = as.integer(str_sub(start_date, 1, 4))) %&gt;% group_by(year, generation) %&gt;% summarise(count = n()) %&gt;% ungroup() %&gt;% group_by(year) %&gt;% mutate(pct = count / sum(count) * 100) Can you utilize this dataframe without pivoting to create the chart in Highcharter?\nThe output from this prompt got me most of the way there. The code runs, but some of the generations were out of order, and there were several tweaks needed to get the chart to look like the one on the page. Still the bones were there, so I took over from this point on, aside from some one off prompts to get exactly what I was looking for, with an example below.\nCan you adjust the tooltip in the chart to have the following format: Generation Year Percent (in percent format 00.0%) So for example: Missionary 1937 44.9%\nThe code and output below is the final product after the combined efforts of ChatGPT and myself.\n\n# Load libraries\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(highcharter)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n# Read the data from the URL\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\"\ndata &lt;- read_csv(url)\n\nRows: 29120 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): chamber, state_abbrev, bioname, bioguide_id, generation\ndbl  (6): congress, party_code, cmltv_cong, cmltv_chamber, age_days, age_years\ndate (2): start_date, birthday\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Calculate percentage of each generation in each congress\ndata1 &lt;- data %&gt;%\n  mutate(year = as.integer(str_sub(start_date, 1, 4))) %&gt;% \n  group_by(year, generation) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(year) %&gt;% \n  mutate(pct = count / sum(count) * 100)\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\ngen_order &lt;- data %&gt;% mutate(byear = as.integer(str_sub(birthday, 1, 4))) %&gt;% group_by(generation) %&gt;% summarize(oldyr = min(byear)) %&gt;% arrange(by = oldyr) %&gt;% select(generation)\n\n\ndata1$generation &lt;- fct_rev(fct_relevel(data1$generation, as.vector(gen_order)))\n\nWarning: Outer names are only allowed for unnamed scalar atomic inputs\n\n\n\ngeneration_colors &lt;- c(\n  \"#D26E8C\",\n  \"#92DCE0\",\n  \"#A593E9\",\n  \"#FD867E\",\n  \"#FDE384\",\n  \"#86D09D\",\n  \"#8B8887\",\n  \"#E98CCA\",\n  \"#FFE7E5\",\n  \"#E9E9E9\"\n)\n\n\nhighchart(type = \"chart\") %&gt;%\n  hc_chart(type = \"area\") %&gt;%\n  hc_title(text = \"&lt;b&gt;Baby boomers are the biggest generation in Congress today&lt;/b&gt;\",\n           align = 'left',\n           style = list(fontSize = \"24px\",\n                        fontFamily = 'Calibri')) %&gt;%\n  hc_subtitle(text = 'Share of members in Congress from each generation, 1919 to 2023',\n              align = 'left',\n              style = list(fontSize = \"20px\",\n                           fontFamily = 'Calibri')) %&gt;% \n  hc_xAxis(gridLineWidth = 0,\n    labels = list(\n    style = list(fontSize = \"16px\")\n    )\n    ) %&gt;%\n  hc_yAxis(max = 100,\n           labels = list(\n      formatter = JS(\"function() {\n                      if (this.value == 100) {\n                        return this.value + '%';\n                      } else {\n                        return this.value;\n                      }\n                    }\"),\n      style = list(fontSize = \"16px\")\n           )\n      ) %&gt;%\n  hc_add_series(data1, \"area\", hcaes(x = year, y = pct, group = generation),\n                color = generation_colors) %&gt;%\n  hc_legend(align = \"left\",\n            verticalAlign = \"top\",\n            layout = \"horizontal\",\n            itemStyle = list(fontFamily = 'Copperplate Gothic Light', fontSize = \"12px\")) %&gt;%\n  hc_tooltip(\n    headerFormat = \"&lt;b&gt;{series.name}&lt;/b&gt;&lt;br/&gt;\",\n    pointFormat = \"{point.x}&lt;br/&gt;{point.y:.1f}%\",\n    crosshairs = TRUE\n  ) %&gt;%\n  hc_plotOptions(\n    area = list(\n      stacking = \"normal\",\n      marker = list(enabled = FALSE,\n                    states = list(\n                      hover = list(\n                        enabled = FALSE\n                        )\n                      )\n                    )  # Disable markers\n      ),\n    series = list(\n      lineColor = '#EDF6E9',\n      trackByArea = TRUE,\n      stickyTracking = FALSE\n    )\n  ) %&gt;% hc_add_theme(hc_theme_538())\n\n\n\n\n\nI believe my output is pretty close to the original, with the exception of a few things like fonts, and the hover animations. I could not figure out how to get the crosshair for the tooltip to stay inside of the relevant generation group, nor could I figure out how to change the shape of the color indicators in the legend from circular to square, but even without those changes it still like it was high quality and close enough."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#publication-quality-tables",
    "href": "presentation-exercise/presentation-exercise.html#publication-quality-tables",
    "title": "Presentation Exercise",
    "section": "Publication Quality Tables",
    "text": "Publication Quality Tables\nUsing the same dataset, I wanted create a quality table that would show what the congressional membership distribution but include things like the mean age and count of members. The chart above does a great job of showing the relative distribution and the change over time, but the actual age is still an important factor. For example, we may have a lot of Congress members from the Baby Boomer generation in today’s Congress, but do they themselves skew younger or older?\nI decided I would use GT tables. I can’t say i have any experience with gt tables, so I thought this might be a challenge, but I was intrigued by the ability to use ggplots as images in the tables themself. I started off with ChatGPT again with the following prompt, continued off the prompts from teh Highcharter exercise:\n  Moving on to a new task with the same raw dataset. Can you create a publication quality table using the gt package in R with the following columns:\n  Generation, Mean Age, Count of Members, and an in-table distribution plot\n  And have the rows grouped by year, similar to a pivot table?\nThe output got some of the overall structure but was very lacking on the details, which again proved challenging due to my unfamiliarity with gt tables. For example, it knew I would need to create a way to call the plots formulaically, however it used the base histogram function instead of ggplot, which from what I can tell would not work with gt tables or at least not in an intuitive way. it also seemed to struggle with how to actually add the distribution plot in to the table, the code it generated would have added the plots after creating the tables initially, which is fine but it used the wrong functions to do so repeatedly. After trading prompts a number of times and getting mostly nowhere I decided to take matters into my own hands and researched how to use gt tables via the provided tutorial, which helped tremendously.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.3.3\n\n# Read the data from the URL\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\"\ndata &lt;- read_csv(url)\n\nRows: 29120 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): chamber, state_abbrev, bioname, bioguide_id, generation\ndbl  (6): congress, party_code, cmltv_cong, cmltv_chamber, age_days, age_years\ndate (2): start_date, birthday\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndata2 &lt;- data %&gt;%\n  select(start_date, generation, age_years)\n\n# Extract year from start_date\ndata2$year &lt;- as.integer(substr(data2$start_date, 1, 4))\n\ndata2 &lt;- data2 %&gt;% arrange(by = -year)\n\ngen_order &lt;- data %&gt;% mutate(byear = as.integer(str_sub(birthday, 1, 4))) %&gt;% group_by(generation) %&gt;% summarize(oldyr = min(byear)) %&gt;% arrange(by = oldyr) %&gt;% select(generation)\n\ndata2$generation &lt;- fct_rev(fct_relevel(data2$generation, as.vector(gen_order)))\n\nWarning: Outer names are only allowed for unnamed scalar atomic inputs\n\n# Group by year and generation, calculate mean age and count of members\ndata_summary &lt;- data2 %&gt;%\n  group_by(year, generation) %&gt;%\n  summarise(mean_age = mean(age_years, na.rm = TRUE),\n            count_members = n()) %&gt;%\n  ungroup() %&gt;%\n  filter(!is.na(mean_age)) %&gt;% arrange(by = -year) # Remove rows with NA mean_age for clarity\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\ndist_plot &lt;- function(fun_gen, fun_yr) {\n  full_range &lt;- data2 %&gt;% \n  pull(age_years) %&gt;% \n  range()\n  \n  data2 %&gt;% \n    filter(generation == !!fun_gen, year == !!fun_yr) %&gt;% \n    ggplot() +\n    geom_violin(aes(x=age_years, y = generation), fill = 'black') +\n    theme_minimal() +\n    scale_y_discrete(breaks = NULL) +\n    scale_x_continuous(breaks = NULL) +\n    labs(x = element_blank(), y = element_blank()) +\n    coord_cartesian(xlim = full_range)\n}\ndist_plot('Boomers', 2021) #testing function and plot appearance\n\n\n\n\n\n\n\n\n\n# Initialize the GT table\ngt_table &lt;- data_summary %&gt;%\n  group_by(generation, year) %&gt;% \n  mutate(Distribution = list(c(as.character(generation), year))) %&gt;%\n  ungroup() %&gt;% \n  gt(groupname_col = 'year', rowname_col = 'generation') %&gt;%\n  tab_header(\n    title = md(\"**Generation and Congress Membership Statistics**\"),\n    subtitle = \"Mean Age, Count of Members, and Distribution Plot by Year\"\n  ) %&gt;%\n  cols_label(\n    generation = md(\"**Generation**\"),\n    mean_age = md(\"**Mean Age**\"),\n    count_members = md(\"**Count**\"),\n    Distribution = md('**Distribution**')\n  ) %&gt;%\n  fmt_number(columns = c(mean_age), decimals = 0) %&gt;% \n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = cells_row_groups()\n  ) %&gt;% \n  tab_style(\n    style = cell_text(align = 'center'),\n    locations = cells_column_labels()\n  ) %&gt;% \n  text_transform(\n    locations = cells_body(columns = 'Distribution'),\n    fn = function(column) {\n      map(column, ~str_split_1(., ', ')) %&gt;% \n        map(~dist_plot(.[1], .[2])) %&gt;% \n        ggplot_image(height = px(30), aspect_ratio =  3)\n    }\n  ) %&gt;%\n  tab_footnote(\n    \"Note: Mean Age is rounded to nearest whole year. Distribution plots represent age distributions within each generation.\"\n  ) %&gt;% \n  tab_options(\n    data_row.padding = px(1),\n    row_group.padding = px(4)\n  )\ngt_table %&gt;% \n  opt_stylize(\n    style = 5, color = 'blue'\n    )\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes.\n\n\nWarning in max(data$density, na.rm = TRUE): no non-missing arguments to max;\nreturning -Inf\n\n\nWarning: Computation failed in `stat_ydensity()`.\nCaused by error in `$&lt;-.data.frame`:\n! replacement has 1 row, data has 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneration and Congress Membership Statistics\n\n\nMean Age, Count of Members, and Distribution Plot by Year\n\n\n\nMean Age\nCount\nDistribution\n\n\n\n\n2023\n\n\nGen Z\n26\n1\n\n\n\nMillennial\n38\n55\n\n\n\nGen X\n50\n192\n\n\n\nBoomers\n67\n259\n\n\n\nSilent\n81\n29\n\n\n\n2021\n\n\nMillennial\n36\n37\n\n\n\nGen X\n49\n175\n\n\n\nBoomers\n65\n301\n\n\n\nSilent\n80\n38\n\n\n\n2019\n\n\nMillennial\n35\n26\n\n\n\nGen X\n47\n162\n\n\n\nBoomers\n63\n303\n\n\n\nSilent\n78\n53\n\n\n\n2017\n\n\nMillennial\n34\n6\n\n\n\nGen X\n45\n136\n\n\n\nBoomers\n61\n351\n\n\n\nSilent\n77\n62\n\n\n\n2015\n\n\nMillennial\n32\n4\n\n\n\nGen X\n44\n122\n\n\n\nBoomers\n60\n340\n\n\n\nSilent\n75\n75\n\n\n\n2013\n\n\nMillennial\n31\n3\n\n\n\nGen X\n42\n101\n\n\n\nBoomers\n58\n346\n\n\n\nSilent\n73\n95\n\n\n\nGreatest\n88\n3\n\n\n\n2011\n\n\nMillennial\n30\n1\n\n\n\nGen X\n41\n80\n\n\n\nBoomers\n56\n336\n\n\n\nSilent\n71\n123\n\n\n\nGreatest\n86\n6\n\n\n\n2009\n\n\nMillennial\n28\n1\n\n\n\nGen X\n39\n55\n\n\n\nBoomers\n55\n330\n\n\n\nSilent\n69\n160\n\n\n\nGreatest\n85\n7\n\n\n\n2007\n\n\nGen X\n38\n35\n\n\n\nBoomers\n53\n328\n\n\n\nSilent\n67\n176\n\n\n\nGreatest\n83\n10\n\n\n\n2005\n\n\nGen X\n36\n25\n\n\n\nBoomers\n52\n305\n\n\n\nSilent\n65\n199\n\n\n\nGreatest\n81\n11\n\n\n\n2003\n\n\nGen X\n35\n19\n\n\n\nBoomers\n50\n289\n\n\n\nSilent\n63\n217\n\n\n\nGreatest\n78\n14\n\n\n\n2001\n\n\nGen X\n33\n12\n\n\n\nBoomers\n48\n274\n\n\n\nSilent\n62\n239\n\n\n\nGreatest\n77\n22\n\n\n\n1999\n\n\nGen X\n32\n7\n\n\n\nBoomers\n46\n251\n\n\n\nSilent\n60\n252\n\n\n\nGreatest\n75\n29\n\n\n\n1997\n\n\nGen X\n30\n6\n\n\n\nBoomers\n45\n235\n\n\n\nSilent\n58\n268\n\n\n\nGreatest\n73\n35\n\n\n\n1995\n\n\nGen X\n29\n3\n\n\n\nBoomers\n43\n207\n\n\n\nSilent\n57\n285\n\n\n\nGreatest\n72\n48\n\n\n\n1993\n\n\nBoomers\n42\n164\n\n\n\nSilent\n55\n314\n\n\n\nGreatest\n70\n68\n\n\n\n1991\n\n\nBoomers\n41\n113\n\n\n\nSilent\n54\n331\n\n\n\nGreatest\n69\n104\n\n\n\n1989\n\n\nBoomers\n39\n95\n\n\n\nSilent\n52\n338\n\n\n\nGreatest\n67\n111\n\n\n\nLost\n88\n1\n\n\n\n1987\n\n\nBoomers\n37\n83\n\n\n\nSilent\n50\n330\n\n\n\nGreatest\n66\n130\n\n\n\nLost\n86\n1\n\n\n\n1985\n\n\nBoomers\n36\n67\n\n\n\nSilent\n48\n318\n\n\n\nGreatest\n64\n155\n\n\n\nLost\n84\n1\n\n\n\n1983\n\n\nBoomers\n34\n59\n\n\n\nSilent\n46\n308\n\n\n\nGreatest\n62\n176\n\n\n\nLost\n82\n1\n\n\n\n1981\n\n\nBoomers\n32\n40\n\n\n\nSilent\n45\n305\n\n\n\nGreatest\n60\n199\n\n\n\nLost\n80\n1\n\n\n\n1979\n\n\nBoomers\n31\n18\n\n\n\nSilent\n43\n277\n\n\n\nGreatest\n59\n247\n\n\n\nLost\n80\n2\n\n\n\n1977\n\n\nBoomers\n29\n10\n\n\n\nSilent\n42\n236\n\n\n\nGreatest\n58\n298\n\n\n\nLost\n78\n7\n\n\n\n1975\n\n\nBoomers\n27\n4\n\n\n\nSilent\n40\n189\n\n\n\nGreatest\n57\n343\n\n\n\nLost\n77\n15\n\n\n\n1973\n\n\nSilent\n40\n129\n\n\n\nGreatest\n55\n398\n\n\n\nLost\n75\n21\n\n\n\n1971\n\n\nSilent\n39\n92\n\n\n\nGreatest\n54\n422\n\n\n\nLost\n74\n34\n\n\n\n1969\n\n\nSilent\n37\n65\n\n\n\nGreatest\n53\n441\n\n\n\nLost\n73\n45\n\n\n\n1967\n\n\nSilent\n36\n45\n\n\n\nGreatest\n51\n436\n\n\n\nLost\n71\n60\n\n\n\nMissionary\n87\n2\n\n\n\n1965\n\n\nSilent\n34\n30\n\n\n\nGreatest\n50\n437\n\n\n\nLost\n69\n79\n\n\n\nMissionary\n85\n2\n\n\n\n1963\n\n\nSilent\n32\n17\n\n\n\nGreatest\n49\n415\n\n\n\nLost\n67\n115\n\n\n\nMissionary\n84\n4\n\n\n\n1961\n\n\nSilent\n32\n5\n\n\n\nGreatest\n48\n390\n\n\n\nLost\n66\n154\n\n\n\nMissionary\n82\n9\n\n\n\n1959\n\n\nSilent\n31\n1\n\n\n\nGreatest\n47\n368\n\n\n\nLost\n64\n169\n\n\n\nMissionary\n81\n14\n\n\n\n1957\n\n\nGreatest\n46\n305\n\n\n\nLost\n63\n215\n\n\n\nMissionary\n79\n25\n\n\n\n1955\n\n\nGreatest\n45\n278\n\n\n\nLost\n61\n235\n\n\n\nMissionary\n76\n27\n\n\n\n1953\n\n\nGreatest\n44\n251\n\n\n\nLost\n59\n263\n\n\n\nMissionary\n75\n38\n\n\n\n1951\n\n\nGreatest\n43\n220\n\n\n\nLost\n57\n283\n\n\n\nMissionary\n74\n50\n\n\n\n1949\n\n\nGreatest\n41\n198\n\n\n\nLost\n56\n289\n\n\n\nMissionary\n72\n66\n\n\n\n1947\n\n\nGreatest\n40\n167\n\n\n\nLost\n54\n297\n\n\n\nMissionary\n70\n89\n\n\n\n1945\n\n\nGreatest\n40\n120\n\n\n\nLost\n53\n316\n\n\n\nMissionary\n68\n127\n\n\n\nProgressive\n87\n1\n\n\n\n1943\n\n\nGreatest\n38\n92\n\n\n\nLost\n51\n306\n\n\n\nMissionary\n67\n155\n\n\n\nProgressive\n85\n1\n\n\n\n1941\n\n\nGreatest\n36\n74\n\n\n\nLost\n49\n311\n\n\n\nMissionary\n65\n173\n\n\n\nProgressive\n84\n3\n\n\n\n1939\n\n\nGreatest\n34\n54\n\n\n\nLost\n47\n293\n\n\n\nMissionary\n64\n211\n\n\n\nProgressive\n80\n3\n\n\n\n1937\n\n\nGreatest\n33\n32\n\n\n\nLost\n46\n271\n\n\n\nMissionary\n62\n249\n\n\n\nProgressive\n79\n2\n\n\n\n1935\n\n\nGreatest\n31\n21\n\n\n\nLost\n44\n239\n\n\n\nMissionary\n61\n284\n\n\n\nProgressive\n77\n5\n\n\n\n1933\n\n\nGreatest\n30\n9\n\n\n\nLost\n43\n198\n\n\n\nMissionary\n60\n329\n\n\n\nProgressive\n76\n11\n\n\n\n1931\n\n\nGreatest\n27\n2\n\n\n\nLost\n41\n146\n\n\n\nMissionary\n58\n396\n\n\n\nProgressive\n74\n19\n\n\n\n1929\n\n\nGreatest\n27\n1\n\n\n\nLost\n40\n107\n\n\n\nMissionary\n57\n424\n\n\n\nProgressive\n73\n34\n\n\n\nGilded\n88\n1\n\n\n\n1927\n\n\nLost\n39\n89\n\n\n\nMissionary\n55\n414\n\n\n\nProgressive\n71\n47\n\n\n\nGilded\n86\n1\n\n\n\n1925\n\n\nLost\n37\n69\n\n\n\nMissionary\n54\n419\n\n\n\nProgressive\n69\n59\n\n\n\nGilded\n84\n1\n\n\n\n1923\n\n\nLost\n36\n49\n\n\n\nMissionary\n52\n431\n\n\n\nProgressive\n68\n74\n\n\n\nGilded\n84\n3\n\n\n\n1921\n\n\nLost\n36\n31\n\n\n\nMissionary\n50\n418\n\n\n\nProgressive\n66\n107\n\n\n\nGilded\n83\n4\n\n\n\n1919\n\n\nLost\n33\n19\n\n\n\nMissionary\n49\n412\n\n\n\nProgressive\n65\n119\n\n\n\nGilded\n80\n5\n\n\n\n\nNote: Mean Age is rounded to nearest whole year. Distribution plots represent age distributions within each generation.\n\n\n\n\n\n\n\n\n\nI’m fairly satisfied with the output. The inline violin charts were especially satisfying to make, and I’m happy with the availability of all the different customization options, while simultaneously there are several quick theme options to make styling even easier. That said, the syntax is less intuitive than I would prefer, I was truly hoping for a more ggplot-like experience."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "",
    "text": "Code\nd1 &lt;- readRDS(here('data','processed-data','processed-crime.rds'))\ntig_zips &lt;- zctas(cb=TRUE, starts_with = c(unique(d1$Zip.Code)), year = 2020, progress_bar = FALSE)\n\ncatYearly &lt;- d1 %&gt;% mutate(\n  years = year(Occurred.Date)\n  #years = as.character(years)\n  ) %&gt;% filter(years != 2024) %&gt;% \n  group_by(Crime.Category, years) %&gt;% \n  summarize(crim.count  = n()) %&gt;% \n  ungroup() %&gt;% \n  as.data.frame()\n\nallYrsCats &lt;- expand.grid(years = unique(catYearly$years), Crime.Category = unique(catYearly$Crime.Category))\n\nyearlyAllCats &lt;- left_join(allYrsCats, catYearly, by = join_by(Crime.Category, years))  %&gt;% \n  mutate(crim.count = ifelse(is.na(crim.count), 0, crim.count))\n\nfig &lt;- plot_ly(yearlyAllCats,\n  x = ~crim.count,\n  y = ~fct_reorder(Crime.Category, crim.count),\n  type = 'bar',\n  showlegend = F,\n  frame = ~years,\n  marker = list(color = '#000067')\n)\n\ncatbarly &lt;- fig %&gt;% \n  layout(\n    yaxis = list(\n      title = '', tickangle = -30, tickfont = list(size = 8)\n      ), \n    xaxis = list(\n      title = 'Count of Occurence' \n    ),\n    title = list(\n      text = 'Yearly Occurence of Crime by Category',\n      y = 0.99, x = 0.1, xanchor = 'left', yanchor = 'top',\n      font = list(\n        size = 18\n      )\n      )\n    )\n\nyearlyCrime &lt;- d1 %&gt;% mutate(\n  Zip.Char = as.character(Zip.Code),\n  years = year(Occurred.Date),\n  years = as.character(years)\n  ) %&gt;% filter(years != 2024) %&gt;% \n  group_by(Zip.Char, years) %&gt;% \n  summarize(crim.count  = n()) %&gt;% \n  ungroup() %&gt;% \n  as.data.frame()\n  #filter(count &gt;= 500) %&gt;%\n  #filter(Zip.Char == '78741') %&gt;%\n  \n\nallYrsZips &lt;- expand.grid(years = unique(yearlyCrime$years), Zip.Char = unique(yearlyCrime$Zip.Char))\n\nyearlyCrimeAll &lt;- left_join(allYrsZips, yearlyCrime, by = join_by(Zip.Char, years))  %&gt;% \n  inner_join(tig_zips, by = join_by(Zip.Char == ZCTA5CE20)) %&gt;% \n  mutate(crim.count = ifelse(is.na(crim.count), 0, crim.count))\n  \ncpeth &lt;- yearlyCrimeAll %&gt;% filter(crim.count &gt;= 50) %&gt;% \n  ggplot() +\n  geom_sf(aes(geometry = geometry, fill = crim.count, frame = years, text = paste(Zip.Char, '&lt;br&gt;', crim.count))) +\n  scale_fill_gradient2(low = \"#E0144C\", mid = '#FFFFFF', high = \"#000067\", midpoint = -5000, \n                       trans = 'reverse') +\n  theme_classic() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.text.y=element_blank(),\n        line = element_blank(),\n        plot.title = element_text(hjust = 0, size = 15)) +\n  labs(fill = 'Crime Count',\n       title = 'Crime by Location Over Time')\n\ncplotly &lt;- cpeth %&gt;% ggplotly() %&gt;% \n  layout(hoverlabel = text) %&gt;% \n  animation_opts(1500, 1) %&gt;% \n  animation_slider(currentvalue = list(visible = FALSE)) %&gt;% \n  animation_button(x = 0, xanchor = \"left\", y = 0, yanchor = \"bottom\")\n\nRecatTable &lt;- d1 %&gt;% \n  filter(year(Occurred.Date) &lt; 2024) %&gt;% \n  mutate(\n  Occur.Years = trunc.Date(Occurred.Date, 'years')\n  ) %&gt;% \n  group_by(Crime.Category, Highest.Offense.Description, Occur.Years) %&gt;% \n  summarize(\n    Count = n(),\n    .groups = 'drop'\n  ) %&gt;% \n  arrange(by = Occur.Years) %&gt;%\n  group_by(Crime.Category, Highest.Offense.Description) %&gt;% \n  summarize(\n    mean = mean(Count),\n    sd = sd(Count),\n    sum = sum(Count),\n    .groups = 'drop'\n  ) %&gt;% \n  gt() %&gt;% \n  tab_header(\n    title = md('**Crime Recategorization Key**')\n    ) %&gt;% \n  cols_label(\n    Crime.Category = md('**New Category**'),\n    Highest.Offense.Description = md('**Original Description**'),\n    mean = md('**Mean Occurrence Across Years**'),\n    sd = md('**Standard Deviation of Yearly Occurrence**'),\n    sum = md('**Total Occurrences**')\n  ) %&gt;% \n  fmt_number(columns = mean:sd, decimals = 1) %&gt;% \n  opt_interactive()"
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#general-background-information",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#general-background-information",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "General Background Information",
    "text": "General Background Information\nLast year, Austin was ranked among the top US cities with a problem in homicide rate [1]. Earlier this year, the claim was made that crime has dropped to a new low since 2020 [2]. The intention of this analysis will be to investigate both claims, and gain a better understanding of Austin’s criminal activity. While these claims will be in mind throughout the steps of the analysis, I intend to approach it from more of an exploratory standpoint, so not to only go out and investigate the claims but really any insights that can be gleaned around Austin criminal activity and how it has changed over time. What the analysis will not cover would be things like causes of the changes in crime, at least not directly. I may identify shifts that could inform the cause, but the goal will not be to drill down into every potential socioeconomic factor and pick the primary drivers, instead simply to observe what crime in the city of Austin looks like and what we might be able to expect going forward. Aside from resources outlined in our course resources, I will also draw from this book [3]. It’s not one I have used before but it had been recommended to me previously and I would like to learn more about time series and forecasting in general."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#description-of-data-and-data-source",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#description-of-data-and-data-source",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Description of data and data source",
    "text": "Description of data and data source\nMy primary data source will come from here. The data is updated weekly by the Austin Police Department, and each record in the dataset represents an Incident Report, with the highest offense within an incident taking precedence in things like the description and categorization Each Incident can have other offense tied to it, however since each record is a unique incident then only the aforementioned Higehst Offense is the one represented (NOTE: At the time of this writing, 7/31/2024, the dataset has been taken down to be replaced with one that aligns more closely with the FBI National Incident Based Reporting System. The datasets are not one-to-one, so reproducibility would require more than a lift and shift, however some of the methods could still be employed).\nThe raw data is represented by several categorical, location, and time-based variables, many of which have missing values, or are formatted incorrectly for the data type, so they will need to be cleaned or recoded. After cleaning the data (done in a separate file found in this repo) we can take a look a more meaningful look.\n\n\nCode\nskim(d1)\n\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n2461621\n\n\nNumber of columns\n28\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n12\n\n\nDate\n3\n\n\ndifftime\n2\n\n\nnumeric\n9\n\n\nPOSIXct\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHighest.Offense.Description\n0\n1\n3\n48\n0\n436\n0\n\n\nFamily.Violence\n0\n1\n1\n1\n0\n2\n0\n\n\nLocation.Type\n0\n1\n7\n47\n0\n47\n0\n\n\nAddress\n0\n1\n8\n74\n0\n246951\n0\n\n\nAPD.Sector\n0\n1\n2\n5\n0\n14\n0\n\n\nAPD.District\n0\n1\n1\n2\n0\n21\n0\n\n\nPRA\n0\n1\n1\n4\n0\n742\n0\n\n\nClearance.Status\n0\n1\n0\n1\n615856\n4\n0\n\n\nUCR.Category\n0\n1\n0\n3\n1550375\n17\n0\n\n\nCategory.Description\n0\n1\n0\n18\n1550375\n8\n0\n\n\nLocation\n0\n1\n0\n27\n32335\n219842\n0\n\n\nCrime.Category\n0\n1\n4\n29\n0\n33\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Date\n0\n1.00\n2003-01-01\n2024-06-01\n2012-05-28\n7823\n\n\nReport.Date\n0\n1.00\n2002-11-29\n2024-06-02\n2012-06-06\n7825\n\n\nClearance.Date\n348308\n0.86\n2003-01-01\n2024-06-02\n2012-10-17\n7814\n\n\n\nVariable type: difftime\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Time\n0\n1\n0 secs\n86340 secs\n14:25:00\n1440\n\n\nReport.Time\n0\n1\n0 secs\n86340 secs\n14:06:00\n1440\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nIncident.Number\n0\n1.00\n6.031558e+10\n2.896224e+11\n20035.00\n2.005329e+10\n2.010505e+10\n2.017186e+10\n2.024242e+12\n▇▁▁▁▁\n\n\nHighest.Offense.Code\n0\n1.00\n1.689080e+03\n1.218280e+03\n100.00\n6.010000e+02\n1.199000e+03\n2.716000e+03\n8.905000e+03\n▇▅▁▁▁\n\n\nZip.Code\n0\n1.00\n7.873243e+04\n2.510000e+01\n76574.00\n7.871700e+04\n7.874100e+04\n7.875200e+04\n7.875900e+04\n▁▁▁▁▇\n\n\nCouncil.District\n30699\n0.99\n4.960000e+00\n2.840000e+00\n1.00\n3.000000e+00\n4.000000e+00\n7.000000e+00\n1.000000e+01\n▅▇▃▃▅\n\n\nCensus.Tract\n8822\n1.00\n2.453700e+02\n3.363970e+03\n1.00\n1.500000e+01\n2.324000e+01\n3.380000e+02\n9.508000e+05\n▇▁▁▁▁\n\n\nX.coordinate\n0\n1.00\n3.075787e+06\n3.551571e+05\n0.00\n3.108421e+06\n3.117292e+06\n3.126595e+06\n3.231806e+06\n▁▁▁▁▇\n\n\nY.coordinate\n0\n1.00\n9.946761e+06\n1.147895e+06\n0.00\n1.005743e+07\n1.007300e+07\n1.010056e+07\n1.021550e+07\n▁▁▁▁▇\n\n\nLatitude\n32335\n0.99\n3.029000e+01\n8.000000e-02\n30.01\n3.023000e+01\n3.028000e+01\n3.035000e+01\n3.067000e+01\n▁▇▇▂▁\n\n\nLongitude\n32335\n0.99\n-9.773000e+01\n5.000000e-02\n-98.18\n-9.776000e+01\n-9.773000e+01\n-9.770000e+01\n-9.737000e+01\n▁▁▇▂▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Date.Time\n0\n1\n2003-01-01 00:00:00\n2024-06-01 23:46:00\n2012-05-28 23:09:00\n1738386\n\n\nReport.Date.Time\n0\n1\n2002-11-29 05:30:00\n2024-06-02 01:20:00\n2012-06-06 11:15:00\n2169726\n\n\n\n\n\nI preserved quite a few variables from the original dataset, but the primary ones of interest will be Occurred.Date.Time, Zip.Code, Highest.Offense.Description, Category.Description, and Crime.Category. The last few variables pertaining to the type of crime committed are really all rolled into the last one, Crime.Category for the purposes of this analysis. Crime Category is a derived field that categorizes crimes into several different categories for the sake of understanding what crime occurred but without getting flooded with minute details. The categorizations were done manually by myself, and can be seen in the processing file as well as a description of the method performed. The short version is the categories are based on the UCR Crime descriptions given by the FBI when available, and are bucketed similarly from the Highest Offense field via string detection. This brings the number of unique categories in the Highest Offense field from 436 to 32 in the derived field."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#questionshypotheses-to-be-addressed",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#questionshypotheses-to-be-addressed",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Questions/Hypotheses to be addressed",
    "text": "Questions/Hypotheses to be addressed\nUltimately, I would like to explore a few questions:\n\nHas crime truly dropped, or is it expected to rise again as the year progresses?\nHas the homicide rate dropped with crime?\nHas the homicide rate been a problem for long time, or was it truly an emerging problem last year?"
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#schematic-of-workflow",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#schematic-of-workflow",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Schematic of workflow",
    "text": "Schematic of workflow\nThe intention of this analysis is to be exploratory; while I do have some questions I would like to chase down, I intend for them to be more of a compass than a map. With that said, the general strategy taken can be boiled down as follows:\n\nCheck for data quality, understand data representation\nClean the data, address problems identified in previous step, some exploration\nMore “formal” exploration. Consider what variables might be most important, and how those variables can be represented in the analysis.\nRepeat any steps performed above as necessary.\nResolve remaining or initial questions using statistical methods."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#exploratorydescriptive-analysis",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#exploratorydescriptive-analysis",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Exploratory/Descriptive analysis",
    "text": "Exploratory/Descriptive analysis\n\nA time series plot is a very natural starting part for the exploration of this analysis. given that the data is at the incident level, it is worth the effort to aggregate the count of occurrences up to some interval of time. We have values of all the way down to reported time of occurrence, but given the imputations that needed to occur in the data cleaning step, as well as the questionable reliability of whether the reported occurrence time is accurate to begin with, daily seems like the smallest interval of time that could be worth aggregating over. Figure 1 below is the corresponding aggregated time series plot. While we can see a general trend, the plot is rather dense since there are so many points plotted along the x-axis. If we want to do anything like isolating down to specific crimes like homicide, we will needed to aggregate at less fine of a grain, otherwise we will have too many intervals with 0 values for the observations.\n\n\n\n\n\n\n\n\nFigure 1: Crime Forecast Residual Plots\n\n\n\n\n\nFigure 2 below is the corresponding time series plot aggregated over each month. We still see a similar rise and fall trend like we did in the daily plot, but it is more pronounced since there are fewer outliers like there were in the daily plot. Outside of the overall trend, we can also see some potential patterns like a seasonal trend, and a change in the overall trend around 2018 until around 2020 where it briefly increases again before returning to a general decrease.\n\n\n\n\n\n\n\n\nFigure 2: Crime Forecast Residual Plots\n\n\n\n\n\nJumping ahead just a bit, after performing the simple lexicon based classification and properly identifying murder crimes, we can isolate to those crimes specifically for a time series plot. Figure 3 is exactly this, and we can see the trend is very different from what is seen in the overall time-series plot. For the most part, the rate of murder is relatively low up through 2015, and it is around 2016 that we see it spike and then slowly increase up until about 2022, where it then starts to slow back down.\n\n\n\n\n\n\n\n\nFigure 3: Monthly Murder Line Plot\n\n\n\n\n\nThese changes are interesting, primarily because we see the same shift before 2020 across overall crime and murder, but the number of murders is too small to be the primary driver in the overall crime trend. It’s also interesting because of the way the increase in murders start happening before the overall increase, and continues after the overall has started to decrease again. These may be caused by correlated factors like population, economic, or political factors, that the rate of murder could be more sensitive to than other crimes are. Furthermore, because the rate of murder is so low (relative to other types of crime), it’s likely that the variance in the trend is always going to be more sensitive to external factors. Once again the goal of this analysis is not intended to explain the why, but I think these observations could make for some good future studies into the potential causes or correlations. For now, the main question that these visuals bring me are the apparent trends; are they actually material, is the decrease seen the result of a change in the actual rate, or merely happenstance and we can expect the rate of murder to increase in future months? Before I attempt to explore this question, I want to see what variables might help me to explain it. The last chart considers the type of crime, which we will see in a moment, but it does not consider the location. For location, I have created a visualization based on Zip Code that I think can be revealing.\n\n\nCode\ncplotly\n\n\n\n\n\n\n\nThe interactive plot above shows us the number of crimes that occurred each year within each zip code, with the number of crimes represented by the continuous color scale, and the years represented by the animation so that values will change as the animation plays. We can see the overall decrease in crime from the way the Zip Codes that are more red slowly start to shift to blue or white. The zip codes that are more blue to start with don’t seem to change much as the animation plays. This suggests that the bulk of the crime is isolated to a handful of locations, and either factors that typically lead to crime are becoming less relevant in those areas, or the areas with heavier crime are seeing a greater focus in terms of prevention. This visualization is great, but with the analysis being more geared for a focus on Murder, it is unlikely to be of much help. There are probably Zip Codes where murder is more likely to occur, but the lower quantities would make it difficult to use that as a method to predict the number of murders that will happen in an interval of time small enough to have enough observations to make such a prediction in the first place. This chart does leave some questions that would be good for a future study, namely what has changed in those Zip Codes that are decreasing? Are there specific crimes that we are seeing fewer occurrences of, or is there better prevention in those areas?\nThe exploration has considered time and location as a variable, but the type of crime is still largely unexplored. The raw data, as stated before, has 434 different descriptions for the highest offense in each incident that was committed over the time period observed. The number of these descriptions makes it difficult to do anything with as a category, but coupled with other variables in the datset like the UCR Category may help us understand a better categorical representation. To accomplish this, crime that already had a UCR description for the observation was assigned this description as its category. A general description of each of these and several others can be found here[4]. Observations in this dataset were given a UCR Category code when they were considered a more serious crime identified by the FBI (this was given in the data dictionary that came with the data. The data dictionary is no longer available since the dataset was removed, but can be found in the repo for this analysis on github in the assets folder, or from the processing file directly since I have it copied there). Because there were many observations that were not given a category in the variable field, a lexicon was built by observing each individual Offense descriptions to find a way to group each offense description into a smaller categorical variable. Make no mistake, this categorization is not official categorization of the crimes themselves, nor is it intended to be a one for one match to any other categorization used by either APD or the FBI. However, it can be interpreted as one person’s layman’s terms attempt at categorizing these offenses. Essentially, how might an average person who does not regularly encounter law enforcement or crime think of each of these offenses? With that in mind, after observing each of the Highest Offense description, key words and phrases were identified that could capture either unique or similar (when possible) descriptions of the crimes, then string detection and regular expressions were used to identify observations with those descripions and then group them into a category of other similar offenses. We can see exactly how each offense description was grouped in the table below.\n\n\nCode\nRecatTable\n\n\n\n\n\nCrime Recategorization Key\n\n\n\n\n\n\n\n\n\nThe total count, yearly mean, and yearly standard deviation of occurrences of the offenses is provided to illustrate the disparity of the frequency of several of the offense descriptions. This is not entirely resolved by the New Category, but it does help for many of them. Again, the new categories are meant to generalize the descriptions. Page 21 of the table above contains the descriptions that I categorized as murder. A good example of how this method is imperfect is there, because of the presence of manslaughter. My initial thought process was to group instances of manslaughter into its own category, which is ultimately where I would have put Justified Homicide as well, however there were occurrences of manslaughter that were already categorized as murder by the UCR categorization, so bringing in those to the murder category would leave Justified Homicide in its own category, despite being a crime with a relatively low rate of occurrence. Ultimately this turned out to be the right call, because the FBI UCR website referenced earlier states that the “program classifies justifiable homicides separately,”[4] but for other cases not explicitly stated on this or other sites I may have just gotten it wrong judging by my first instinct. All said, a more summarized version of this chart is below, complete with sparklines as well to give an idea of the rate over time of each category.\n\n\nThis table is intended to give a general idea of the change of each crime over time and the general quantity relative to others. We can again see that our specific crime of focus, Murder, is lower than many other types, but is far from the lowest and still has a quantifiable rate. The sparklines don’t do the difference between crimes justice, because there are several that have a far greater rate of occurrence than others, but we can visualize this and the year over year change with the chart below.\n\n\nCode\ncatbarly\n\n\n\n\n\n\n\nAgain, the rate of murder is generally so low that it is dwarfed by the occurrences of most other crimes (particularly theft). We can see the bar chart peak out towards the end in 2021 and beyond, which is exactly what we expect. Overall, the rate of murder is so low compared to others that we may not be able to give it the same treatment as others, but begs the question if the rate of crime as a whole could somehow be used to help us predict each individual type of crime. We will use this idea during the Forecasting steps described below."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#time-series-and-forecasting",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#time-series-and-forecasting",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Time Series and Forecasting",
    "text": "Time Series and Forecasting\n\nTo determine if the recent decreases seen in crime overall and murder are coincidence or not, we need to establish what our expectation is. There are many ways we could do this, but a forecast seems the best, given the time series nature of our data. A few different time series models were attempted, like ARIMA and Exponential Smoothing, but the confidence interval for these was too wide for any group to make any reasonable inference with. However, utilizing the Fable package, a top-down hierarchical Seasonal ARIMA model is fit to the data and gives us a much more reasonable fit, both for the aggregated level and for the individual crime level for Murder. To fit the model the specific model, automatic model selection in the fable package was utilized while reconciling with a top-down hierarchical method. The selected model for the aggregate level is an ARIMA(0,1,2)(2,1,1)12. We can see from the residual plots for All Crime that the fit is generally fair. Innovation residuals are mostly centered around 0, the ACF plot suggests no autocorrelation, and even the distribution of the residuals is pretty normal, though there is a high frequency at 0 and just below 0 that may be questionable. The Ljung-Box test was not found to be significant, meaning we can reject the null hypothesis that the residuals are distinguishable from white noise agreeing with our observations from the ACF plot. The most questionable aspect here is likely the heteroscedasticity of the residuals, which appears to decrease and then increase over time, but this is not a requirement for a good time series model.\n\n\n\n\n\n\n\n\nFigure 4: Crime Forecast Residual Plots\n\n\n\n\n\n\n\nIt is not quite the same story regarding the residuals for murder. The selected model for the frequency of murder is ARIMA(0,1,1)(0,0,1)12. The plot of the residuals over time is still mostly centered at zero but the variance is less steady than it was for all crime, increasing quite a bit in the last few years, which makes sense since that is when we started to see the rate of Murder increase. Two spikes in the ACF plot are found to be significant, but this is not terribly abormal since 24 spikes are plotted, and the Ljung-Box test results are also not found to be significant. The distribution of the residuals is the most questionable, it is fairly asymmetrical so it is unlikely to be normally distributed, but again this is less important than the requirements for uncorrelated and mean 0 residuals.\n\n\n\n\n\n\n\n\nFigure 5: Murder Forecast Residual Plots\n\n\n\n\n\n\n\nThe plots below show these forecasts, the aggregation all crime and for murder, as well as two other crimes picked for their similarity to murder, Aggregated Assault and Unlawful Restraint (kidnapping and similar crimes). Aggravated Assault was chosen due to its similarity in how the crime is performed, and Unlawful Restraint was chosen due to the number of occurrences being similar to Murder. The plot for Murder is repeated enlarged for readability.\n\n\n\n\n\n\n\n\nFigure 6: Monthly Crime Forecast\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Murder Forecast\n\n\n\n\n\nFrom these forecasts, we can see that for overall crime the decrease observed earlier is expected to continue, however the confidence interval is quite wide so it could just as easily change course and begin to increase . For Murder, the decrease after the observed period is much less pronounced, but the confidence interval is much smaller . The difference in confidence interval is likely due to the smaller magnitude of crime counts compared to the aggregated model, and in this case the bounds are often not enough for more than one integer value between . That’s somewhat problematic early on, because it means the forecast is truly only predicting one value since it is a count, but it matters very little since the last value is well outside of any of the confidence bounds . The table below shows the point estimates of the forecast versus the exact value, and we can see that the the estimates were not terribly bad except for March, where we see the large dip in the plot . It is worth noting that April and May were not plotted since the article that spawned the question at the heart of this analysis was only observing the count of murders through the first quarter of the year ."
  },
  {
    "objectID": "HYLTIN-PII-project/products/manuscript/Manuscript.html#interpretation-and-conlusions",
    "href": "HYLTIN-PII-project/products/manuscript/Manuscript.html#interpretation-and-conlusions",
    "title": "Crime in Austin Texas: Exploration, Classification, and Trends",
    "section": "Interpretation and Conlusions",
    "text": "Interpretation and Conlusions\n\nThe findings in the analysis are somewhat inconclusive, though they were really intended to be exploratory. There are some clear trends in crime rate over time, and forecasting methods do not look to be beyond the realm of possibility but should should certainly be taken with a grain of salt. There was a drop in the count of Murders in the first quarter, primarily due to a drop in the month of March. The forecast suggested it was expected to be higher, but we are talking about a difference of 2.7 actual vs forecast. The counts of murder are so low that there is bound to be error, and if not it would likely be due confidence intervals so wide they would be almost meaningless. with that in mind, the table above suggests that we have 5 fewer murders in the first 5 months of 2024 than expected, so cautious optimism may be warranted.\nThe model itself is certainly not perfect, and perhaps future analyses could explore ways to improve the predictability. Hierarchical time series models can have multiple levels in their hierarchy, so it’s not impossible that other variables could have been combined with the category to improve results. Other crimes may also be more worthwhile in predicting; theft as an example dwarfed all other crimes and may very well be the biggest opportunity for the city of Austin. Models built on other higher frequency crimes can also likely make better use of location variables as well. More recent crime reporting data on the Austin data portal site have different methods of classification, and include individual charges with each incident, so it seems that they would be ripe for new variables to be implemented. Socio-economic variable could also potentially be used to see the impacts that they have on the rates of crime as whole or certain specific crimes. Comparisons with other cities would also be interesting, especially those with similarities to Austin to see how individual differences could lead to changes in the city’s environment. These are all ideas I could see myself returning to this analyses for, and I hope this analyses can provide myself or anyone else some inspiration for a more detailed analysis."
  },
  {
    "objectID": "HYLTIN-PII-project/code/processing-code/readme.html",
    "href": "HYLTIN-PII-project/code/processing-code/readme.html",
    "title": "processing-code",
    "section": "",
    "text": "processing-code\nThis folder contains code for processing data. All of the relevant code should be located in the processingfile.qmd document, which should clean the data and save the final output data inot the data -&gt; processed-data folders."
  },
  {
    "objectID": "HYLTIN-PII-project/code/eda-code/readme.html",
    "href": "HYLTIN-PII-project/code/eda-code/readme.html",
    "title": "eda-code",
    "section": "",
    "text": "eda-code\nThis folder contains code to do exploratory data analysis (EDA) on the processed/cleaned data."
  },
  {
    "objectID": "HYLTIN-PII-project/code/analysis-code/analysisfile.html",
    "href": "HYLTIN-PII-project/code/analysis-code/analysisfile.html",
    "title": "Analysis",
    "section": "",
    "text": "Setup\n\n#load needed packages. make sure they are installed.\npacman::p_load(here, knitr, tidyverse, skimr, fpp3, urca, forecast, gt)\ntheme_set(theme_minimal())\n\n\nd1 &lt;- readRDS(here('data','processed-data','processed-crime.rds'))\n\n\nd1 %&gt;% mutate(Category.Description = if_else(Category.Description == '', 'NA/Unknown', Category.Description)) %&gt;% group_by(Category.Description, Highest.Offense.Description) %&gt;% summarize(cnt = n()) %&gt;% pivot_wider(names_from = Category.Description, values_from = cnt)\n\n`summarise()` has grouped output by 'Category.Description'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 436 × 9\n   Highest.Offense.Description `Aggravated Assault` `Auto Theft` Burglary Murder\n   &lt;chr&gt;                                      &lt;int&gt;        &lt;int&gt;    &lt;int&gt;  &lt;int&gt;\n 1 AGG ASLT ENHANC STRANGL/SU…                 1098           NA       NA     NA\n 2 AGG ASLT STRANGLE/SUFFOCATE                 7712           NA       NA     NA\n 3 AGG ASLT W/MOTOR VEH FAM/D…                  772           NA       NA     NA\n 4 AGG ASSAULT                                18634           NA       NA     NA\n 5 AGG ASSAULT BY PUBLIC SERV…                   10           NA       NA     NA\n 6 AGG ASSAULT FAM/DATE VIOLE…                 9603           NA       NA     NA\n 7 AGG ASSAULT ON PEACE OFFIC…                   70           NA       NA     NA\n 8 AGG ASSAULT ON PUBLIC SERV…                  329           NA       NA     NA\n 9 AGG ASSAULT WITH MOTOR VEH                  1823           NA       NA     NA\n10 ARSON WITH BODILY INJURY                      15           NA       NA     NA\n# ℹ 426 more rows\n# ℹ 4 more variables: `NA/Unknown` &lt;int&gt;, Rape &lt;int&gt;, Robbery &lt;int&gt;,\n#   Theft &lt;int&gt;\n\ntemp &lt;- d1 %&gt;%\n         mutate(Occurred.Date = trunc.Date(Occurred.Date, 'months')) %&gt;% \n         group_by(Occurred.Date) %&gt;% \n         summarize(\n             cnt = n()\n         ) %&gt;% as.data.frame() %&gt;% \n  filter(year(Occurred.Date) != 2024)\nrownames(temp) &lt;- temp$Occurred.Date\ntemp &lt;- temp %&gt;% select(cnt)\n\ntemp2 &lt;- temp %&gt;% as.ts()\nAcf(temp2)\n\n\n\n\n\n\n\nBox.test(temp2, lag = 12, type = 'Ljung-Box')\n\n\n    Box-Ljung test\n\ndata:  temp2\nX-squared = 2101.8, df = 12, p-value &lt; 2.2e-16\n\nauto.arima(temp2)\n\nSeries: temp2 \nARIMA(5,1,1) \n\nCoefficients:\n         ar1     ar2     ar3      ar4      ar5      ma1\n      0.1644  0.3414  0.0846  -0.2550  -0.1711  -0.8042\ns.e.  0.0728  0.0650  0.0658   0.0626   0.0666   0.0418\n\nsigma^2 = 295701:  log likelihood = -1934.67\nAIC=3883.35   AICc=3883.81   BIC=3908.02\n\nauto.arima(temp2) %&gt;% forecast(h=24) %&gt;% autoplot()\n\n\n\n\n\n\n\n#temp2 %&gt;% log() %&gt;% diff(12) %&gt;% diff() %&gt;% diff() %&gt;% diff() %&gt;% diff() %&gt;% diff() %&gt;% Acf()\n\n\ntemp3 &lt;- d1 %&gt;%\n         filter(Crime.Category == 'MURDER') %&gt;% \n         mutate(Occurred.Date = trunc.Date(Occurred.Date, 'months')) %&gt;% \n         group_by(Occurred.Date) %&gt;% \n         summarize(\n             cnt = n()\n         ) %&gt;% as.data.frame() %&gt;% \n  filter(year(Occurred.Date) &lt; 2024)\nrownames(temp3) &lt;- temp3$Occurred.Date\ntemp3 &lt;- temp3 %&gt;% select(cnt)\n\ntemp5 &lt;- temp3 %&gt;% as.ts()\nAcf(temp5)\n\n\n\n\n\n\n\nauto.arima(log(temp5))\n\nSeries: log(temp5) \nARIMA(0,1,1) \n\nCoefficients:\n          ma1\n      -0.9234\ns.e.   0.0233\n\nsigma^2 = 0.3504:  log likelihood = -211.6\nAIC=427.2   AICc=427.25   BIC=434.12\n\nBox.test(diff(log(temp5)), lag = 4, type = 'Ljung-Box')\n\n\n    Box-Ljung test\n\ndata:  diff(log(temp5))\nX-squared = 51.075, df = 4, p-value = 2.153e-10\n\ntemp5 %&gt;% log() %&gt;% diff() %&gt;% ur.kpss() %&gt;% summary()\n\n\n####################### \n# KPSS Unit Root Test # \n####################### \n\nTest is of type: mu with 4 lags. \n\nValue of test-statistic is: 0.0174 \n\nCritical value for a significance level of: \n                10pct  5pct 2.5pct  1pct\ncritical values 0.347 0.463  0.574 0.739\n\ntemp5 %&gt;% log() %&gt;% diff() %&gt;% Acf()\n\n\n\n\n\n\n\ntemp5 %&gt;% log() %&gt;% diff() %&gt;% plot()\n\n\n\n\n\n\n\n\n\nfit &lt;- Arima(temp5, order = c(0,1,1))\nautoplot(forecast(fit, h=24))\n\n\n\n\n\n\n\nfit2 &lt;- Arima(log(temp5), order = c(0,1,1))\nautoplot(forecast(fit2, h=24))\n\n\n\n\n\n\n\n\n\nfets &lt;- function(x, h) {\n  forecast(ets(x), h=h)\n}\nfarima &lt;- function(x, h) {\n  forecast(auto.arima(x), h=h)\n}\n\ne1 &lt;- tsCV(log(temp5), fets, h=1)\ne2 &lt;- tsCV(log(temp5), farima, h=1)\n\n# MSE comparisons\nmean(e1^2, na.rm=TRUE)\n\n[1] 0.3646552\n\nmean(e2^2, na.rm=TRUE)\n\n[1] 0.3675369\n\n\n\ncrime_agg &lt;- d1 %&gt;%\n  filter(Occurred.Date &lt;= '2024-03-31') %&gt;% \n  mutate(Occurred.Date = yearmonth(Occurred.Date)) %&gt;% \n  group_by(Occurred.Date, Crime.Category) %&gt;% \n  summarize(\n     cnt = n()\n  ) %&gt;% as.data.frame() %&gt;% \n  as_tsibble(key = Crime.Category, index = Occurred.Date) %&gt;% \n  aggregate_key(Crime.Category, Count = sum(cnt)) %&gt;% \n  fill_gaps(Count = 0) \n\n`summarise()` has grouped output by 'Occurred.Date'. You can override using the\n`.groups` argument.\n\ngroup_fit &lt;- crime_agg %&gt;% \n  filter(year(Occurred.Date) &lt; 2024) %&gt;%\n  model(base = ARIMA(Count)) %&gt;% \n  reconcile(\n    td = top_down(base)\n  )\n\nWarning in sqrt(diag(best$var.coef)): NaNs produced\nWarning in sqrt(diag(best$var.coef)): NaNs produced\n\ngroup_fit\n\n# A mable: 34 x 3\n# Key:     Crime.Category [34]\n   Crime.Category                                    base\n   &lt;chr*&gt;                                         &lt;model&gt;\n 1 ABUSE OF OFFICE                         &lt;ARIMA(1,1,2)&gt;\n 2 AGGRAVATED ASSAULT  &lt;ARIMA(0,1,3)(0,0,2)[12] w/ drift&gt;\n 3 ALCOHOL RELATED              &lt;ARIMA(1,1,2)(0,0,2)[12]&gt;\n 4 AUTO THEFT                   &lt;ARIMA(2,1,0)(0,0,2)[12]&gt;\n 5 BRIBERY                         &lt;ARIMA(1,0,0) w/ mean&gt;\n 6 BURGLARY                     &lt;ARIMA(0,1,2)(2,0,0)[12]&gt;\n 7 CRIMINAL CONSPIRACY             &lt;ARIMA(1,0,1) w/ mean&gt;\n 8 CRIMINAL MISCHIEF            &lt;ARIMA(0,1,2)(0,0,2)[12]&gt;\n 9 DISORDERLY CONDUCT           &lt;ARIMA(2,1,2)(2,0,0)[12]&gt;\n10 DRUG RELATED                 &lt;ARIMA(0,1,1)(2,0,0)[12]&gt;\n# ℹ 24 more rows\n# ℹ 1 more variable: td &lt;model&gt;\n\ngroup_fcast &lt;- group_fit %&gt;% \n  forecast(h = '2 years') \n\ngrpPlot &lt;- group_fcast %&gt;% \n  filter(Crime.Category %in% c('MURDER', 'AGGRAVATED ASSAULT', 'UNLAWFUL RESTRAINT' )| is_aggregated(Crime.Category), .model == 'td') %&gt;% \n  autoplot(\n    crime_agg %&gt;% filter(year(Occurred.Date) &gt;= 2019)#, Crime.Category == 'MURDER')\n  ) + facet_wrap(vars(Crime.Category), scales = 'free_y',\n                 labeller = labeller(\n                   Crime.Category = c('&lt;aggregated&gt;' = 'All Crime', \n                                      'AGGRAVATED ASSAULT' = 'Aggravated Assault', \n                                      'MURDER' = 'Murder', \n                                      'UNLAWFUL RESTRAINT' = 'Unlawful Restraint')\n                   )\n                 ) +\n  labs(x = 'Month', y = 'Count', title = 'Grouped Forecasts of Crime Occurence by Month') +\n  theme(panel.background = element_rect(fill = 'white',\n                                        color = 'white'),\n        plot.background = element_rect(fill = 'white',\n                                        color = 'white')\n        )\n\nmurdplot &lt;- group_fcast %&gt;% \n  filter(Crime.Category %in% c('MURDER'), .model == 'td') %&gt;% \n  autoplot(\n    crime_agg %&gt;% filter(year(Occurred.Date) &gt;= 2019, Crime.Category == 'MURDER')\n  ) +\n  labs(x = 'Month', y = 'Count', title = 'Murder Forecasts by Month') +\n  theme(panel.background = element_rect(fill = 'white',\n                                        color = 'white'),\n        plot.background = element_rect(fill = 'white',\n                                        color = 'white')\n        )\n\n\ngrpPlot  \n\n\n\n\n\n\n\nmurdplot\n\n\n\n\n\n\n\nggsave(here('results', 'figures', 'static-plots', 'crime-forecast.png'), grpPlot)\n\nSaving 7 x 5 in image\n\nggsave(here('results', 'figures', 'static-plots', 'murder-forecast.png'), murdplot)\n\nSaving 7 x 5 in image\n\n\n\ncrime_agg %&gt;% \n    filter(year(Occurred.Date) &lt; 2024,Crime.Category == 'MURDER') %&gt;%\n    model(base = ARIMA(Count))\n\n# A mable: 1 x 2\n# Key:     Crime.Category [1]\n  Crime.Category                      base\n  &lt;chr*&gt;                           &lt;model&gt;\n1 MURDER         &lt;ARIMA(0,1,1)(0,0,1)[12]&gt;\n\ncrime_agg %&gt;% \n    filter(year(Occurred.Date) &lt; 2024,Crime.Category == 'MURDER') %&gt;%\n    model(base = ARIMA(Count)) %&gt;% forecast(h = '2 years') %&gt;% \n  autoplot(crime_agg %&gt;% filter(year(Occurred.Date) &gt;= 2019, Crime.Category == 'MURDER'))\n\n\n\n\n\n\n\n\n\nAvF &lt;- d1 %&gt;% \n  filter(year(Occurred.Date) == 2024, Crime.Category == 'MURDER') %&gt;% \n  mutate(\n    Occurred.Date = yearmonth(Occurred.Date)\n    ) %&gt;% \n  group_by(Occurred.Date) %&gt;% \n  summarize(\n    n = n()\n    ) %&gt;% left_join(filter(group_fcast, Crime.Category == 'MURDER' & .model == 'td'), join_by(Occurred.Date == Occurred.Date)) %&gt;% \n  select(Occurred.Date, n, .mean) %&gt;% \n  gt() %&gt;% \n  tab_header(\n    title = md('**Forecast vs Actual&lt;br&gt;Murder Count**')\n    ) %&gt;% \n  cols_label(\n    Occurred.Date = md('**Month**'),\n    n = md('**Actual**'),\n    .mean = md('**Forecasted**')\n  ) %&gt;% \n  fmt_number(columns = .mean, decimals = 1)\n\nAvF\n\n\n\n\n\n\n\n\n\n\n\n\n\nForecast vs ActualMurder Count\n\n\nMonth\nActual\nForecasted\n\n\n\n\n2024 Jan\n5\n5.9\n\n\n2024 Feb\n6\n5.4\n\n\n2024 Mar\n3\n5.7\n\n\n2024 Apr\n4\n5.0\n\n\n2024 May\n4\n5.0\n\n\n\n\n\n\n\ngtsave(AvF, 'actual-vs-forecast.html', here('results', 'figures', 'static-plots'))\n\n\ntheme_set(theme_gray())\naggresid &lt;- group_fit %&gt;% filter(is_aggregated(Crime.Category)) %&gt;% select(td) %&gt;% gg_tsresiduals()\n\naggresid\n\n\n\n\n\n\n\nggsave(here('results', 'figures', 'static-plots', 'crime-resid.png'), aggresid)\n\nSaving 7 x 5 in image\n\n\n\ntheme_set(theme_gray())\nmurdresid &lt;- group_fit %&gt;% filter(Crime.Category == 'MURDER') %&gt;% select(td) %&gt;% gg_tsresiduals()\n\nmurdresid\n\n\n\n\n\n\n\nggsave(here('results', 'figures', 'static-plots', 'murd-resid.png'), murdresid)\n\nSaving 7 x 5 in image\n\n\n\ntheme_set(theme_minimal())\nagglb &lt;- augment(filter(group_fit, is_aggregated(Crime.Category))) %&gt;% \n  features(.resid, ljung_box, l = 24) %&gt;% gt()\n\nagglb\n\n\n\n\n\n\n\n\nCrime.Category\n.model\nlb_stat\nlb_pvalue\n\n\n\n\n&lt;aggregated&gt;\nbase\n0.01390739\n0.9061235\n\n\n&lt;aggregated&gt;\ntd\n0.01390739\n0.9061235\n\n\n\n\n\n\n\ngtsave(agglb, 'aggregated-ljung.html', here('results', 'figures', 'static-plots'))\n\n\ntheme_set(theme_minimal())\nmurdlb &lt;- augment(filter(group_fit, Crime.Category == 'MURDER')) %&gt;% \n  features(.resid, ljung_box, l = 24) %&gt;% gt()\n\nmurdlb\n\n\n\n\n\n\n\n\nCrime.Category\n.model\nlb_stat\nlb_pvalue\n\n\n\n\nMURDER\nbase\n0.3738661\n0.5409044\n\n\nMURDER\ntd\n0.3738661\n0.5409044\n\n\n\n\n\n\n\ngtsave(murdlb, 'murder-ljung.html', here('results', 'figures', 'static-plots'))"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "This project is intended to serve as R coding practice, both for familiarity with R language and for some practice documenting code. We will be using the gapminder health and income dataset.\nLoading necessary packages for the project, I like to use pacman::p_load() because it checks for installation of packages before loading the library and automatically installs ones that I don’t have, as well it allows me to load multiple in just one line. The only major downside is that it does require the installation of the pacman package to use.\npacman::p_load(dslabs, tidyverse, readxl)\n# I have the library versions commented out here for anyone needing to \n# replicate this code that doesnt have the pacman package installed.\n\n#library(dslabs)\n#library(tidyverse)\n#library(readxl)\nNow we can take a look at the gapminder dataset. We start by using the str() function to get the Structure of the data. Then use the summary() function to get a quick summary of each of the variables in the dataset. Finally we use the class() function to confirm what type of object the gapminder dataset is.\n#help(gapminder) #commented out for the sake of rendering later\nprint('-----Data Structure-----') #these are just to make the outputs a little more readable.\n\n[1] \"-----Data Structure-----\"\n\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\nprint('-----Data Summary-----')\n\n[1] \"-----Data Summary-----\"\n\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\nprint('-----Object Type-----')\n\n[1] \"-----Object Type-----\"\n\nclass(gapminder)\n\n[1] \"data.frame\"\nNow we can filter to the continent variable to just Africa. To do this I use the dplyr function filter() along with the pipe %&gt;%. I primarily do this for readability, although subsetting with base R syntax would be just as easy to do. After that I use str() and summary() again to get the data structure and summaries.\nafricadata &lt;- gapminder %&gt;% filter(continent == 'Africa')\n\nprint('-----Data Structure-----')\n\n[1] \"-----Data Structure-----\"\n\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nprint('-----Data Summary-----')\n\n[1] \"-----Data Summary-----\"\n\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0\nNow we will create two new objects, im.le and pop.le to isolate variables of interest.\nWe will accomplish this by using again the pipe operator %&gt;% and the select() function.\nim.le &lt;- africadata %&gt;% select(infant_mortality, life_expectancy) \n  # select() allows us to choose relevant columns for our new objects\npop.le &lt;- africadata %&gt;% select(population, life_expectancy)\n\nprint('-----Data Structure-----')\n\n[1] \"-----Data Structure-----\"\n\nprint('---Population x Life Expectancy---')\n\n[1] \"---Population x Life Expectancy---\"\n\nstr(im.le)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nprint('---Infant Mortality x Life Expectancy---')\n\n[1] \"---Infant Mortality x Life Expectancy---\"\n\nstr(pop.le)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nprint('-----Data Summary-----')\n\n[1] \"-----Data Summary-----\"\n\nprint('---Population x Life Expectancy---')\n\n[1] \"---Population x Life Expectancy---\"\n\nsummary(im.le)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\nprint('---Infant Mortality x Life Expectancy---')\n\n[1] \"---Infant Mortality x Life Expectancy---\"\n\nsummary(pop.le)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51\nim.le %&gt;% \n  ggplot(aes(x=infant_mortality, y=life_expectancy)) +\n  geom_point() +\n  labs(x='Infant Mortality', y='Life Expectancy', \n       title='Examination of Life Expectancy in African Countries', \n       subtitle='Relationship of Infant Mortality and Life Expectancy')\n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\npop.le %&gt;% \n  ggplot(aes(x=population, y=life_expectancy)) +\n  coord_trans(x='log2') +\n  geom_point() +\n  labs(x='Population', y='Life Expectancy', \n       title='Examination of Life Expectancy in African Countries', \n       subtitle='Relationship of Population and Life Expectancy')\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\nThe charts imply a negative correlation between infant mortality and life expectancy, as well as a positive correlation between population and life expectancy. However the “streaks” of points that we are seeing are likely due to the year variable from our original dataset. What we are seeing is the year over year change in each country’s life expectancy and infant mortality or population. We can isolate to one year in particular to avoid this. First we should determine which years have missing data for infant_mortality.\nafricadata %&gt;% \n  group_by(year) %&gt;% #group_by function will allow us to easily identify the year\n  summarize(\n    missing_im = sum(is.na(infant_mortality)) #takes advantage of sum function and logical values since TRUE==1\n  )\n\n# A tibble: 57 × 2\n    year missing_im\n   &lt;int&gt;      &lt;int&gt;\n 1  1960         10\n 2  1961         17\n 3  1962         16\n 4  1963         16\n 5  1964         15\n 6  1965         14\n 7  1966         13\n 8  1967         11\n 9  1968         11\n10  1969          7\n# ℹ 47 more rows\nAs we can see, there are missing values for infant mortality all the way up to 1981, and again in 2016. So we will just need to choose a year after 1981, but not 2016.\nWe will isolate to the year 2000 by using the dplyr filter() function again.\nim.le2000 &lt;- africadata %&gt;% \n  filter(year == 2000) %&gt;% \n  select(infant_mortality, life_expectancy)\n\npop.le2000 &lt;- africadata %&gt;% \n  filter(year == 2000) %&gt;% \n  select(population, life_expectancy)\n\nprint('-----Data Structure-----')\n\n[1] \"-----Data Structure-----\"\n\nprint('---Population x Life Expectancy---')\n\n[1] \"---Population x Life Expectancy---\"\n\nstr(im.le2000)\n\n'data.frame':   51 obs. of  2 variables:\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n\nprint('---Infant Mortality x Life Expectancy---')\n\n[1] \"---Infant Mortality x Life Expectancy---\"\n\nstr(pop.le2000)\n\n'data.frame':   51 obs. of  2 variables:\n $ population     : num  31183658 15058638 6949366 1736579 11607944 ...\n $ life_expectancy: num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n\nprint('-----Data Summary-----')\n\n[1] \"-----Data Summary-----\"\n\nprint('---Population x Life Expectancy---')\n\n[1] \"---Population x Life Expectancy---\"\n\nsummary(im.le2000)\n\n infant_mortality life_expectancy\n Min.   : 12.30   Min.   :37.60  \n 1st Qu.: 60.80   1st Qu.:51.75  \n Median : 80.30   Median :54.30  \n Mean   : 78.93   Mean   :56.36  \n 3rd Qu.:103.30   3rd Qu.:60.00  \n Max.   :143.30   Max.   :75.00  \n\nprint('---Infant Mortality x Life Expectancy---')\n\n[1] \"---Infant Mortality x Life Expectancy---\"\n\nsummary(pop.le2000)\n\n   population        life_expectancy\n Min.   :    81154   Min.   :37.60  \n 1st Qu.:  2304687   1st Qu.:51.75  \n Median :  8799165   Median :54.30  \n Mean   : 15659800   Mean   :56.36  \n 3rd Qu.: 17391242   3rd Qu.:60.00  \n Max.   :122876723   Max.   :75.00\nRepeating the process for our plots with the new data:\nim.le2000 %&gt;% \n  ggplot(aes(x=infant_mortality, y=life_expectancy)) +\n  geom_point() +\n  labs(x='Infant Mortality', y='Life Expectancy', \n       title='Examination of Life Expectancy in African Countries', \n       subtitle='Relationship of Infant Mortality and Life Expectancy During the Year 2000')\n\n\n\n\n\n\n\npop.le2000 %&gt;% \n  ggplot(aes(x=population, y=life_expectancy)) +\n  coord_trans(x='log2') +\n  geom_point() +\n  labs(x='Population', y='Life Expectancy', \n       title='Examination of Life Expectancy in African Countries', \n       subtitle='Relationship of Population and Life Expectancy During the Year 2000')\nWe see there is still likely a relationship between infant mortality and life expectancy, but the relationship between population and life expectancy is less apparent, if it is still there at all. To get more conclusive results, we can fit a linear model between the variables using the lm() function.\nfit1 &lt;- lm(life_expectancy~., data = im.le2000)\n#fits life expectancy as a function of infant mortality. \n#Infant mortality is not explicitly listed since it is the only other variable in the object.\n\nfit2 &lt;- lm(life_expectancy~., data = pop.le2000) \n#fits life expectancy as a function of population. \n#Population is not explicitly listed since it is the only other variable in the object.\n\nprint('-----Life Expectancy as a function of Infant Mortality-----')\n\n[1] \"-----Life Expectancy as a function of Infant Mortality-----\"\n\nsummary(fit1) #summary of each linear model fit to get results.\n\n\nCall:\nlm(formula = life_expectancy ~ ., data = im.le2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\nprint('-----Life Expectancy as a function of Population-----')\n\n[1] \"-----Life Expectancy as a function of Population-----\"\n\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ ., data = pop.le2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\nFor the fit of Life Expectancy as a function of Infant Mortality, we see a p-value of 2.826e-08. Against an alpha-level of 0.05, we would conclude that there is a significant linear relationship between Infant Mortality and Life Expectancy.\nFor the fit of Life Expectancy as a function of Population, we see a p-value of 0.6159. Against an alpha-level of 0.05, we would conclude that there is not a significant linear relationship between Population and Life Expectancy."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#the-following-section-is-contributed-by-zane-chumley.",
    "href": "coding-exercise/coding-exercise.html#the-following-section-is-contributed-by-zane-chumley.",
    "title": "R Coding Exercise",
    "section": "The following section is contributed by ZANE CHUMLEY.",
    "text": "The following section is contributed by ZANE CHUMLEY.\n\nPick a dataset\n\n# It's an election year, so let's look at the polls from the year Trump eventually won.\nZaneA03 &lt;- results_us_election_2016\n\n\n\nExplore the dataset\n\n# look at the data's type\nclass(ZaneA03)\n\n[1] \"data.frame\"\n\n# look at the data's structure\nstr(ZaneA03)\n\n'data.frame':   51 obs. of  5 variables:\n $ state          : chr  \"California\" \"Texas\" \"Florida\" \"New York\" ...\n $ electoral_votes: int  55 38 29 29 20 20 18 16 16 15 ...\n $ clinton        : num  61.7 43.2 47.8 59 55.8 47.9 43.5 45.9 47.3 46.2 ...\n $ trump          : num  31.6 52.2 49 36.5 38.8 48.6 51.7 51 47.5 49.8 ...\n $ others         : num  6.7 4.5 3.2 4.5 5.4 3.6 4.8 3.1 5.2 4 ...\n\n# look at a summary of the data\nsummary(ZaneA03)\n\n    state           electoral_votes    clinton          trump      \n Length:51          Min.   : 3.00   Min.   :21.90   Min.   : 4.10  \n Class :character   1st Qu.: 4.50   1st Qu.:36.00   1st Qu.:41.15  \n Mode  :character   Median : 8.00   Median :46.20   Median :48.70  \n                    Mean   :10.55   Mean   :44.79   Mean   :48.45  \n                    3rd Qu.:11.50   3rd Qu.:51.75   3rd Qu.:57.40  \n                    Max.   :55.00   Max.   :90.90   Max.   :68.60  \n     others      \n Min.   : 1.900  \n 1st Qu.: 4.650  \n Median : 5.800  \n Mean   : 6.767  \n 3rd Qu.: 7.450  \n Max.   :27.000  \n\n\nIt is worth noting that the dataset is significantly less detailed than described in https://cran.r-project.org/web/packages/dslabs/dslabs.pdf. While there are only 5 columns in the dataset, the description indicated many more columns would be provided:\n\nstate. State in which poll was taken. ’U.S‘ is for national polls.\nstartdate. Poll’s start date.\nenddate. Poll’s end date.\npollster. Pollster conducting the poll.\ngrade. Grade assigned by fivethirtyeight to pollster.\nsamplesize. Sample size.\npopulation. Type of population being polled.\nrawpoll_clinton. Percentage for Hillary Clinton.\nrawpoll_trump. Percentage for Donald Trump\nrawpoll_johnson. Percentage for Gary Johnson\nrawpoll_mcmullin. Percentage for Evan McMullin.\nadjpoll_clinton. Fivethirtyeight adjusted percentage for Hillary Clinton.\najdpoll_trump. Fivethirtyeight adjusted percentage for Donald Trump\nadjpoll_johnson. Fivethirtyeight adjusted percentage for Gary Johnson\nadjpoll_mcmullin. Fivethirtyeight adjusted percentage for Evan McMullin\n\n\n\nDo any processing/cleaning you want to do\nFrom the exploration above it does not appear there are any NA values in the data. Let’s check to be sure.\n\n# Any NA values in the state column?\nZaneA03.state.NAs &lt;- ZaneA03[ZaneA03$state==\"NA\",]\nstr(ZaneA03.state.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n# Any NA values in the electoral_votes column?\nZaneA03.electorals.NAs &lt;- ZaneA03[ZaneA03$electoral_votes==\"NA\",]\nstr(ZaneA03.electorals.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n# Any NA values in the clinton column?\nZaneA03.clinton.NAs &lt;- ZaneA03[ZaneA03$clinton==\"NA\",]\nstr(ZaneA03.clinton.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n# Any NA values in the trump column?\nZaneA03.trump.NAs &lt;- ZaneA03[ZaneA03$trump==\"NA\",]\nstr(ZaneA03.trump.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n# Any NA values in the others column?\nZaneA03.others.NAs &lt;- ZaneA03[ZaneA03$others==\"NA\",]\nstr(ZaneA03.others.NAs)\n\n'data.frame':   0 obs. of  5 variables:\n $ state          : chr \n $ electoral_votes: int \n $ clinton        : num \n $ trump          : num \n $ others         : num \n\n\nThere are no objects in any of the five (5) datasets housing NA values. Therefore, no cleaning is warranted.\nBut are there any outliers?\n\n\nMake a few exploratory figures.\n\n# Let's use boxplots to see if there are any outliers in the four (4) columns containing numerical data\nboxplot(ZaneA03$electoral_votes\n        , main=\"Boxplot of Electorcal Votes\"\n        , xlab=\"\"\n        , horizontal=TRUE\n        )\n\n\n\n\n\n\n\nboxplot(ZaneA03$clinton\n        , main=\"Boxplot of Clinton poll readings\"\n        , xlab=\"\"\n        , horizontal=TRUE\n        )\n\n\n\n\n\n\n\nboxplot(ZaneA03$trump\n        , main=\"Boxplot of Trump poll readings\"\n        , xlab=\"\"\n        , horizontal=TRUE\n        )\n\n\n\n\n\n\n\nboxplot(ZaneA03$others\n        , main=\"Boxplot of pool readings for other candidates\"\n        , xlab=\"\"\n        , horizontal=TRUE\n        )\n\n\n\n\n\n\n\n\n\n\nOptionally, also some tables.\n\n# Let's display all the rows containing an outlier revealed by the boxplots above.\n\n# We'll sorting the rows by the values in each column into new datasets.\n# The sorting will be largest values first.\n# Then we will display the top and/or the bottom of the dataset corresponding to the upper and lower outliers, respectively.\nZaneA03.electorals.sorted &lt;- ZaneA03[order(-ZaneA03$electoral_votes),]\nhead(ZaneA03.electorals.sorted\n     , n=3\n     )\n\n       state electoral_votes clinton trump others\n1 California              55    61.7  31.6    6.7\n2      Texas              38    43.2  52.2    4.5\n3    Florida              29    47.8  49.0    3.2\n\nZaneA03.clinton.sorted &lt;- ZaneA03[order(-ZaneA03$clinton),]\nhead(ZaneA03.clinton.sorted\n     , n=1\n     )\n\n                  state electoral_votes clinton trump others\n51 District of Columbia               3    90.9   4.1      5\n\nZaneA03.trump.sorted &lt;- ZaneA03[order(-ZaneA03$trump),]\ntail(ZaneA03.trump.sorted\n     , n=1\n     )\n\n                  state electoral_votes clinton trump others\n51 District of Columbia               3    90.9   4.1      5\n\nZaneA03.others.sorted &lt;- ZaneA03[order(-ZaneA03$others),]\nhead(ZaneA03.others.sorted\n     , n=5\n     )\n\n        state electoral_votes clinton trump others\n35       Utah               6    27.5  45.5   27.0\n40      Idaho               4    27.5  59.3   13.2\n49    Vermont               3    56.7  30.3   13.1\n44     Alaska               3    36.6  51.3   12.2\n37 New Mexico               5    48.3  40.0   11.7\n\n\n\n\nRun some simple statistical model(s). Your choice.\nHow successful were the polls in predicting which candidate ultimately carried the state in the election? Well, we’ll need another dataset … the results from the voting.\n\n# Load actual votes from 2016 \nZaneA03.votedata &lt;- read_xlsx(\"1976-2020-president.xlsx\"\n                              , sheet=\"2016Flat\"\n                              , col_names = TRUE\n                              )\n# look at the data's type\nclass(ZaneA03.votedata)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# look at the data's structure\nstr(ZaneA03.votedata)\n\ntibble [51 × 9] (S3: tbl_df/tbl/data.frame)\n $ State       : chr [1:51] \"ALABAMA\" \"ALASKA\" \"ARIZONA\" \"ARKANSAS\" ...\n $ VotesClinton: num [1:51] 729547 116454 1161167 380494 8753788 ...\n $ VotesTrump  : num [1:51] 1318255 163387 1252401 684872 4483810 ...\n $ VotesOther  : num [1:51] 75570 38767 159597 65269 943997 ...\n $ TotalVotes  : num [1:51] 2123372 318608 2573165 1130635 14181595 ...\n $ ActClinton  : num [1:51] 34.4 36.6 45.1 33.7 61.7 ...\n $ ActTrump    : num [1:51] 62.1 51.3 48.7 60.6 31.6 ...\n $ ActOthers   : num [1:51] 3.56 12.17 6.2 5.77 6.66 ...\n $ TrumpWin    : logi [1:51] TRUE TRUE TRUE TRUE FALSE FALSE ...\n\n# look at a summary of the data\nsummary(ZaneA03.votedata)\n\n    State            VotesClinton       VotesTrump        VotesOther    \n Length:51          Min.   :  55973   Min.   :  12723   Min.   : 17022  \n Class :character   1st Qu.: 297584   1st Qu.: 377422   1st Qu.: 56876  \n Mode  :character   Median : 780154   Median : 949136   Median : 93418  \n                    Mean   :1291247   Mean   :1235001   Mean   :155854  \n                    3rd Qu.:1810340   3rd Qu.:1575898   3rd Qu.:225032  \n                    Max.   :8753788   Max.   :4685047   Max.   :943997  \n   TotalVotes         ActClinton       ActTrump       ActOthers     \n Min.   :  258788   Min.   :21.63   Min.   : 4.07   Min.   : 1.944  \n 1st Qu.:  758094   1st Qu.:35.99   1st Qu.:41.14   1st Qu.: 4.739  \n Median : 2001336   Median :46.17   Median :48.67   Median : 5.821  \n Mean   : 2682102   Mean   :44.61   Mean   :48.32   Mean   : 7.071  \n 3rd Qu.: 3347920   3rd Qu.:51.31   3rd Qu.:57.44   3rd Qu.: 8.611  \n Max.   :14181595   Max.   :90.48   Max.   :68.63   Max.   :26.998  \n  TrumpWin      \n Mode :logical  \n FALSE:21       \n TRUE :30       \n                \n                \n                \n\n\n\n# Are we fortunate enough that the dataset of polls and the dataset of votes are in the same order by state?   One way to check is through visual inspection.\n\nZaneA03.state.sorted &lt;- ZaneA03[order(ZaneA03$state),]\nZaneA03.sortcheck &lt;- rbind(ZaneA03.state.sorted$state\n                           , ZaneA03.votedata$State\n                            )\nhead(ZaneA03.sortcheck\n     , n=51\n     )\n\n     [,1]      [,2]     [,3]      [,4]       [,5]         [,6]      \n[1,] \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" \"California\" \"Colorado\"\n[2,] \"ALABAMA\" \"ALASKA\" \"ARIZONA\" \"ARKANSAS\" \"CALIFORNIA\" \"COLORADO\"\n     [,7]          [,8]       [,9]                   [,10]     [,11]    \n[1,] \"Connecticut\" \"Delaware\" \"District of Columbia\" \"Florida\" \"Georgia\"\n[2,] \"CONNECTICUT\" \"DELAWARE\" \"DISTRICT OF COLUMBIA\" \"FLORIDA\" \"GEORGIA\"\n     [,12]    [,13]   [,14]      [,15]     [,16]  [,17]    [,18]     \n[1,] \"Hawaii\" \"Idaho\" \"Illinois\" \"Indiana\" \"Iowa\" \"Kansas\" \"Kentucky\"\n[2,] \"HAWAII\" \"IDAHO\" \"ILLINOIS\" \"INDIANA\" \"IOWA\" \"KANSAS\" \"KENTUCKY\"\n     [,19]       [,20]   [,21]      [,22]           [,23]      [,24]      \n[1,] \"Louisiana\" \"Maine\" \"Maryland\" \"Massachusetts\" \"Michigan\" \"Minnesota\"\n[2,] \"LOUISIANA\" \"MAINE\" \"MARYLAND\" \"MASSACHUSETTS\" \"MICHIGAN\" \"MINNESOTA\"\n     [,25]         [,26]      [,27]     [,28]      [,29]    [,30]          \n[1,] \"Mississippi\" \"Missouri\" \"Montana\" \"Nebraska\" \"Nevada\" \"New Hampshire\"\n[2,] \"MISSISSIPPI\" \"MISSOURI\" \"MONTANA\" \"NEBRASKA\" \"NEVADA\" \"NEW HAMPSHIRE\"\n     [,31]        [,32]        [,33]      [,34]            [,35]         \n[1,] \"New Jersey\" \"New Mexico\" \"New York\" \"North Carolina\" \"North Dakota\"\n[2,] \"NEW JERSEY\" \"NEW MEXICO\" \"NEW YORK\" \"NORTH CAROLINA\" \"NORTH DAKOTA\"\n     [,36]  [,37]      [,38]    [,39]          [,40]          [,41]           \n[1,] \"Ohio\" \"Oklahoma\" \"Oregon\" \"Pennsylvania\" \"Rhode Island\" \"South Carolina\"\n[2,] \"OHIO\" \"OKLAHOMA\" \"OREGON\" \"PENNSYLVANIA\" \"RHODE ISLAND\" \"SOUTH CAROLINA\"\n     [,42]          [,43]       [,44]   [,45]  [,46]     [,47]     \n[1,] \"South Dakota\" \"Tennessee\" \"Texas\" \"Utah\" \"Vermont\" \"Virginia\"\n[2,] \"SOUTH DAKOTA\" \"TENNESSEE\" \"TEXAS\" \"UTAH\" \"VERMONT\" \"VIRGINIA\"\n     [,48]        [,49]           [,50]       [,51]    \n[1,] \"Washington\" \"West Virginia\" \"Wisconsin\" \"Wyoming\"\n[2,] \"WASHINGTON\" \"WEST VIRGINIA\" \"WISCONSIN\" \"WYOMING\"\n\n\nThe visual inspection reveals the data is aligned by state.\n\n# The visual inspection reveals the data is aligned by state. \n# So, polls and votes, by the power invested in me by R,\n# I pronounce your merged!\n\n# Build the list of datasets to merge\n\n# ZaneA03.datasetlist &lt;- c(\"ZaneA03.state.sorted\"\n#                         , \"ZaneA03.votedata\"\n#                         )\n# class(ZaneA03.datasetlist)\n\n\n# Go forth and merge!\n\n# Rest of this section commented out so you can see everything above in the render\n# ZaneA03.PollsAndVotes &lt;- rbind(ZaneA03.state.sorted\n#                               , ZaneA03.votedata)\n\n\n# \n#ZaneA03.PollsAndVotes &lt;- ZaneA03.state.sorted + ZaneA03.votedata"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "My name is William Hyltin, I am currently a student in the Master of Science in Data Analytics program at the University of Texas at San Antonio. I have roughly 2 years of experience as an analyst, a year and a half as a Reporting Analyst for Omnichannel Analytics for a bank, and about 4 months of experience as a Business Strategy Analyst for the Actuary and Analytics organization of an insurance company. Professionally I’ve had the opportunity to use a number of different platforms, to include SQL (Snowflake, PROC SQL, and some SQL Server), SAS Enterprise Guide, Minitab, and Excel. Educationally I’ve had the opportunity to use R, Tableau, SAS (both Base and Enterprise Guide), and Python. Prior to working as an Analyst I worked for a call center as a phone representative, but my aspirations at the time had been to be come an Actuary, for which I was able to complete the first two exams (Probability and Financial Markets). It was ultimately several rejections for an entry level actuarial role that lead to me to my job as a Reporting Analyst, but I am finding that these more Data Analyst and adjacent roles are more enjoyable than the work I would have been doing as an Actuary.\n\n\n\n\n\n\n\nMy work as a reporting analyst was rotational, and had me on a team of several other Strategy Analysts. As a result I ended up doing a lot of strategy analytics work in addition to my reporting responsibilities. Some of my responsibilities in the role were reporting on several call center metrics, like satisfaction survey results, handle time of calls, and call volume. The more strategic work I got to do were things like root cause analysis for changes in performance metrics, estimating the impacts that future changes would have on those metrics, and setting goals for call center employees based on their current performance and skills. Most of the work I did here was done in Snowflake SQL, excel, and SAS. I would use statistical techniques like regression, ANOVA, t-test, and descriptive statistics, then utilize my findings to build reports, visualizations, and make recommendations.\n\n\n\n\n\n\n\nIt has often been said to me that analysts will spend 71% of their time massaging their data before they can start on the “real” analytic work. My role as a Business Strategy Analyst is intended to combat that. I work on a team whose purpose is to identify meaningful transactions or events that our customers are performing in complex and often disparate data, and translating this data into a consumable format so that Analysts can spend more time focusing on answering the questions at hand. This is similar to what a Data Engineer might do, but with a greater focus on how the business itself operates, and what an analyst would be looking for given a request from the business. This also means that analyses that we do are usually done in the name of accuracy over insight, and there is less of an opportunity for statistic methods outside of things like frequency-related measures and the occasional descriptive statistic.\n\n\n\n\n\n\n\nThe genesis of this site is from my Practicum II class for my Masters Program, in which I am excited about the opportunity to create a proper portfolio for myself. I see it as a means of displaying the kind of real work I am capable of, and as a way to inspire myself to take on more curiosity projects that go beyond the requirements of formal education and my profession. I’m also excited for the opportunity to use more R, since its not a language I get to use often but when I do I always enjoy it.\n\n\n\n\n\n\n\nBeyond my professional and educational experience, I have a number of other interests that I struggle to make time for. I play dungeons and dragons fairly regularly on the weekends with a few other friends. I enjoy reading, particularly horror and classic science fiction (think more Flowers for Algernon and Invisible Man than Ender’s Game). My girlfriend and I like going to concerts when we can, most recent standouts have been the reunion tour for the Postal Service and a Hot Mulligan show where one of the openers threw Burger Boy burgers into the crowd. I have two kids, a 6 year old aspiring princess/ doctor/ dancer/ singer/ chef, and a soon to be 14 year old son who is a trumpet and piano master with aspirations to become a music therapist."
  },
  {
    "objectID": "aboutme.html#a-man-who-needs-no-introduction-but-since-youre-here",
    "href": "aboutme.html#a-man-who-needs-no-introduction-but-since-youre-here",
    "title": "About me",
    "section": "",
    "text": "My name is William Hyltin, I am currently a student in the Master of Science in Data Analytics program at the University of Texas at San Antonio. I have roughly 2 years of experience as an analyst, a year and a half as a Reporting Analyst for Omnichannel Analytics for a bank, and about 4 months of experience as a Business Strategy Analyst for the Actuary and Analytics organization of an insurance company. Professionally I’ve had the opportunity to use a number of different platforms, to include SQL (Snowflake, PROC SQL, and some SQL Server), SAS Enterprise Guide, Minitab, and Excel. Educationally I’ve had the opportunity to use R, Tableau, SAS (both Base and Enterprise Guide), and Python. Prior to working as an Analyst I worked for a call center as a phone representative, but my aspirations at the time had been to be come an Actuary, for which I was able to complete the first two exams (Probability and Financial Markets). It was ultimately several rejections for an entry level actuarial role that lead to me to my job as a Reporting Analyst, but I am finding that these more Data Analyst and adjacent roles are more enjoyable than the work I would have been doing as an Actuary."
  },
  {
    "objectID": "aboutme.html#work-as-a-reporting-analyst",
    "href": "aboutme.html#work-as-a-reporting-analyst",
    "title": "About me",
    "section": "",
    "text": "My work as a reporting analyst was rotational, and had me on a team of several other Strategy Analysts. As a result I ended up doing a lot of strategy analytics work in addition to my reporting responsibilities. Some of my responsibilities in the role were reporting on several call center metrics, like satisfaction survey results, handle time of calls, and call volume. The more strategic work I got to do were things like root cause analysis for changes in performance metrics, estimating the impacts that future changes would have on those metrics, and setting goals for call center employees based on their current performance and skills. Most of the work I did here was done in Snowflake SQL, excel, and SAS. I would use statistical techniques like regression, ANOVA, t-test, and descriptive statistics, then utilize my findings to build reports, visualizations, and make recommendations."
  },
  {
    "objectID": "aboutme.html#work-as-a-business-strategy-analyst",
    "href": "aboutme.html#work-as-a-business-strategy-analyst",
    "title": "About me",
    "section": "",
    "text": "It has often been said to me that analysts will spend 71% of their time massaging their data before they can start on the “real” analytic work. My role as a Business Strategy Analyst is intended to combat that. I work on a team whose purpose is to identify meaningful transactions or events that our customers are performing in complex and often disparate data, and translating this data into a consumable format so that Analysts can spend more time focusing on answering the questions at hand. This is similar to what a Data Engineer might do, but with a greater focus on how the business itself operates, and what an analyst would be looking for given a request from the business. This also means that analyses that we do are usually done in the name of accuracy over insight, and there is less of an opportunity for statistic methods outside of things like frequency-related measures and the occasional descriptive statistic."
  },
  {
    "objectID": "aboutme.html#course-aspirations",
    "href": "aboutme.html#course-aspirations",
    "title": "About me",
    "section": "",
    "text": "The genesis of this site is from my Practicum II class for my Masters Program, in which I am excited about the opportunity to create a proper portfolio for myself. I see it as a means of displaying the kind of real work I am capable of, and as a way to inspire myself to take on more curiosity projects that go beyond the requirements of formal education and my profession. I’m also excited for the opportunity to use more R, since its not a language I get to use often but when I do I always enjoy it."
  },
  {
    "objectID": "aboutme.html#but-enough-about-me-lets-talk-about-yours-truly",
    "href": "aboutme.html#but-enough-about-me-lets-talk-about-yours-truly",
    "title": "About me",
    "section": "",
    "text": "Beyond my professional and educational experience, I have a number of other interests that I struggle to make time for. I play dungeons and dragons fairly regularly on the weekends with a few other friends. I enjoy reading, particularly horror and classic science fiction (think more Flowers for Algernon and Invisible Man than Ender’s Game). My girlfriend and I like going to concerts when we can, most recent standouts have been the reunion tour for the Postal Service and a Hot Mulligan show where one of the openers threw Burger Boy burgers into the crowd. I have two kids, a 6 year old aspiring princess/ doctor/ dancer/ singer/ chef, and a soon to be 14 year old son who is a trumpet and piano master with aspirations to become a music therapist."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "The CDC data set I have chosen for this exercise is the Tobacco Usage Behavioral Risk Factor data set. The data comes from a survey administered by the CDC as part of the State Tobacco Activities Tracking and Evaluation (STATE) system. The data set contains information about the year the survey was administered, respondent location (State or territory), demographics (namely Race, Age, Education, and Gender), and it asks questions about a person’s tobacco usage status (current, former, never used), their cessation status (whether they have quit or attempted to quit in the last year), and the frequency that they use tobacco for those that do (daily, some days). The usage questions are asked for three different types of Tobacco products, Cigarettes, Smokeless Tobacco, and E-cigarettes.\n\npacman::p_load(here, tidyverse, skimr, plotly, synthpop, patchwork)\n\n\nrawdata &lt;- read_csv('cdc-data-raw.csv')\n\nRows: 43341 Columns: 31\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (25): YEAR, LocationAbbr, LocationDesc, TopicType, TopicDesc, MeasureDes...\ndbl  (6): Data_Value, Data_Value_Std_Err, Low_Confidence_Limit, High_Confide...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nstr(rawdata) # getting an idea for data structure\n\nspc_tbl_ [43,341 × 31] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ YEAR                      : chr [1:43341] \"2017\" \"2018\" \"2017\" \"2016\" ...\n $ LocationAbbr              : chr [1:43341] \"GU\" \"US\" \"US\" \"GU\" ...\n $ LocationDesc              : chr [1:43341] \"Guam\" \"National Median (States and DC)\" \"National Median (States and DC)\" \"Guam\" ...\n $ TopicType                 : chr [1:43341] \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" ...\n $ TopicDesc                 : chr [1:43341] \"Cigarette Use (Adults)\" \"Cigarette Use (Adults)\" \"Cigarette Use (Adults)\" \"Smokeless Tobacco Use (Adults)\" ...\n $ MeasureDesc               : chr [1:43341] \"Current Smoking\" \"Smoking Status\" \"Smoking Status\" \"Current Use\" ...\n $ DataSource                : chr [1:43341] \"BRFSS\" \"BRFSS\" \"BRFSS\" \"BRFSS\" ...\n $ Response                  : chr [1:43341] NA \"Current\" \"Never\" NA ...\n $ Data_Value_Unit           : chr [1:43341] \"%\" \"%\" \"%\" \"%\" ...\n $ Data_Value_Type           : chr [1:43341] \"Percentage\" \"Percentage\" \"Percentage\" \"Percentage\" ...\n $ Data_Value                : num [1:43341] 30 16.1 58.2 0.5 21.9 21.6 2.8 3.6 29.2 14.8 ...\n $ Data_Value_Footnote_Symbol: chr [1:43341] NA NA NA NA ...\n $ Data_Value_Footnote       : chr [1:43341] NA NA NA NA ...\n $ Data_Value_Std_Err        : num [1:43341] 2.3 NA NA 0.4 4.9 0.8 0.3 0.4 2.4 0.6 ...\n $ Low_Confidence_Limit      : num [1:43341] 25.4 NA NA 0 12.4 20 2.2 2.7 24.5 13.6 ...\n $ High_Confidence_Limit     : num [1:43341] 34.6 NA NA 1.2 31.4 23.2 3.4 4.5 33.9 16 ...\n $ Sample_Size               : num [1:43341] 692 NA NA 123 209 ...\n $ Gender                    : chr [1:43341] \"Male\" \"Overall\" \"Overall\" \"Overall\" ...\n $ Race                      : chr [1:43341] \"All Races\" \"All Races\" \"All Races\" \"Hispanic\" ...\n $ Age                       : chr [1:43341] \"All Ages\" \"All Ages\" \"All Ages\" \"All Ages\" ...\n $ Education                 : chr [1:43341] \"All Grades\" \"All Grades\" \"All Grades\" \"All Grades\" ...\n $ GeoLocation               : chr [1:43341] \"(13.444304, 144.793731)\" NA NA \"(13.444304, 144.793731)\" ...\n $ TopicTypeId               : chr [1:43341] \"BEH\" \"BEH\" \"BEH\" \"BEH\" ...\n $ TopicId                   : chr [1:43341] \"100BEH\" \"100BEH\" \"100BEH\" \"150BEH\" ...\n $ MeasureId                 : chr [1:43341] \"110CSA\" \"165SSA\" \"165SSA\" \"177SCU\" ...\n $ StratificationID1         : chr [1:43341] \"2GEN\" \"1GEN\" \"1GEN\" \"1GEN\" ...\n $ StratificationID2         : chr [1:43341] \"8AGE\" \"8AGE\" \"8AGE\" \"8AGE\" ...\n $ StratificationID3         : chr [1:43341] \"6RAC\" \"6RAC\" \"6RAC\" \"4RAC\" ...\n $ StratificationID4         : chr [1:43341] \"6EDU\" \"6EDU\" \"6EDU\" \"6EDU\" ...\n $ SubMeasureID              : chr [1:43341] \"BRF21\" \"BRF27\" \"BRF28\" \"BRF69\" ...\n $ DisplayOrder              : num [1:43341] 21 27 28 69 22 21 77 71 26 27 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   YEAR = col_character(),\n  ..   LocationAbbr = col_character(),\n  ..   LocationDesc = col_character(),\n  ..   TopicType = col_character(),\n  ..   TopicDesc = col_character(),\n  ..   MeasureDesc = col_character(),\n  ..   DataSource = col_character(),\n  ..   Response = col_character(),\n  ..   Data_Value_Unit = col_character(),\n  ..   Data_Value_Type = col_character(),\n  ..   Data_Value = col_double(),\n  ..   Data_Value_Footnote_Symbol = col_character(),\n  ..   Data_Value_Footnote = col_character(),\n  ..   Data_Value_Std_Err = col_double(),\n  ..   Low_Confidence_Limit = col_double(),\n  ..   High_Confidence_Limit = col_double(),\n  ..   Sample_Size = col_double(),\n  ..   Gender = col_character(),\n  ..   Race = col_character(),\n  ..   Age = col_character(),\n  ..   Education = col_character(),\n  ..   GeoLocation = col_character(),\n  ..   TopicTypeId = col_character(),\n  ..   TopicId = col_character(),\n  ..   MeasureId = col_character(),\n  ..   StratificationID1 = col_character(),\n  ..   StratificationID2 = col_character(),\n  ..   StratificationID3 = col_character(),\n  ..   StratificationID4 = col_character(),\n  ..   SubMeasureID = col_character(),\n  ..   DisplayOrder = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(rawdata) # summary statistics\n\n     YEAR           LocationAbbr       LocationDesc        TopicType        \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  TopicDesc         MeasureDesc         DataSource          Response        \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n Data_Value_Unit    Data_Value_Type      Data_Value   \n Length:43341       Length:43341       Min.   : 0.00  \n Class :character   Class :character   1st Qu.: 5.10  \n Mode  :character   Mode  :character   Median :17.10  \n                                       Mean   :25.36  \n                                       3rd Qu.:38.90  \n                                       Max.   :99.90  \n                                       NA's   :2117   \n Data_Value_Footnote_Symbol Data_Value_Footnote Data_Value_Std_Err\n Length:43341               Length:43341        Min.   : 0.000    \n Class :character           Class :character    1st Qu.: 0.600    \n Mode  :character           Mode  :character    Median : 1.100    \n                                                Mean   : 1.711    \n                                                3rd Qu.: 2.200    \n                                                Max.   :16.600    \n                                                NA's   :2195      \n Low_Confidence_Limit High_Confidence_Limit  Sample_Size       Gender         \n Min.   : 0.00        Min.   :  0.00        Min.   :   50   Length:43341      \n 1st Qu.: 3.20        1st Qu.:  6.90        1st Qu.:  480   Class :character  \n Median :14.30        Median : 19.80        Median : 1798   Mode  :character  \n Mean   :22.01        Mean   : 28.68        Mean   : 3050                     \n 3rd Qu.:30.50        3rd Qu.: 47.40        3rd Qu.: 4203                     \n Max.   :99.80        Max.   :100.00        Max.   :40726                     \n NA's   :2195         NA's   :2195          NA's   :2195                      \n     Race               Age             Education         GeoLocation       \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n TopicTypeId          TopicId           MeasureId         StratificationID1 \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n StratificationID2  StratificationID3  StratificationID4  SubMeasureID      \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  DisplayOrder  \n Min.   : 5.00  \n 1st Qu.:24.00  \n Median :51.00  \n Mean   :47.08  \n 3rd Qu.:71.00  \n Max.   :81.00  \n                \n\nskim(rawdata) # primarily to get completion rates\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n43341\n\n\nNumber of columns\n31\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n25\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nYEAR\n0\n1.00\n4\n9\n0\n17\n0\n\n\nLocationAbbr\n0\n1.00\n2\n2\n0\n54\n0\n\n\nLocationDesc\n0\n1.00\n4\n31\n0\n54\n0\n\n\nTopicType\n0\n1.00\n25\n25\n0\n1\n0\n\n\nTopicDesc\n0\n1.00\n18\n30\n0\n4\n0\n\n\nMeasureDesc\n0\n1.00\n11\n59\n0\n10\n0\n\n\nDataSource\n0\n1.00\n5\n5\n0\n1\n0\n\n\nResponse\n28323\n0.35\n5\n11\n0\n6\n0\n\n\nData_Value_Unit\n0\n1.00\n1\n1\n0\n1\n0\n\n\nData_Value_Type\n0\n1.00\n10\n10\n0\n1\n0\n\n\nData_Value_Footnote_Symbol\n41224\n0.05\n1\n1\n0\n1\n0\n\n\nData_Value_Footnote\n41224\n0.05\n71\n71\n0\n1\n0\n\n\nGender\n0\n1.00\n4\n7\n0\n3\n0\n\n\nRace\n0\n1.00\n5\n29\n0\n6\n0\n\n\nAge\n0\n1.00\n8\n18\n0\n8\n0\n\n\nEducation\n0\n1.00\n10\n12\n0\n4\n0\n\n\nGeoLocation\n78\n1.00\n23\n41\n0\n53\n0\n\n\nTopicTypeId\n0\n1.00\n3\n3\n0\n1\n0\n\n\nTopicId\n0\n1.00\n6\n6\n0\n4\n0\n\n\nMeasureId\n0\n1.00\n6\n6\n0\n14\n0\n\n\nStratificationID1\n0\n1.00\n4\n4\n0\n3\n0\n\n\nStratificationID2\n0\n1.00\n4\n4\n0\n8\n0\n\n\nStratificationID3\n0\n1.00\n4\n4\n0\n6\n0\n\n\nStratificationID4\n0\n1.00\n4\n4\n0\n4\n0\n\n\nSubMeasureID\n0\n1.00\n5\n5\n0\n53\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nData_Value\n2117\n0.95\n25.36\n25.00\n0\n5.1\n17.1\n38.9\n99.9\n▇▃▂▁▁\n\n\nData_Value_Std_Err\n2195\n0.95\n1.71\n1.72\n0\n0.6\n1.1\n2.2\n16.6\n▇▁▁▁▁\n\n\nLow_Confidence_Limit\n2195\n0.95\n22.01\n24.14\n0\n3.2\n14.3\n30.5\n99.8\n▇▂▂▁▁\n\n\nHigh_Confidence_Limit\n2195\n0.95\n28.68\n26.21\n0\n6.9\n19.8\n47.4\n100.0\n▇▃▂▂▁\n\n\nSample_Size\n2195\n0.95\n3050.17\n3798.87\n50\n480.0\n1798.0\n4203.0\n40726.0\n▇▁▁▁▁\n\n\nDisplayOrder\n0\n1.00\n47.08\n24.49\n5\n24.0\n51.0\n71.0\n81.0\n▂▇▁▂▇\n\n\n\n\n\n\nhead(rawdata, 20) # getting first 20 rows\n\n# A tibble: 20 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2017     GU           Guam         Tobacco … Cigarett… Current Sm… BRFSS     \n 2 2018     US           National Me… Tobacco … Cigarett… Smoking St… BRFSS     \n 3 2017     US           National Me… Tobacco … Cigarett… Smoking St… BRFSS     \n 4 2016     GU           Guam         Tobacco … Smokeles… Current Use BRFSS     \n 5 2014     GU           Guam         Tobacco … Cigarett… Current Sm… BRFSS     \n 6 2012     IN           Indiana      Tobacco … Cigarett… Current Sm… BRFSS     \n 7 2018-20… DE           Delaware     Tobacco … Smokeles… Current Us… BRFSS     \n 8 2011     MO           Missouri     Tobacco … Smokeles… Current Use BRFSS     \n 9 2013     ME           Maine        Tobacco … Cigarett… Smoking Fr… BRFSS     \n10 2017     WA           Washington   Tobacco … Cigarett… Smoking St… BRFSS     \n11 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n12 2011     MN           Minnesota    Tobacco … Smokeles… User Status BRFSS     \n13 2018     VA           Virginia     Tobacco … Cigarett… Smoking Fr… BRFSS     \n14 2014     MN           Minnesota    Tobacco … Cigarett… Smoking St… BRFSS     \n15 2017-20… WA           Washington   Tobacco … Cigarett… Current Sm… BRFSS     \n16 2017     UT           Utah         Tobacco … Smokeles… Current Use BRFSS     \n17 2012     CA           California   Tobacco … Smokeles… Frequency … BRFSS     \n18 2015     FL           Florida      Tobacco … Smokeles… Current Use BRFSS     \n19 2015     RI           Rhode Island Tobacco … Smokeles… Current Use BRFSS     \n20 2019     AZ           Arizona      Tobacco … Smokeles… Current Use BRFSS     \n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, StratificationID1 &lt;chr&gt;,\n#   StratificationID2 &lt;chr&gt;, StratificationID3 &lt;chr&gt;, …\n\n\nThe data set is not the most analyst friendly format. Generally the ideal format would be variables in the columns, observations in the rows. This data set has variables across several rows, and it is mostly aggregated. The questions are broken up by the different response options, which are spread across rows even though the values are proportions and therefore the samples sizes listed for the different response options for a single question come from the same audience. Aggregations grouped by each of the demographics, locations, and timeframes are also in the rows, similar to a pivot table, so getting things like summary statistics will require filtering these aggregated values. Some survey question variables have missing values, but with the data in this format it’s difficult to make much of them, but we will come back to that. There are a few variables that only serve to give information about the data set itself, like Data_Value_Unit, which only contains one value, “Percentage”, to inform that the Value column is a percentage. Variable like this (Zero or Near Zero Variance) can be removed, since they give no information about the individual observations.\nVariables to be removed: Zero Variance variables: Data_Value_Unit, Data_Value_Type, DataSource, TopicType, TopicTypeId NZV: Data_Value_Footnote_Symbol, Data_Value_Footnote\n\nrawdata %&gt;% filter(is.na(Data_Value)) #looking at nulls to determine why they are there\n\n# A tibble: 2,117 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n 2 2016     IA           Iowa         Tobacco … Smokeles… Frequency … BRFSS     \n 3 2014     ID           Idaho        Tobacco … Smokeles… Frequency … BRFSS     \n 4 2019     MI           Michigan     Tobacco … Smokeles… Frequency … BRFSS     \n 5 2016-20… MA           Massachuset… Tobacco … Smokeles… Current Us… BRFSS     \n 6 2013-20… DC           District of… Tobacco … Cigarett… Current Sm… BRFSS     \n 7 2013     ME           Maine        Tobacco … Cigarett… Current Sm… BRFSS     \n 8 2011     AZ           Arizona      Tobacco … Smokeles… Frequency … BRFSS     \n 9 2018     AL           Alabama      Tobacco … Smokeles… Frequency … BRFSS     \n10 2018     NC           North Carol… Tobacco … Smokeles… Frequency … BRFSS     \n# ℹ 2,107 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\nMissing values from the Data_Value column occur when the sample size is not large enough to report results. Some of these missing values may have the opportunity for imputation, and filtering these could cause some confusion in the actual structure of the data (complete and consistent combinations of categorical values) so we won’t filter anything out just yet. note this is evidenced by the Footnote column with the following message: “Data in these cells have been suppressed because of a small sample size.”\n\nunique(paste(rawdata$LocationDesc,rawdata$LocationAbbr)) # paste concatenates columns, unique to get distinct values.\n\n [1] \"Guam GU\"                            \"National Median (States and DC) US\"\n [3] \"Indiana IN\"                         \"Delaware DE\"                       \n [5] \"Missouri MO\"                        \"Maine ME\"                          \n [7] \"Washington WA\"                      \"Illinois IL\"                       \n [9] \"Minnesota MN\"                       \"Virginia VA\"                       \n[11] \"Utah UT\"                            \"California CA\"                     \n[13] \"Florida FL\"                         \"Rhode Island RI\"                   \n[15] \"Arizona AZ\"                         \"District of Columbia DC\"           \n[17] \"Kansas KS\"                          \"Nevada NV\"                         \n[19] \"Alabama AL\"                         \"West Virginia WV\"                  \n[21] \"Wisconsin WI\"                       \"Oklahoma OK\"                       \n[23] \"New York NY\"                        \"Iowa IA\"                           \n[25] \"Colorado CO\"                        \"Idaho ID\"                          \n[27] \"Alaska AK\"                          \"North Dakota ND\"                   \n[29] \"North Carolina NC\"                  \"New Hampshire NH\"                  \n[31] \"Vermont VT\"                         \"Texas TX\"                          \n[33] \"Nebraska NE\"                        \"Hawaii HI\"                         \n[35] \"Wyoming WY\"                         \"South Dakota SD\"                   \n[37] \"Louisiana LA\"                       \"Kentucky KY\"                       \n[39] \"Puerto Rico PR\"                     \"Arkansas AR\"                       \n[41] \"Maryland MD\"                        \"Ohio OH\"                           \n[43] \"South Carolina SC\"                  \"Massachusetts MA\"                  \n[45] \"Michigan MI\"                        \"Oregon OR\"                         \n[47] \"Montana MT\"                         \"Tennessee TN\"                      \n[49] \"Connecticut CT\"                     \"Mississippi MS\"                    \n[51] \"New Mexico NM\"                      \"Pennsylvania PA\"                   \n[53] \"New Jersey NJ\"                      \"Georgia GA\"                        \n\n\nTo understand the scope of the data set and check for things like inconsistencies, mismatched abbreviations, etc we look at all the unique values for LocationDesc and LocationAbbr. For simplicity sake I will remove the Abbreviated column in the future.\n\nunique(rawdata$YEAR) # seeing unique values of year\n\n [1] \"2017\"      \"2018\"      \"2016\"      \"2014\"      \"2012\"      \"2018-2019\"\n [7] \"2011\"      \"2013\"      \"2014-2015\" \"2017-2018\" \"2015\"      \"2019\"     \n[13] \"2016-2017\" \"2013-2014\" \"2015-2016\" \"2011-2012\" \"2012-2013\"\n\n\n\nrawdata %&gt;% filter(nchar(YEAR) &gt; 4) # confirming what the two-year values are\n\n# A tibble: 4,670 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2018-20… DE           Delaware     Tobacco … Smokeles… Current Us… BRFSS     \n 2 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n 3 2017-20… WA           Washington   Tobacco … Cigarett… Current Sm… BRFSS     \n 4 2018-20… DC           District of… Tobacco … Smokeles… Current Us… BRFSS     \n 5 2016-20… ID           Idaho        Tobacco … Cigarett… Current Sm… BRFSS     \n 6 2017-20… CO           Colorado     Tobacco … Smokeles… Current Us… BRFSS     \n 7 2018-20… TX           Texas        Tobacco … Cigarett… Current Sm… BRFSS     \n 8 2018-20… KS           Kansas       Tobacco … Cigarett… Current Sm… BRFSS     \n 9 2014-20… KS           Kansas       Tobacco … Smokeles… Current Us… BRFSS     \n10 2013-20… VT           Vermont      Tobacco … Cigarett… Current Sm… BRFSS     \n# ℹ 4,660 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\nAgain, understanding the scope of the data. Two things I’m noticing, first being that the year column is a character variable, and not a number. Second being that there are some two-year aggregations that are not necessary since we have every year between 2011 and 2019, these can likely be filtered out.\n\n# filtering down to one Locationa and a smaller year set, again to diagnose the two-year observations\nrawdata %&gt;% filter(LocationAbbr == 'IL', YEAR %in% c('2014-2015', '2014', '2015')) %&gt;% arrange(DisplayOrder)\n\n# A tibble: 152 × 31\n   YEAR  LocationAbbr LocationDesc TopicType    TopicDesc MeasureDesc DataSource\n   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 2 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 3 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 4 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 5 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 6 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 7 2014  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n 8 2015  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n 9 2015  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n10 2014  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n# ℹ 142 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\n\nrawdata %&gt;% filter(LocationAbbr == 'IL', YEAR %in% c('2014-2015', '2014', '2015')) %&gt;% \n  group_by(YEAR, Race, Gender, MeasureDesc) %&gt;% # groups picked to confirm aggregation hierarchy\n  summarize(\n    Samp = sum(Sample_Size, na.rm = TRUE) # sums sample size across the above groups\n  )\n\n`summarise()` has grouped output by 'YEAR', 'Race', 'Gender'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 78 × 5\n# Groups:   YEAR, Race, Gender [21]\n   YEAR  Race             Gender  MeasureDesc                               Samp\n   &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;                                    &lt;dbl&gt;\n 1 2014  African American Overall Current Smoking                            488\n 2 2014  African American Overall Current Use                                489\n 3 2014  All Races        Female  Current Smoking                           3459\n 4 2014  All Races        Female  Current Use                               3454\n 5 2014  All Races        Female  Frequency of Use                             0\n 6 2014  All Races        Female  Percent of Former Smokers Among Ever Sm…  1056\n 7 2014  All Races        Female  Quit Attempt in Past Year Among Every D…   225\n 8 2014  All Races        Female  Smoking Frequency                          686\n 9 2014  All Races        Female  Smoking Status                            8241\n10 2014  All Races        Female  User Status                               5486\n# ℹ 68 more rows\n\n\nThis was primarily to confirm that the two-year rows are in fact aggregations of the data we have in the single-year rows, which we can see that they are. Filtering to a specific location and timeframe also gives us an idea of data structure.\nNote: Values like Current Use and Current Smoking are intentionally different. ‘Use’ Corresponds to smokeless tobacco use, ‘Smoking’ refers to cigarrette/ non-electronic usage.\nThis is enough exploration to give me an idea of what I would want my final data set to look like after cleaning and processing.\nGoal Data Set Mapping:\n|Year|Location|Age|Race|Gender|Education|Value for Measure Desc combined with Values for Response, Surveyed then Freq\nThis would ultimately be a wider data set than we have now, and we will get there in steps.\n\nd1 &lt;- rawdata %&gt;% filter(nchar(YEAR) == 4) %&gt;% \n  mutate(\n  QA = paste(TopicDesc, MeasureDesc,Response), # concatenating all of the question-response related columns\n  Data_Value = as.numeric(Data_Value)/100, # transforming percentages into proper proportions\n  Year = as.numeric(YEAR) # Year variable was previously a string\n) %&gt;% select(Year, LocationDesc, Age, Race, Gender, Education, QA, Data_Value, Sample_Size) # reordering some but mostly getting rid of the original columns that were concatenated\n\nhead(d1)\n\n# A tibble: 6 × 9\n   Year LocationDesc   Age   Race  Gender Education QA    Data_Value Sample_Size\n  &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1  2017 Guam           All … All … Male   All Grad… Ciga…      0.3           692\n2  2018 National Medi… All … All … Overa… All Grad… Ciga…      0.161          NA\n3  2017 National Medi… All … All … Overa… All Grad… Ciga…      0.582          NA\n4  2016 Guam           All … Hisp… Overa… All Grad… Smok…      0.005         123\n5  2014 Guam           All … White Overa… All Grad… Ciga…      0.219         209\n6  2012 Indiana        All … All … Female All Grad… Ciga…      0.216        5165\n\n\nCombining Topic, Measures, and response effectively gives us a unique identifier for each answer choice for each question. Since the data set is already aggregated, we can use these combined question identifiers as variables and line them up with our categorical variables. This gets us one step closer to the rows as observations, or aggregations of observations in this case. Note that we essentially have two numeric values, the sample size and the observed proportion of interest. The sample size is not the same for each question or demographic, so we cant really solve for one without solving for both. To make this easier, I will split these up into two data sets then bring them back together at the end.\n\nd2 &lt;- d1 %&gt;% select(-Sample_Size) %&gt;% # removing sample size so the pivot over Data_Value works correctly\n  pivot_wider(names_from = QA, values_from = Data_Value) \n# transposes unique values of question-response identifier (QA) into columns, \n#essentially grouping by year, Location, and Demographics\n\nhead(d2)\n\n# A tibble: 6 × 25\n   Year LocationDesc         Age   Race  Gender Education Cigarette Use (Adult…¹\n  &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;dbl&gt;\n1  2017 Guam                 All … All … Male   All Grad…                  0.3  \n2  2018 National Median (St… All … All … Overa… All Grad…                 NA    \n3  2017 National Median (St… All … All … Overa… All Grad…                 NA    \n4  2016 Guam                 All … Hisp… Overa… All Grad…                  0.283\n5  2014 Guam                 All … White Overa… All Grad…                  0.219\n6  2012 Indiana              All … All … Female All Grad…                  0.216\n# ℹ abbreviated name: ¹​`Cigarette Use (Adults) Current Smoking NA`\n# ℹ 18 more variables: `Cigarette Use (Adults) Smoking Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Never` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) Current Use NA` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Frequency Some Days` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) User Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Former` &lt;dbl&gt;, …\n\n\n\nd3 &lt;- d1 %&gt;% select(-Data_Value) %&gt;% # removing Date_Value so the pivot over Sample_Size works correctly\n  pivot_wider(names_from = QA, values_from = Sample_Size)\n# transposes unique values of question-response identifier (QA) into columns, \n#essentially grouping by year, Location, and Demographics\n\nhead(d3)\n\n# A tibble: 6 × 25\n   Year LocationDesc         Age   Race  Gender Education Cigarette Use (Adult…¹\n  &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;dbl&gt;\n1  2017 Guam                 All … All … Male   All Grad…                    692\n2  2018 National Median (St… All … All … Overa… All Grad…                     NA\n3  2017 National Median (St… All … All … Overa… All Grad…                     NA\n4  2016 Guam                 All … Hisp… Overa… All Grad…                    123\n5  2014 Guam                 All … White Overa… All Grad…                    209\n6  2012 Indiana              All … All … Female All Grad…                   5165\n# ℹ abbreviated name: ¹​`Cigarette Use (Adults) Current Smoking NA`\n# ℹ 18 more variables: `Cigarette Use (Adults) Smoking Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Never` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) Current Use NA` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Frequency Some Days` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) User Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Former` &lt;dbl&gt;, …\n\n\n\nsum(d2[,1:6] != d3[,1:6]) # counts number of mismatches between categorical columns in the split dataset \n\n[1] 0\n\n\nThe pivot allows us to get each question-response identifier into a variable and line them up with categorical variables that are the same. Our d2 data frame is now our response frequency data set, and d3 is now our sample size data set. The last chunk is just checking that my categorical variables maintained the same structure after pivoting, which they should.\n\nd2 &lt;- d2 %&gt;%  select(c(names(d2[,1:6]),sort(names(d2[,7:25])))) %&gt;% \n  arrange(Year, LocationDesc, Age, Race)\n# Reorder the columns so like questions are next to each other, then reorders rows so like observations are next to each other\nd3 &lt;- d3 %&gt;%  select(c(names(d3[,1:6]),sort(names(d3[,7:25])))) %&gt;% \n  arrange(Year, LocationDesc, Age, Race)\n\nnames(d3)\n\n [1] \"Year\"                                                                             \n [2] \"LocationDesc\"                                                                     \n [3] \"Age\"                                                                              \n [4] \"Race\"                                                                             \n [5] \"Gender\"                                                                           \n [6] \"Education\"                                                                        \n [7] \"Cessation (Adults) Percent of Former Smokers Among Ever Smokers NA\"               \n [8] \"Cessation (Adults) Quit Attempt in Past Year Among Every Day Cigarette Smokers NA\"\n [9] \"Cigarette Use (Adults) Current Smoking NA\"                                        \n[10] \"Cigarette Use (Adults) Smoking Frequency Every Day\"                               \n[11] \"Cigarette Use (Adults) Smoking Frequency Some Days\"                               \n[12] \"Cigarette Use (Adults) Smoking Status Current\"                                    \n[13] \"Cigarette Use (Adults) Smoking Status Former\"                                     \n[14] \"Cigarette Use (Adults) Smoking Status Never\"                                      \n[15] \"E-Cigarette Use (Adults) Current Use NA\"                                          \n[16] \"E-Cigarette Use (Adults) Frequency of Use Every Day\"                              \n[17] \"E-Cigarette Use (Adults) Frequency of Use Some Days\"                              \n[18] \"E-Cigarette Use (Adults) User Status Current\"                                     \n[19] \"E-Cigarette Use (Adults) User Status Former\"                                      \n[20] \"E-Cigarette Use (Adults) User Status Never\"                                       \n[21] \"Smokeless Tobacco Use (Adults) Current Use NA\"                                    \n[22] \"Smokeless Tobacco Use (Adults) Frequency of Use Every Day\"                        \n[23] \"Smokeless Tobacco Use (Adults) Frequency of Use Some Days\"                        \n[24] \"Smokeless Tobacco Use (Adults) User Status Current\"                               \n[25] \"Smokeless Tobacco Use (Adults) User Status Not Current\"                           \n\n\nSince we lead off the question-response identifiers with the topic and Measure, we can get sort the variables alphabetically to get like questions next to each other. We do this for each data frame.\n\n# Manually renaming columns for conciseness\nnames(d3) &lt;- c(names(d3[,1:6]), \n               'QuitPctFrmr',\n               'QuitAttmpt',\n               \n               'CigCurrSmker',\n               'CigFreqDaily',\n               'CigFreqSome',\n               'CigStatCurr',\n               'CigStatFrmr',\n               'CigStatNvr',\n               \n               'EcigCurrUse',\n               'EcigFreqDaily',\n               'EcigFreqSome',\n               'EcigStatCurr',\n               'EcigStatFrmr',\n               'EcigStatNvr',\n               \n               'TobCurrUse',\n               'TobFreqDaily',\n               'TobFreqSome',\n               'TobStatCurr',\n               'TobStatNonCurr'\n               )\n\nredict &lt;- cbind(names(d3), names(d2)) # creates pseduo-dictionary\n\nnames(d2) &lt;- c(names(d3)) # copies new name convention from d3 to d2\n\n# concatenates the type of value identifier onto the columns, makes it easier to interpret\nnames(d2) &lt;- c(names(d2[,1:6]),paste0(names(d2[,7:25]), 'RespFreq')) \nnames(d3) &lt;- c(names(d3[,1:6]),paste0(names(d3[,7:25]), 'SrvCnt'))\n\nThere are shorter ways to make the column names usable, but I want column names that are also more concise. Here I have renamed each question-response identifier individually, to make them much easier to undrstand while I work with them. the redict part also creates a pseudo-data dictionary so I can remember what each renamed column corresponds to, if i forget. I rename the columns for d2 the lazy way, by copying the naming conventions from d3 since the structre is the same. Finally I add ‘RespFreq’ to the column names of my response frequency dataset, and ‘SrvCnt’ to the Survey Count sample size for my Sample Size dataset.\n\nsum(d2[,1:6] != d3[,1:6]) # one last check to make sure nothing got shuffled incorrectly\n\n[1] 0\n\nd4 &lt;- merge(d2,d3) # merging the two datasets into a master\n\nAgain, one last check to make sure everything is in the correct order, then merging the data sets into one master data set.\n\n# filter to narrow scope and see if transformations worked correctly and see what can be removed.\nd4 %&gt;% filter(Year == 2016, LocationDesc == 'Wyoming')\n\n   Year LocationDesc                Age                          Race  Gender\n1  2016      Wyoming     18 to 24 Years                     All Races Overall\n2  2016      Wyoming     18 to 44 Years                     All Races  Female\n3  2016      Wyoming     25 to 44 Years                     All Races Overall\n4  2016      Wyoming     45 to 64 Years                     All Races Overall\n5  2016      Wyoming 65 Years and Older                     All Races Overall\n6  2016      Wyoming   Age 20 and Older                     All Races Overall\n7  2016      Wyoming   Age 20 and Older                     All Races Overall\n8  2016      Wyoming   Age 20 and Older                     All Races Overall\n9  2016      Wyoming   Age 25 and Older                     All Races Overall\n10 2016      Wyoming   Age 25 and Older                     All Races Overall\n11 2016      Wyoming   Age 25 and Older                     All Races Overall\n12 2016      Wyoming           All Ages              African American Overall\n13 2016      Wyoming           All Ages                     All Races  Female\n14 2016      Wyoming           All Ages                     All Races    Male\n15 2016      Wyoming           All Ages                     All Races Overall\n16 2016      Wyoming           All Ages American Indian/Alaska Native Overall\n17 2016      Wyoming           All Ages        Asian/Pacific Islander Overall\n18 2016      Wyoming           All Ages                      Hispanic Overall\n19 2016      Wyoming           All Ages                         White Overall\n      Education QuitPctFrmrRespFreq QuitAttmptRespFreq CigCurrSmkerRespFreq\n1    All Grades                  NA                 NA                0.225\n2    All Grades                  NA                 NA                0.239\n3    All Grades                  NA                 NA                0.240\n4    All Grades                  NA                 NA                0.181\n5    All Grades                  NA                 NA                0.095\n6  &lt; 12th Grade                  NA                 NA                0.402\n7  &gt; 12th Grade                  NA                 NA                0.122\n8    12th Grade                  NA                 NA                0.267\n9  &lt; 12th Grade                  NA                 NA                0.386\n10 &gt; 12th Grade                  NA                 NA                0.122\n11   12th Grade                  NA                 NA                0.270\n12   All Grades                  NA                 NA                   NA\n13   All Grades               0.542              0.518                0.191\n14   All Grades               0.611              0.393                0.188\n15   All Grades               0.580              0.457                0.189\n16   All Grades                  NA                 NA                   NA\n17   All Grades                  NA                 NA                   NA\n18   All Grades                  NA                 NA                0.201\n19   All Grades                  NA                 NA                0.181\n   CigFreqDailyRespFreq CigFreqSomeRespFreq CigStatCurrRespFreq\n1                    NA                  NA                  NA\n2                    NA                  NA                  NA\n3                    NA                  NA                  NA\n4                    NA                  NA                  NA\n5                    NA                  NA                  NA\n6                    NA                  NA                  NA\n7                    NA                  NA                  NA\n8                    NA                  NA                  NA\n9                    NA                  NA                  NA\n10                   NA                  NA                  NA\n11                   NA                  NA                  NA\n12                   NA                  NA                  NA\n13                0.796               0.204               0.191\n14                0.745               0.255               0.188\n15                0.771               0.229               0.189\n16                   NA                  NA                  NA\n17                   NA                  NA                  NA\n18                   NA                  NA                  NA\n19                   NA                  NA                  NA\n   CigStatFrmrRespFreq CigStatNvrRespFreq EcigCurrUseRespFreq\n1                   NA                 NA               0.130\n2                   NA                 NA               0.075\n3                   NA                 NA               0.077\n4                   NA                 NA               0.033\n5                   NA                 NA               0.010\n6                   NA                 NA               0.100\n7                   NA                 NA               0.039\n8                   NA                 NA               0.066\n9                   NA                 NA               0.036\n10                  NA                 NA               0.042\n11                  NA                 NA               0.055\n12                  NA                 NA                  NA\n13               0.227              0.582               0.050\n14               0.295              0.518               0.061\n15               0.261              0.549               0.055\n16                  NA                 NA                  NA\n17                  NA                 NA                  NA\n18                  NA                 NA               0.033\n19                  NA                 NA               0.055\n   EcigFreqDailyRespFreq EcigFreqSomeRespFreq EcigStatCurrRespFreq\n1                     NA                   NA                   NA\n2                     NA                   NA                   NA\n3                     NA                   NA                   NA\n4                     NA                   NA                   NA\n5                     NA                   NA                   NA\n6                     NA                   NA                   NA\n7                     NA                   NA                   NA\n8                     NA                   NA                   NA\n9                     NA                   NA                   NA\n10                    NA                   NA                   NA\n11                    NA                   NA                   NA\n12                    NA                   NA                   NA\n13                 0.290                0.710                0.050\n14                 0.419                0.581                0.061\n15                 0.362                0.638                0.055\n16                    NA                   NA                   NA\n17                    NA                   NA                   NA\n18                    NA                   NA                   NA\n19                    NA                   NA                   NA\n   EcigStatFrmrRespFreq EcigStatNvrRespFreq TobCurrUseRespFreq\n1                    NA                  NA              0.175\n2                    NA                  NA              0.044\n3                    NA                  NA              0.139\n4                    NA                  NA              0.063\n5                    NA                  NA              0.038\n6                    NA                  NA              0.084\n7                    NA                  NA              0.079\n8                    NA                  NA              0.137\n9                    NA                  NA              0.086\n10                   NA                  NA              0.068\n11                   NA                  NA              0.127\n12                   NA                  NA                 NA\n13                0.157               0.793              0.023\n14                0.215               0.725              0.170\n15                0.186               0.758              0.098\n16                   NA                  NA                 NA\n17                   NA                  NA                 NA\n18                   NA                  NA              0.061\n19                   NA                  NA              0.097\n   TobFreqDailyRespFreq TobFreqSomeRespFreq TobStatCurrRespFreq\n1                    NA                  NA                  NA\n2                    NA                  NA                  NA\n3                    NA                  NA                  NA\n4                    NA                  NA                  NA\n5                    NA                  NA                  NA\n6                    NA                  NA                  NA\n7                    NA                  NA                  NA\n8                    NA                  NA                  NA\n9                    NA                  NA                  NA\n10                   NA                  NA                  NA\n11                   NA                  NA                  NA\n12                   NA                  NA                  NA\n13                   NA                  NA               0.023\n14                0.693               0.307               0.170\n15                0.672               0.328               0.098\n16                   NA                  NA                  NA\n17                   NA                  NA                  NA\n18                   NA                  NA                  NA\n19                   NA                  NA                  NA\n   TobStatNonCurrRespFreq QuitPctFrmrSrvCnt QuitAttmptSrvCnt CigCurrSmkerSrvCnt\n1                      NA                NA               NA                140\n2                      NA                NA               NA                469\n3                      NA                NA               NA                758\n4                      NA                NA               NA               1707\n5                      NA                NA               NA               1788\n6                      NA                NA               NA                211\n7                      NA                NA               NA               2896\n8                      NA                NA               NA               1231\n9                      NA                NA               NA                203\n10                     NA                NA               NA               2840\n11                     NA                NA               NA               1200\n12                     NA                NA               NA                 NA\n13                  0.977              1019              258               2492\n14                  0.830               951              217               1901\n15                  0.902              1970              475               4393\n16                     NA                NA               NA                 NA\n17                     NA                NA               NA                 NA\n18                     NA                NA               NA                213\n19                     NA                NA               NA               3961\n   CigFreqDailySrvCnt CigFreqSomeSrvCnt CigStatCurrSrvCnt CigStatFrmrSrvCnt\n1                  NA                NA                NA                NA\n2                  NA                NA                NA                NA\n3                  NA                NA                NA                NA\n4                  NA                NA                NA                NA\n5                  NA                NA                NA                NA\n6                  NA                NA                NA                NA\n7                  NA                NA                NA                NA\n8                  NA                NA                NA                NA\n9                  NA                NA                NA                NA\n10                 NA                NA                NA                NA\n11                 NA                NA                NA                NA\n12                 NA                NA                NA                NA\n13                339               339              2492              2492\n14                285               285              1901              1901\n15                624               624              4393              4393\n16                 NA                NA                NA                NA\n17                 NA                NA                NA                NA\n18                 NA                NA                NA                NA\n19                 NA                NA                NA                NA\n   CigStatNvrSrvCnt EcigCurrUseSrvCnt EcigFreqDailySrvCnt EcigFreqSomeSrvCnt\n1                NA               139                  NA                 NA\n2                NA               470                  NA                 NA\n3                NA               759                  NA                 NA\n4                NA              1713                  NA                 NA\n5                NA              1799                  NA                 NA\n6                NA               212                  NA                 NA\n7                NA              2902                  NA                 NA\n8                NA              1239                  NA                 NA\n9                NA               204                  NA                 NA\n10               NA              2847                  NA                 NA\n11               NA              1208                  NA                 NA\n12               NA                NA                  NA                 NA\n13             2492              2500                  76                 76\n14             1901              1910                  65                 65\n15             4393              4410                 141                141\n16               NA                NA                  NA                 NA\n17               NA                NA                  NA                 NA\n18               NA               213                  NA                 NA\n19               NA              3978                  NA                 NA\n   EcigStatCurrSrvCnt EcigStatFrmrSrvCnt EcigStatNvrSrvCnt TobCurrUseSrvCnt\n1                  NA                 NA                NA              140\n2                  NA                 NA                NA              470\n3                  NA                 NA                NA              756\n4                  NA                 NA                NA             1715\n5                  NA                 NA                NA             1799\n6                  NA                 NA                NA              213\n7                  NA                 NA                NA             2901\n8                  NA                 NA                NA             1239\n9                  NA                 NA                NA              205\n10                 NA                 NA                NA             2845\n11                 NA                 NA                NA             1208\n12                 NA                 NA                NA               NA\n13               2500               2500              2500             2503\n14               1910               1910              1910             1907\n15               4410               4410              4410             4410\n16                 NA                 NA                NA               NA\n17                 NA                 NA                NA               NA\n18                 NA                 NA                NA              214\n19                 NA                 NA                NA             3978\n   TobFreqDailySrvCnt TobFreqSomeSrvCnt TobStatCurrSrvCnt TobStatNonCurrSrvCnt\n1                  NA                NA                NA                   NA\n2                  NA                NA                NA                   NA\n3                  NA                NA                NA                   NA\n4                  NA                NA                NA                   NA\n5                  NA                NA                NA                   NA\n6                  NA                NA                NA                   NA\n7                  NA                NA                NA                   NA\n8                  NA                NA                NA                   NA\n9                  NA                NA                NA                   NA\n10                 NA                NA                NA                   NA\n11                 NA                NA                NA                   NA\n12                 NA                NA                NA                   NA\n13                 NA                NA              2503                 2503\n14                252               252              1907                 1907\n15                278               278              4410                 4410\n16                 NA                NA                NA                   NA\n17                 NA                NA                NA                   NA\n18                 NA                NA                NA                   NA\n19                 NA                NA                NA                   NA\n\n\nFiltering down to one Year and Location, so I can see how some of the aggregated values roll up and how they combine with other demographic variables. This gives me some insight into which questions were missing at what grains, and also allows me to check my merge to make sure it looks right. We finally have the data in a shape and layout that makes sense, and we see that its still kind of a mess. The intention was to maintain the separated demographic columns, and get each survey question into a column or columns. We accomplished that, however we can see that the demographics don’t stack in most cases. For example, I can see aggregated sample sizes for each Gender and the frequency for each question’s response, but I cannot see the age, education, or race makeup for each gender, or any combination of the demographic variables really. This was likely a deliberate choice for sample size constraints, nonetheless it complicates the next steps of this exercise. For a more in depth exercise, I would probably consider imputing the missing sample sizes or rates to get stratification by gender, age and race combined. However for the sake of this exercise, I will break each of the demographics into separate data sets since they have different questions considered. Once done, I will focus in on the Gender data set, since it looks like the gender demographic is the most complete across all questions.\nThe aggregated values by demographic aren’t necessary in the final version of any of these data sets, but we may need them to get the response volumes of some of the demographic values if there few responses. So, we can filter against all of the other demographic variables to only included their roll-up or overall value. First we will check for what those values are for each variable.\n\nunique(d4$Age) #'All Ages', also note some of the age bins are overlapping.\n\n[1] \"18 to 24 Years\"     \"18 to 44 Years\"     \"25 to 44 Years\"    \n[4] \"45 to 64 Years\"     \"65 Years and Older\" \"Age 20 and Older\"  \n[7] \"Age 25 and Older\"   \"All Ages\"          \n\nunique(d4$Gender) #'Overall'\n\n[1] \"Overall\" \"Female\"  \"Male\"   \n\nunique(d4$Race) #'All Races'\n\n[1] \"All Races\"                     \"African American\"             \n[3] \"American Indian/Alaska Native\" \"Asian/Pacific Islander\"       \n[5] \"Hispanic\"                      \"White\"                        \n\nunique(d4$Education) #'All Grades'\n\n[1] \"All Grades\"   \"&lt; 12th Grade\" \"&gt; 12th Grade\" \"12th Grade\"  \n\n\n\n# Age dataset, looking at rollup for all other variables except age, \n# then selecting relevant columns and filtering any last missing values\nAgeSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age != 'All Ages'\n              ,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\n# Gender Dataset. This time keeping the rollup value as well as the others,\n# but still filtering all other variables to their rollup\n# Also filtering out the national Median rows since its not the same grain as the others\n# finally removing last missing values. Will select relevant columns later.\nGndSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age == 'All Ages'\n              #,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  filter(LocationDesc != 'National Median (States and DC)',\n         !is.na(CigCurrSmkerRespFreq)\n         )\n\n# Race dataset, looking at rollup for all other variables except Race, \n# then selecting relevant columns and filtering any last missing values\nRaceSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age == 'All Ages'\n              ,Gender == 'Overall'\n              #,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\n# Education dataset, looking at rollup for all other variables except Education and Age,\n#since age actually is startified with education (interestingly, not vice-versa)\n# then selecting relevant columns and filtering any last missing values\nEdSrv &lt;- d4 %&gt;% filter(1==1\n              #,Age == 'All Ages'\n              ,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education != 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\nNow to focus in on the gender data set. I did not select specific columns earlier because the treatment was a bit different than the other data sets. This data set in particular has a lot more opportunity for imputing and other transformations, but for the simplicity of this exercise we will select only a few interesting and complete columns. Also one last note, the Gender variables has three values, Male, Female, and Overall. Since the Response frequency is the true variable of interest, I’m going to leave the “Overall” values in. Under other circumstances I would remove these and calculate overall values only when they apply, that way there aren’t “repeated” observations in the data set. However, the response frequency variables are pre-processed and rounded, so it would only unnecessarily complicate things to try and calculate response counts to get a proper aggregation.\n\n#Selecting relevant columns for the remainder of the exercise\nGndSrv &lt;- GndSrv %&gt;% \n  select(\n    Year\n    ,LocationDesc\n    ,Gender\n    ,QuitAttmptRespFreq\n    ,CigStatCurrRespFreq\n    ,CigStatFrmrRespFreq\n    ,CigStatNvrRespFreq\n    )\n\nI have decided to focus in on the questions for Cigarette usage, namely status and if they have attempted to quit in the last year. I also kept Year and Location since those variables were the least problematic, and of course Gender since this is the Gender-specific dataset.\n\nGndSrv %&gt;% group_by(Gender) %&gt;% #grouped by general to see potential differences in mean.\n  summarize( # getting mean values for each of my questions\n    AvgQuitAttempt = mean(QuitAttmptRespFreq)\n    ,AvgCurentSmokers = mean(CigStatCurrRespFreq)\n    ,AvgFormerSmokers = mean(CigStatFrmrRespFreq)\n    ,AvgNeverSmoked = mean(CigStatNvrRespFreq)\n  )\n\n# A tibble: 3 × 5\n  Gender  AvgQuitAttempt AvgCurentSmokers AvgFormerSmokers AvgNeverSmoked\n  &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1 Female           0.526            0.163            0.215          0.622\n2 Male             0.510            0.203            0.283          0.514\n3 Overall          0.518            0.183            0.248          0.569\n\n\nLet’s start by getting some quick descriptive statistics. Notice that this table will get the average Rate for each of the questions across all states and timeframes. The number of combinations we have between categorical variables, especially since the Location variable has so many categories, complicates our ability to get a closer look. Faceting and framing will help with this, as we will see in a moment.\n\nbox1o &lt;- GndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_boxplot() +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\nbox1o\n\n\n\n\n\n\n\nhist1o &lt;- GndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\nhist1o\n\n\n\n\n\n\n\n\nthe boxplots should show the distribution of the Percent of Current Smokers across the different locations. I have filter to 2011, the earliest year in the data set, and 2019, the latest year. Here I wanted to see if there would be a large difference from the beginning of the timeframe to the end, and for the most part we can see that there is. It appears as though we have a lower rate of “Current” Smokers from beginning to end. the distribution is not particularly clean, but it also looks to be shifting left while keeping some outliers on the higher end to make for a bit of a right skew.\n\nGndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=QuitAttmptRespFreq)) + # Quit Attempt Rate observed\n  geom_boxplot() +\n  labs(x = 'Percent of Quit Attempts\\nAmong Current Smokers') +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\n\n\n\n\n\n\n\nGndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=QuitAttmptRespFreq)) + # Quit Attempt Rate observed\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Quit Attempts\\nAmong Current Smokers', y = NULL) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\n\n\n\n\n\n\n\n\nGoing into the quit rate, it’s a bit of a different story here. There is maybe a little bit of a left shift in the Males and Overall, but it is far less pronounced than it was for the percent of current smokers. This makes sense, since it is the quit rate for current smokers, meaning we would expect it to either stay the same or potentially increase over time. This begs the question if the decrease in current smokers is coming from more people quitting or fewer people starting. The distribution is also all over the place, so there is a wider variance. This will be apparent in the next visual.\n\naniplot &lt;- GndSrv %&gt;% filter(Gender != 'Overall') %&gt;% # wanting Male and Female only\n  ggplot(aes(x=CigStatCurrRespFreq, y = QuitAttmptRespFreq)) + #plotting current smokers against quit attempt rate\n  geom_point(aes(frame = Year, color = Gender, ids = LocationDesc)) + #frame for animation, ids for hover text, color for comparison\n  labs(x = 'Percent of Current Smokers', y = 'Percent of Quit Attempts\\nAmong Current Smokers')\n\nWarning in geom_point(aes(frame = Year, color = Gender, ids = LocationDesc)):\nIgnoring unknown aesthetics: frame and ids\n\nggplotly(aniplot) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animaton, also slows down the slider when i click play\n\n\n\n\n\nThis was a pretty telling visual, so a few observations. First, there tend to be a higher rate of men who are current smokers over women. Second, the distribution is slowly but surely shifting left over time, it was not simply a fluke between 2011 and 2019 earlier. Finally, the wide variance in the percent of quit attempts is apparent, with the amount of vertical movement each point had in the visual. Still the distribution did not look to have any large shift up or down over time, which tells me the rate of people quitting is roughly the same, so more like the decrease in current smokers is happening because fewer people ever start smoking.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in later chunks\n  ggplot(aes(x = CigStatCurrRespFreq, y = fct_reorder(LocationDesc, CigStatCurrRespFreq), frame = Year)) + #fct_reorder orders location by Current smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of Current Smokers', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nMostly wanting to see if any outliers are more apparent here. Guam looks to have the highest rate of people who smoke, but they also have wide swings, likely due to a smaller sample size. Worth noting that the ranking of rates by state does not change significantly over time either.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in other chunks\n  ggplot(aes(x = CigStatFrmrRespFreq, y = fct_reorder(LocationDesc, CigStatFrmrRespFreq), frame = Year)) + #fct_reorder orders location by former smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of Former Smokers', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nThe rate of Former smokers does not look like it shares the trend. Again, if we saw an increase in the rates of people quitting we might expect the rate of Former Smokers to increase over time, but we don’t really see that much here.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in other chunks\n  ggplot(aes(x = CigStatNvrRespFreq, y = fct_reorder(LocationDesc, CigStatNvrRespFreq), frame = Year)) + #fct_reorder orders location by Never smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of People Who Have Never Smoked', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nHere we can see that increase in rate of people who have never smoked, though admittedly it still seems less pronounced than it was for the decrease in Current smokers. Still, it’s in line with the hypothesis that people are more likely are more likely to have never started smoking than they were in the past, and that is why we are seeing such a decrease in the rate of Current Smokers. Not to end on a darker note, but this is also likely a result of those who were Current Smokers in the earlier years dying off at a higher rate than non-smokers. This may suggest that younger people are much less likely to start smoking as well."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#exploring-the-original-data-set",
    "href": "cdcdata-exercise/cdcdata-exercise.html#exploring-the-original-data-set",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "The CDC data set I have chosen for this exercise is the Tobacco Usage Behavioral Risk Factor data set. The data comes from a survey administered by the CDC as part of the State Tobacco Activities Tracking and Evaluation (STATE) system. The data set contains information about the year the survey was administered, respondent location (State or territory), demographics (namely Race, Age, Education, and Gender), and it asks questions about a person’s tobacco usage status (current, former, never used), their cessation status (whether they have quit or attempted to quit in the last year), and the frequency that they use tobacco for those that do (daily, some days). The usage questions are asked for three different types of Tobacco products, Cigarettes, Smokeless Tobacco, and E-cigarettes.\n\npacman::p_load(here, tidyverse, skimr, plotly, synthpop, patchwork)\n\n\nrawdata &lt;- read_csv('cdc-data-raw.csv')\n\nRows: 43341 Columns: 31\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (25): YEAR, LocationAbbr, LocationDesc, TopicType, TopicDesc, MeasureDes...\ndbl  (6): Data_Value, Data_Value_Std_Err, Low_Confidence_Limit, High_Confide...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nstr(rawdata) # getting an idea for data structure\n\nspc_tbl_ [43,341 × 31] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ YEAR                      : chr [1:43341] \"2017\" \"2018\" \"2017\" \"2016\" ...\n $ LocationAbbr              : chr [1:43341] \"GU\" \"US\" \"US\" \"GU\" ...\n $ LocationDesc              : chr [1:43341] \"Guam\" \"National Median (States and DC)\" \"National Median (States and DC)\" \"Guam\" ...\n $ TopicType                 : chr [1:43341] \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" \"Tobacco Use – Survey Data\" ...\n $ TopicDesc                 : chr [1:43341] \"Cigarette Use (Adults)\" \"Cigarette Use (Adults)\" \"Cigarette Use (Adults)\" \"Smokeless Tobacco Use (Adults)\" ...\n $ MeasureDesc               : chr [1:43341] \"Current Smoking\" \"Smoking Status\" \"Smoking Status\" \"Current Use\" ...\n $ DataSource                : chr [1:43341] \"BRFSS\" \"BRFSS\" \"BRFSS\" \"BRFSS\" ...\n $ Response                  : chr [1:43341] NA \"Current\" \"Never\" NA ...\n $ Data_Value_Unit           : chr [1:43341] \"%\" \"%\" \"%\" \"%\" ...\n $ Data_Value_Type           : chr [1:43341] \"Percentage\" \"Percentage\" \"Percentage\" \"Percentage\" ...\n $ Data_Value                : num [1:43341] 30 16.1 58.2 0.5 21.9 21.6 2.8 3.6 29.2 14.8 ...\n $ Data_Value_Footnote_Symbol: chr [1:43341] NA NA NA NA ...\n $ Data_Value_Footnote       : chr [1:43341] NA NA NA NA ...\n $ Data_Value_Std_Err        : num [1:43341] 2.3 NA NA 0.4 4.9 0.8 0.3 0.4 2.4 0.6 ...\n $ Low_Confidence_Limit      : num [1:43341] 25.4 NA NA 0 12.4 20 2.2 2.7 24.5 13.6 ...\n $ High_Confidence_Limit     : num [1:43341] 34.6 NA NA 1.2 31.4 23.2 3.4 4.5 33.9 16 ...\n $ Sample_Size               : num [1:43341] 692 NA NA 123 209 ...\n $ Gender                    : chr [1:43341] \"Male\" \"Overall\" \"Overall\" \"Overall\" ...\n $ Race                      : chr [1:43341] \"All Races\" \"All Races\" \"All Races\" \"Hispanic\" ...\n $ Age                       : chr [1:43341] \"All Ages\" \"All Ages\" \"All Ages\" \"All Ages\" ...\n $ Education                 : chr [1:43341] \"All Grades\" \"All Grades\" \"All Grades\" \"All Grades\" ...\n $ GeoLocation               : chr [1:43341] \"(13.444304, 144.793731)\" NA NA \"(13.444304, 144.793731)\" ...\n $ TopicTypeId               : chr [1:43341] \"BEH\" \"BEH\" \"BEH\" \"BEH\" ...\n $ TopicId                   : chr [1:43341] \"100BEH\" \"100BEH\" \"100BEH\" \"150BEH\" ...\n $ MeasureId                 : chr [1:43341] \"110CSA\" \"165SSA\" \"165SSA\" \"177SCU\" ...\n $ StratificationID1         : chr [1:43341] \"2GEN\" \"1GEN\" \"1GEN\" \"1GEN\" ...\n $ StratificationID2         : chr [1:43341] \"8AGE\" \"8AGE\" \"8AGE\" \"8AGE\" ...\n $ StratificationID3         : chr [1:43341] \"6RAC\" \"6RAC\" \"6RAC\" \"4RAC\" ...\n $ StratificationID4         : chr [1:43341] \"6EDU\" \"6EDU\" \"6EDU\" \"6EDU\" ...\n $ SubMeasureID              : chr [1:43341] \"BRF21\" \"BRF27\" \"BRF28\" \"BRF69\" ...\n $ DisplayOrder              : num [1:43341] 21 27 28 69 22 21 77 71 26 27 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   YEAR = col_character(),\n  ..   LocationAbbr = col_character(),\n  ..   LocationDesc = col_character(),\n  ..   TopicType = col_character(),\n  ..   TopicDesc = col_character(),\n  ..   MeasureDesc = col_character(),\n  ..   DataSource = col_character(),\n  ..   Response = col_character(),\n  ..   Data_Value_Unit = col_character(),\n  ..   Data_Value_Type = col_character(),\n  ..   Data_Value = col_double(),\n  ..   Data_Value_Footnote_Symbol = col_character(),\n  ..   Data_Value_Footnote = col_character(),\n  ..   Data_Value_Std_Err = col_double(),\n  ..   Low_Confidence_Limit = col_double(),\n  ..   High_Confidence_Limit = col_double(),\n  ..   Sample_Size = col_double(),\n  ..   Gender = col_character(),\n  ..   Race = col_character(),\n  ..   Age = col_character(),\n  ..   Education = col_character(),\n  ..   GeoLocation = col_character(),\n  ..   TopicTypeId = col_character(),\n  ..   TopicId = col_character(),\n  ..   MeasureId = col_character(),\n  ..   StratificationID1 = col_character(),\n  ..   StratificationID2 = col_character(),\n  ..   StratificationID3 = col_character(),\n  ..   StratificationID4 = col_character(),\n  ..   SubMeasureID = col_character(),\n  ..   DisplayOrder = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(rawdata) # summary statistics\n\n     YEAR           LocationAbbr       LocationDesc        TopicType        \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  TopicDesc         MeasureDesc         DataSource          Response        \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n Data_Value_Unit    Data_Value_Type      Data_Value   \n Length:43341       Length:43341       Min.   : 0.00  \n Class :character   Class :character   1st Qu.: 5.10  \n Mode  :character   Mode  :character   Median :17.10  \n                                       Mean   :25.36  \n                                       3rd Qu.:38.90  \n                                       Max.   :99.90  \n                                       NA's   :2117   \n Data_Value_Footnote_Symbol Data_Value_Footnote Data_Value_Std_Err\n Length:43341               Length:43341        Min.   : 0.000    \n Class :character           Class :character    1st Qu.: 0.600    \n Mode  :character           Mode  :character    Median : 1.100    \n                                                Mean   : 1.711    \n                                                3rd Qu.: 2.200    \n                                                Max.   :16.600    \n                                                NA's   :2195      \n Low_Confidence_Limit High_Confidence_Limit  Sample_Size       Gender         \n Min.   : 0.00        Min.   :  0.00        Min.   :   50   Length:43341      \n 1st Qu.: 3.20        1st Qu.:  6.90        1st Qu.:  480   Class :character  \n Median :14.30        Median : 19.80        Median : 1798   Mode  :character  \n Mean   :22.01        Mean   : 28.68        Mean   : 3050                     \n 3rd Qu.:30.50        3rd Qu.: 47.40        3rd Qu.: 4203                     \n Max.   :99.80        Max.   :100.00        Max.   :40726                     \n NA's   :2195         NA's   :2195          NA's   :2195                      \n     Race               Age             Education         GeoLocation       \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n TopicTypeId          TopicId           MeasureId         StratificationID1 \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n StratificationID2  StratificationID3  StratificationID4  SubMeasureID      \n Length:43341       Length:43341       Length:43341       Length:43341      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  DisplayOrder  \n Min.   : 5.00  \n 1st Qu.:24.00  \n Median :51.00  \n Mean   :47.08  \n 3rd Qu.:71.00  \n Max.   :81.00  \n                \n\nskim(rawdata) # primarily to get completion rates\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n43341\n\n\nNumber of columns\n31\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n25\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nYEAR\n0\n1.00\n4\n9\n0\n17\n0\n\n\nLocationAbbr\n0\n1.00\n2\n2\n0\n54\n0\n\n\nLocationDesc\n0\n1.00\n4\n31\n0\n54\n0\n\n\nTopicType\n0\n1.00\n25\n25\n0\n1\n0\n\n\nTopicDesc\n0\n1.00\n18\n30\n0\n4\n0\n\n\nMeasureDesc\n0\n1.00\n11\n59\n0\n10\n0\n\n\nDataSource\n0\n1.00\n5\n5\n0\n1\n0\n\n\nResponse\n28323\n0.35\n5\n11\n0\n6\n0\n\n\nData_Value_Unit\n0\n1.00\n1\n1\n0\n1\n0\n\n\nData_Value_Type\n0\n1.00\n10\n10\n0\n1\n0\n\n\nData_Value_Footnote_Symbol\n41224\n0.05\n1\n1\n0\n1\n0\n\n\nData_Value_Footnote\n41224\n0.05\n71\n71\n0\n1\n0\n\n\nGender\n0\n1.00\n4\n7\n0\n3\n0\n\n\nRace\n0\n1.00\n5\n29\n0\n6\n0\n\n\nAge\n0\n1.00\n8\n18\n0\n8\n0\n\n\nEducation\n0\n1.00\n10\n12\n0\n4\n0\n\n\nGeoLocation\n78\n1.00\n23\n41\n0\n53\n0\n\n\nTopicTypeId\n0\n1.00\n3\n3\n0\n1\n0\n\n\nTopicId\n0\n1.00\n6\n6\n0\n4\n0\n\n\nMeasureId\n0\n1.00\n6\n6\n0\n14\n0\n\n\nStratificationID1\n0\n1.00\n4\n4\n0\n3\n0\n\n\nStratificationID2\n0\n1.00\n4\n4\n0\n8\n0\n\n\nStratificationID3\n0\n1.00\n4\n4\n0\n6\n0\n\n\nStratificationID4\n0\n1.00\n4\n4\n0\n4\n0\n\n\nSubMeasureID\n0\n1.00\n5\n5\n0\n53\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nData_Value\n2117\n0.95\n25.36\n25.00\n0\n5.1\n17.1\n38.9\n99.9\n▇▃▂▁▁\n\n\nData_Value_Std_Err\n2195\n0.95\n1.71\n1.72\n0\n0.6\n1.1\n2.2\n16.6\n▇▁▁▁▁\n\n\nLow_Confidence_Limit\n2195\n0.95\n22.01\n24.14\n0\n3.2\n14.3\n30.5\n99.8\n▇▂▂▁▁\n\n\nHigh_Confidence_Limit\n2195\n0.95\n28.68\n26.21\n0\n6.9\n19.8\n47.4\n100.0\n▇▃▂▂▁\n\n\nSample_Size\n2195\n0.95\n3050.17\n3798.87\n50\n480.0\n1798.0\n4203.0\n40726.0\n▇▁▁▁▁\n\n\nDisplayOrder\n0\n1.00\n47.08\n24.49\n5\n24.0\n51.0\n71.0\n81.0\n▂▇▁▂▇\n\n\n\n\n\n\nhead(rawdata, 20) # getting first 20 rows\n\n# A tibble: 20 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2017     GU           Guam         Tobacco … Cigarett… Current Sm… BRFSS     \n 2 2018     US           National Me… Tobacco … Cigarett… Smoking St… BRFSS     \n 3 2017     US           National Me… Tobacco … Cigarett… Smoking St… BRFSS     \n 4 2016     GU           Guam         Tobacco … Smokeles… Current Use BRFSS     \n 5 2014     GU           Guam         Tobacco … Cigarett… Current Sm… BRFSS     \n 6 2012     IN           Indiana      Tobacco … Cigarett… Current Sm… BRFSS     \n 7 2018-20… DE           Delaware     Tobacco … Smokeles… Current Us… BRFSS     \n 8 2011     MO           Missouri     Tobacco … Smokeles… Current Use BRFSS     \n 9 2013     ME           Maine        Tobacco … Cigarett… Smoking Fr… BRFSS     \n10 2017     WA           Washington   Tobacco … Cigarett… Smoking St… BRFSS     \n11 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n12 2011     MN           Minnesota    Tobacco … Smokeles… User Status BRFSS     \n13 2018     VA           Virginia     Tobacco … Cigarett… Smoking Fr… BRFSS     \n14 2014     MN           Minnesota    Tobacco … Cigarett… Smoking St… BRFSS     \n15 2017-20… WA           Washington   Tobacco … Cigarett… Current Sm… BRFSS     \n16 2017     UT           Utah         Tobacco … Smokeles… Current Use BRFSS     \n17 2012     CA           California   Tobacco … Smokeles… Frequency … BRFSS     \n18 2015     FL           Florida      Tobacco … Smokeles… Current Use BRFSS     \n19 2015     RI           Rhode Island Tobacco … Smokeles… Current Use BRFSS     \n20 2019     AZ           Arizona      Tobacco … Smokeles… Current Use BRFSS     \n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, StratificationID1 &lt;chr&gt;,\n#   StratificationID2 &lt;chr&gt;, StratificationID3 &lt;chr&gt;, …\n\n\nThe data set is not the most analyst friendly format. Generally the ideal format would be variables in the columns, observations in the rows. This data set has variables across several rows, and it is mostly aggregated. The questions are broken up by the different response options, which are spread across rows even though the values are proportions and therefore the samples sizes listed for the different response options for a single question come from the same audience. Aggregations grouped by each of the demographics, locations, and timeframes are also in the rows, similar to a pivot table, so getting things like summary statistics will require filtering these aggregated values. Some survey question variables have missing values, but with the data in this format it’s difficult to make much of them, but we will come back to that. There are a few variables that only serve to give information about the data set itself, like Data_Value_Unit, which only contains one value, “Percentage”, to inform that the Value column is a percentage. Variable like this (Zero or Near Zero Variance) can be removed, since they give no information about the individual observations.\nVariables to be removed: Zero Variance variables: Data_Value_Unit, Data_Value_Type, DataSource, TopicType, TopicTypeId NZV: Data_Value_Footnote_Symbol, Data_Value_Footnote\n\nrawdata %&gt;% filter(is.na(Data_Value)) #looking at nulls to determine why they are there\n\n# A tibble: 2,117 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n 2 2016     IA           Iowa         Tobacco … Smokeles… Frequency … BRFSS     \n 3 2014     ID           Idaho        Tobacco … Smokeles… Frequency … BRFSS     \n 4 2019     MI           Michigan     Tobacco … Smokeles… Frequency … BRFSS     \n 5 2016-20… MA           Massachuset… Tobacco … Smokeles… Current Us… BRFSS     \n 6 2013-20… DC           District of… Tobacco … Cigarett… Current Sm… BRFSS     \n 7 2013     ME           Maine        Tobacco … Cigarett… Current Sm… BRFSS     \n 8 2011     AZ           Arizona      Tobacco … Smokeles… Frequency … BRFSS     \n 9 2018     AL           Alabama      Tobacco … Smokeles… Frequency … BRFSS     \n10 2018     NC           North Carol… Tobacco … Smokeles… Frequency … BRFSS     \n# ℹ 2,107 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\nMissing values from the Data_Value column occur when the sample size is not large enough to report results. Some of these missing values may have the opportunity for imputation, and filtering these could cause some confusion in the actual structure of the data (complete and consistent combinations of categorical values) so we won’t filter anything out just yet. note this is evidenced by the Footnote column with the following message: “Data in these cells have been suppressed because of a small sample size.”\n\nunique(paste(rawdata$LocationDesc,rawdata$LocationAbbr)) # paste concatenates columns, unique to get distinct values.\n\n [1] \"Guam GU\"                            \"National Median (States and DC) US\"\n [3] \"Indiana IN\"                         \"Delaware DE\"                       \n [5] \"Missouri MO\"                        \"Maine ME\"                          \n [7] \"Washington WA\"                      \"Illinois IL\"                       \n [9] \"Minnesota MN\"                       \"Virginia VA\"                       \n[11] \"Utah UT\"                            \"California CA\"                     \n[13] \"Florida FL\"                         \"Rhode Island RI\"                   \n[15] \"Arizona AZ\"                         \"District of Columbia DC\"           \n[17] \"Kansas KS\"                          \"Nevada NV\"                         \n[19] \"Alabama AL\"                         \"West Virginia WV\"                  \n[21] \"Wisconsin WI\"                       \"Oklahoma OK\"                       \n[23] \"New York NY\"                        \"Iowa IA\"                           \n[25] \"Colorado CO\"                        \"Idaho ID\"                          \n[27] \"Alaska AK\"                          \"North Dakota ND\"                   \n[29] \"North Carolina NC\"                  \"New Hampshire NH\"                  \n[31] \"Vermont VT\"                         \"Texas TX\"                          \n[33] \"Nebraska NE\"                        \"Hawaii HI\"                         \n[35] \"Wyoming WY\"                         \"South Dakota SD\"                   \n[37] \"Louisiana LA\"                       \"Kentucky KY\"                       \n[39] \"Puerto Rico PR\"                     \"Arkansas AR\"                       \n[41] \"Maryland MD\"                        \"Ohio OH\"                           \n[43] \"South Carolina SC\"                  \"Massachusetts MA\"                  \n[45] \"Michigan MI\"                        \"Oregon OR\"                         \n[47] \"Montana MT\"                         \"Tennessee TN\"                      \n[49] \"Connecticut CT\"                     \"Mississippi MS\"                    \n[51] \"New Mexico NM\"                      \"Pennsylvania PA\"                   \n[53] \"New Jersey NJ\"                      \"Georgia GA\"                        \n\n\nTo understand the scope of the data set and check for things like inconsistencies, mismatched abbreviations, etc we look at all the unique values for LocationDesc and LocationAbbr. For simplicity sake I will remove the Abbreviated column in the future.\n\nunique(rawdata$YEAR) # seeing unique values of year\n\n [1] \"2017\"      \"2018\"      \"2016\"      \"2014\"      \"2012\"      \"2018-2019\"\n [7] \"2011\"      \"2013\"      \"2014-2015\" \"2017-2018\" \"2015\"      \"2019\"     \n[13] \"2016-2017\" \"2013-2014\" \"2015-2016\" \"2011-2012\" \"2012-2013\"\n\n\n\nrawdata %&gt;% filter(nchar(YEAR) &gt; 4) # confirming what the two-year values are\n\n# A tibble: 4,670 × 31\n   YEAR     LocationAbbr LocationDesc TopicType TopicDesc MeasureDesc DataSource\n   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2018-20… DE           Delaware     Tobacco … Smokeles… Current Us… BRFSS     \n 2 2014-20… IL           Illinois     Tobacco … Cigarett… Current Sm… BRFSS     \n 3 2017-20… WA           Washington   Tobacco … Cigarett… Current Sm… BRFSS     \n 4 2018-20… DC           District of… Tobacco … Smokeles… Current Us… BRFSS     \n 5 2016-20… ID           Idaho        Tobacco … Cigarett… Current Sm… BRFSS     \n 6 2017-20… CO           Colorado     Tobacco … Smokeles… Current Us… BRFSS     \n 7 2018-20… TX           Texas        Tobacco … Cigarett… Current Sm… BRFSS     \n 8 2018-20… KS           Kansas       Tobacco … Cigarett… Current Sm… BRFSS     \n 9 2014-20… KS           Kansas       Tobacco … Smokeles… Current Us… BRFSS     \n10 2013-20… VT           Vermont      Tobacco … Cigarett… Current Sm… BRFSS     \n# ℹ 4,660 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\nAgain, understanding the scope of the data. Two things I’m noticing, first being that the year column is a character variable, and not a number. Second being that there are some two-year aggregations that are not necessary since we have every year between 2011 and 2019, these can likely be filtered out.\n\n# filtering down to one Locationa and a smaller year set, again to diagnose the two-year observations\nrawdata %&gt;% filter(LocationAbbr == 'IL', YEAR %in% c('2014-2015', '2014', '2015')) %&gt;% arrange(DisplayOrder)\n\n# A tibble: 152 × 31\n   YEAR  LocationAbbr LocationDesc TopicType    TopicDesc MeasureDesc DataSource\n   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     \n 1 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 2 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 3 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 4 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 5 2015  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 6 2014  IL           Illinois     Tobacco Use… Cessatio… Percent of… BRFSS     \n 7 2014  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n 8 2015  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n 9 2015  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n10 2014  IL           Illinois     Tobacco Use… Cessatio… Quit Attem… BRFSS     \n# ℹ 142 more rows\n# ℹ 24 more variables: Response &lt;chr&gt;, Data_Value_Unit &lt;chr&gt;,\n#   Data_Value_Type &lt;chr&gt;, Data_Value &lt;dbl&gt;, Data_Value_Footnote_Symbol &lt;chr&gt;,\n#   Data_Value_Footnote &lt;chr&gt;, Data_Value_Std_Err &lt;dbl&gt;,\n#   Low_Confidence_Limit &lt;dbl&gt;, High_Confidence_Limit &lt;dbl&gt;, Sample_Size &lt;dbl&gt;,\n#   Gender &lt;chr&gt;, Race &lt;chr&gt;, Age &lt;chr&gt;, Education &lt;chr&gt;, GeoLocation &lt;chr&gt;,\n#   TopicTypeId &lt;chr&gt;, TopicId &lt;chr&gt;, MeasureId &lt;chr&gt;, …\n\n\n\nrawdata %&gt;% filter(LocationAbbr == 'IL', YEAR %in% c('2014-2015', '2014', '2015')) %&gt;% \n  group_by(YEAR, Race, Gender, MeasureDesc) %&gt;% # groups picked to confirm aggregation hierarchy\n  summarize(\n    Samp = sum(Sample_Size, na.rm = TRUE) # sums sample size across the above groups\n  )\n\n`summarise()` has grouped output by 'YEAR', 'Race', 'Gender'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 78 × 5\n# Groups:   YEAR, Race, Gender [21]\n   YEAR  Race             Gender  MeasureDesc                               Samp\n   &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;                                    &lt;dbl&gt;\n 1 2014  African American Overall Current Smoking                            488\n 2 2014  African American Overall Current Use                                489\n 3 2014  All Races        Female  Current Smoking                           3459\n 4 2014  All Races        Female  Current Use                               3454\n 5 2014  All Races        Female  Frequency of Use                             0\n 6 2014  All Races        Female  Percent of Former Smokers Among Ever Sm…  1056\n 7 2014  All Races        Female  Quit Attempt in Past Year Among Every D…   225\n 8 2014  All Races        Female  Smoking Frequency                          686\n 9 2014  All Races        Female  Smoking Status                            8241\n10 2014  All Races        Female  User Status                               5486\n# ℹ 68 more rows\n\n\nThis was primarily to confirm that the two-year rows are in fact aggregations of the data we have in the single-year rows, which we can see that they are. Filtering to a specific location and timeframe also gives us an idea of data structure.\nNote: Values like Current Use and Current Smoking are intentionally different. ‘Use’ Corresponds to smokeless tobacco use, ‘Smoking’ refers to cigarrette/ non-electronic usage.\nThis is enough exploration to give me an idea of what I would want my final data set to look like after cleaning and processing.\nGoal Data Set Mapping:\n|Year|Location|Age|Race|Gender|Education|Value for Measure Desc combined with Values for Response, Surveyed then Freq\nThis would ultimately be a wider data set than we have now, and we will get there in steps.\n\nd1 &lt;- rawdata %&gt;% filter(nchar(YEAR) == 4) %&gt;% \n  mutate(\n  QA = paste(TopicDesc, MeasureDesc,Response), # concatenating all of the question-response related columns\n  Data_Value = as.numeric(Data_Value)/100, # transforming percentages into proper proportions\n  Year = as.numeric(YEAR) # Year variable was previously a string\n) %&gt;% select(Year, LocationDesc, Age, Race, Gender, Education, QA, Data_Value, Sample_Size) # reordering some but mostly getting rid of the original columns that were concatenated\n\nhead(d1)\n\n# A tibble: 6 × 9\n   Year LocationDesc   Age   Race  Gender Education QA    Data_Value Sample_Size\n  &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1  2017 Guam           All … All … Male   All Grad… Ciga…      0.3           692\n2  2018 National Medi… All … All … Overa… All Grad… Ciga…      0.161          NA\n3  2017 National Medi… All … All … Overa… All Grad… Ciga…      0.582          NA\n4  2016 Guam           All … Hisp… Overa… All Grad… Smok…      0.005         123\n5  2014 Guam           All … White Overa… All Grad… Ciga…      0.219         209\n6  2012 Indiana        All … All … Female All Grad… Ciga…      0.216        5165\n\n\nCombining Topic, Measures, and response effectively gives us a unique identifier for each answer choice for each question. Since the data set is already aggregated, we can use these combined question identifiers as variables and line them up with our categorical variables. This gets us one step closer to the rows as observations, or aggregations of observations in this case. Note that we essentially have two numeric values, the sample size and the observed proportion of interest. The sample size is not the same for each question or demographic, so we cant really solve for one without solving for both. To make this easier, I will split these up into two data sets then bring them back together at the end.\n\nd2 &lt;- d1 %&gt;% select(-Sample_Size) %&gt;% # removing sample size so the pivot over Data_Value works correctly\n  pivot_wider(names_from = QA, values_from = Data_Value) \n# transposes unique values of question-response identifier (QA) into columns, \n#essentially grouping by year, Location, and Demographics\n\nhead(d2)\n\n# A tibble: 6 × 25\n   Year LocationDesc         Age   Race  Gender Education Cigarette Use (Adult…¹\n  &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;dbl&gt;\n1  2017 Guam                 All … All … Male   All Grad…                  0.3  \n2  2018 National Median (St… All … All … Overa… All Grad…                 NA    \n3  2017 National Median (St… All … All … Overa… All Grad…                 NA    \n4  2016 Guam                 All … Hisp… Overa… All Grad…                  0.283\n5  2014 Guam                 All … White Overa… All Grad…                  0.219\n6  2012 Indiana              All … All … Female All Grad…                  0.216\n# ℹ abbreviated name: ¹​`Cigarette Use (Adults) Current Smoking NA`\n# ℹ 18 more variables: `Cigarette Use (Adults) Smoking Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Never` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) Current Use NA` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Frequency Some Days` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) User Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Former` &lt;dbl&gt;, …\n\n\n\nd3 &lt;- d1 %&gt;% select(-Data_Value) %&gt;% # removing Date_Value so the pivot over Sample_Size works correctly\n  pivot_wider(names_from = QA, values_from = Sample_Size)\n# transposes unique values of question-response identifier (QA) into columns, \n#essentially grouping by year, Location, and Demographics\n\nhead(d3)\n\n# A tibble: 6 × 25\n   Year LocationDesc         Age   Race  Gender Education Cigarette Use (Adult…¹\n  &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;dbl&gt;\n1  2017 Guam                 All … All … Male   All Grad…                    692\n2  2018 National Median (St… All … All … Overa… All Grad…                     NA\n3  2017 National Median (St… All … All … Overa… All Grad…                     NA\n4  2016 Guam                 All … Hisp… Overa… All Grad…                    123\n5  2014 Guam                 All … White Overa… All Grad…                    209\n6  2012 Indiana              All … All … Female All Grad…                   5165\n# ℹ abbreviated name: ¹​`Cigarette Use (Adults) Current Smoking NA`\n# ℹ 18 more variables: `Cigarette Use (Adults) Smoking Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Never` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) Current Use NA` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Frequency Some Days` &lt;dbl&gt;,\n#   `Smokeless Tobacco Use (Adults) User Status Current` &lt;dbl&gt;,\n#   `Cigarette Use (Adults) Smoking Status Former` &lt;dbl&gt;, …\n\n\n\nsum(d2[,1:6] != d3[,1:6]) # counts number of mismatches between categorical columns in the split dataset \n\n[1] 0\n\n\nThe pivot allows us to get each question-response identifier into a variable and line them up with categorical variables that are the same. Our d2 data frame is now our response frequency data set, and d3 is now our sample size data set. The last chunk is just checking that my categorical variables maintained the same structure after pivoting, which they should.\n\nd2 &lt;- d2 %&gt;%  select(c(names(d2[,1:6]),sort(names(d2[,7:25])))) %&gt;% \n  arrange(Year, LocationDesc, Age, Race)\n# Reorder the columns so like questions are next to each other, then reorders rows so like observations are next to each other\nd3 &lt;- d3 %&gt;%  select(c(names(d3[,1:6]),sort(names(d3[,7:25])))) %&gt;% \n  arrange(Year, LocationDesc, Age, Race)\n\nnames(d3)\n\n [1] \"Year\"                                                                             \n [2] \"LocationDesc\"                                                                     \n [3] \"Age\"                                                                              \n [4] \"Race\"                                                                             \n [5] \"Gender\"                                                                           \n [6] \"Education\"                                                                        \n [7] \"Cessation (Adults) Percent of Former Smokers Among Ever Smokers NA\"               \n [8] \"Cessation (Adults) Quit Attempt in Past Year Among Every Day Cigarette Smokers NA\"\n [9] \"Cigarette Use (Adults) Current Smoking NA\"                                        \n[10] \"Cigarette Use (Adults) Smoking Frequency Every Day\"                               \n[11] \"Cigarette Use (Adults) Smoking Frequency Some Days\"                               \n[12] \"Cigarette Use (Adults) Smoking Status Current\"                                    \n[13] \"Cigarette Use (Adults) Smoking Status Former\"                                     \n[14] \"Cigarette Use (Adults) Smoking Status Never\"                                      \n[15] \"E-Cigarette Use (Adults) Current Use NA\"                                          \n[16] \"E-Cigarette Use (Adults) Frequency of Use Every Day\"                              \n[17] \"E-Cigarette Use (Adults) Frequency of Use Some Days\"                              \n[18] \"E-Cigarette Use (Adults) User Status Current\"                                     \n[19] \"E-Cigarette Use (Adults) User Status Former\"                                      \n[20] \"E-Cigarette Use (Adults) User Status Never\"                                       \n[21] \"Smokeless Tobacco Use (Adults) Current Use NA\"                                    \n[22] \"Smokeless Tobacco Use (Adults) Frequency of Use Every Day\"                        \n[23] \"Smokeless Tobacco Use (Adults) Frequency of Use Some Days\"                        \n[24] \"Smokeless Tobacco Use (Adults) User Status Current\"                               \n[25] \"Smokeless Tobacco Use (Adults) User Status Not Current\"                           \n\n\nSince we lead off the question-response identifiers with the topic and Measure, we can get sort the variables alphabetically to get like questions next to each other. We do this for each data frame.\n\n# Manually renaming columns for conciseness\nnames(d3) &lt;- c(names(d3[,1:6]), \n               'QuitPctFrmr',\n               'QuitAttmpt',\n               \n               'CigCurrSmker',\n               'CigFreqDaily',\n               'CigFreqSome',\n               'CigStatCurr',\n               'CigStatFrmr',\n               'CigStatNvr',\n               \n               'EcigCurrUse',\n               'EcigFreqDaily',\n               'EcigFreqSome',\n               'EcigStatCurr',\n               'EcigStatFrmr',\n               'EcigStatNvr',\n               \n               'TobCurrUse',\n               'TobFreqDaily',\n               'TobFreqSome',\n               'TobStatCurr',\n               'TobStatNonCurr'\n               )\n\nredict &lt;- cbind(names(d3), names(d2)) # creates pseduo-dictionary\n\nnames(d2) &lt;- c(names(d3)) # copies new name convention from d3 to d2\n\n# concatenates the type of value identifier onto the columns, makes it easier to interpret\nnames(d2) &lt;- c(names(d2[,1:6]),paste0(names(d2[,7:25]), 'RespFreq')) \nnames(d3) &lt;- c(names(d3[,1:6]),paste0(names(d3[,7:25]), 'SrvCnt'))\n\nThere are shorter ways to make the column names usable, but I want column names that are also more concise. Here I have renamed each question-response identifier individually, to make them much easier to undrstand while I work with them. the redict part also creates a pseudo-data dictionary so I can remember what each renamed column corresponds to, if i forget. I rename the columns for d2 the lazy way, by copying the naming conventions from d3 since the structre is the same. Finally I add ‘RespFreq’ to the column names of my response frequency dataset, and ‘SrvCnt’ to the Survey Count sample size for my Sample Size dataset.\n\nsum(d2[,1:6] != d3[,1:6]) # one last check to make sure nothing got shuffled incorrectly\n\n[1] 0\n\nd4 &lt;- merge(d2,d3) # merging the two datasets into a master\n\nAgain, one last check to make sure everything is in the correct order, then merging the data sets into one master data set.\n\n# filter to narrow scope and see if transformations worked correctly and see what can be removed.\nd4 %&gt;% filter(Year == 2016, LocationDesc == 'Wyoming')\n\n   Year LocationDesc                Age                          Race  Gender\n1  2016      Wyoming     18 to 24 Years                     All Races Overall\n2  2016      Wyoming     18 to 44 Years                     All Races  Female\n3  2016      Wyoming     25 to 44 Years                     All Races Overall\n4  2016      Wyoming     45 to 64 Years                     All Races Overall\n5  2016      Wyoming 65 Years and Older                     All Races Overall\n6  2016      Wyoming   Age 20 and Older                     All Races Overall\n7  2016      Wyoming   Age 20 and Older                     All Races Overall\n8  2016      Wyoming   Age 20 and Older                     All Races Overall\n9  2016      Wyoming   Age 25 and Older                     All Races Overall\n10 2016      Wyoming   Age 25 and Older                     All Races Overall\n11 2016      Wyoming   Age 25 and Older                     All Races Overall\n12 2016      Wyoming           All Ages              African American Overall\n13 2016      Wyoming           All Ages                     All Races  Female\n14 2016      Wyoming           All Ages                     All Races    Male\n15 2016      Wyoming           All Ages                     All Races Overall\n16 2016      Wyoming           All Ages American Indian/Alaska Native Overall\n17 2016      Wyoming           All Ages        Asian/Pacific Islander Overall\n18 2016      Wyoming           All Ages                      Hispanic Overall\n19 2016      Wyoming           All Ages                         White Overall\n      Education QuitPctFrmrRespFreq QuitAttmptRespFreq CigCurrSmkerRespFreq\n1    All Grades                  NA                 NA                0.225\n2    All Grades                  NA                 NA                0.239\n3    All Grades                  NA                 NA                0.240\n4    All Grades                  NA                 NA                0.181\n5    All Grades                  NA                 NA                0.095\n6  &lt; 12th Grade                  NA                 NA                0.402\n7  &gt; 12th Grade                  NA                 NA                0.122\n8    12th Grade                  NA                 NA                0.267\n9  &lt; 12th Grade                  NA                 NA                0.386\n10 &gt; 12th Grade                  NA                 NA                0.122\n11   12th Grade                  NA                 NA                0.270\n12   All Grades                  NA                 NA                   NA\n13   All Grades               0.542              0.518                0.191\n14   All Grades               0.611              0.393                0.188\n15   All Grades               0.580              0.457                0.189\n16   All Grades                  NA                 NA                   NA\n17   All Grades                  NA                 NA                   NA\n18   All Grades                  NA                 NA                0.201\n19   All Grades                  NA                 NA                0.181\n   CigFreqDailyRespFreq CigFreqSomeRespFreq CigStatCurrRespFreq\n1                    NA                  NA                  NA\n2                    NA                  NA                  NA\n3                    NA                  NA                  NA\n4                    NA                  NA                  NA\n5                    NA                  NA                  NA\n6                    NA                  NA                  NA\n7                    NA                  NA                  NA\n8                    NA                  NA                  NA\n9                    NA                  NA                  NA\n10                   NA                  NA                  NA\n11                   NA                  NA                  NA\n12                   NA                  NA                  NA\n13                0.796               0.204               0.191\n14                0.745               0.255               0.188\n15                0.771               0.229               0.189\n16                   NA                  NA                  NA\n17                   NA                  NA                  NA\n18                   NA                  NA                  NA\n19                   NA                  NA                  NA\n   CigStatFrmrRespFreq CigStatNvrRespFreq EcigCurrUseRespFreq\n1                   NA                 NA               0.130\n2                   NA                 NA               0.075\n3                   NA                 NA               0.077\n4                   NA                 NA               0.033\n5                   NA                 NA               0.010\n6                   NA                 NA               0.100\n7                   NA                 NA               0.039\n8                   NA                 NA               0.066\n9                   NA                 NA               0.036\n10                  NA                 NA               0.042\n11                  NA                 NA               0.055\n12                  NA                 NA                  NA\n13               0.227              0.582               0.050\n14               0.295              0.518               0.061\n15               0.261              0.549               0.055\n16                  NA                 NA                  NA\n17                  NA                 NA                  NA\n18                  NA                 NA               0.033\n19                  NA                 NA               0.055\n   EcigFreqDailyRespFreq EcigFreqSomeRespFreq EcigStatCurrRespFreq\n1                     NA                   NA                   NA\n2                     NA                   NA                   NA\n3                     NA                   NA                   NA\n4                     NA                   NA                   NA\n5                     NA                   NA                   NA\n6                     NA                   NA                   NA\n7                     NA                   NA                   NA\n8                     NA                   NA                   NA\n9                     NA                   NA                   NA\n10                    NA                   NA                   NA\n11                    NA                   NA                   NA\n12                    NA                   NA                   NA\n13                 0.290                0.710                0.050\n14                 0.419                0.581                0.061\n15                 0.362                0.638                0.055\n16                    NA                   NA                   NA\n17                    NA                   NA                   NA\n18                    NA                   NA                   NA\n19                    NA                   NA                   NA\n   EcigStatFrmrRespFreq EcigStatNvrRespFreq TobCurrUseRespFreq\n1                    NA                  NA              0.175\n2                    NA                  NA              0.044\n3                    NA                  NA              0.139\n4                    NA                  NA              0.063\n5                    NA                  NA              0.038\n6                    NA                  NA              0.084\n7                    NA                  NA              0.079\n8                    NA                  NA              0.137\n9                    NA                  NA              0.086\n10                   NA                  NA              0.068\n11                   NA                  NA              0.127\n12                   NA                  NA                 NA\n13                0.157               0.793              0.023\n14                0.215               0.725              0.170\n15                0.186               0.758              0.098\n16                   NA                  NA                 NA\n17                   NA                  NA                 NA\n18                   NA                  NA              0.061\n19                   NA                  NA              0.097\n   TobFreqDailyRespFreq TobFreqSomeRespFreq TobStatCurrRespFreq\n1                    NA                  NA                  NA\n2                    NA                  NA                  NA\n3                    NA                  NA                  NA\n4                    NA                  NA                  NA\n5                    NA                  NA                  NA\n6                    NA                  NA                  NA\n7                    NA                  NA                  NA\n8                    NA                  NA                  NA\n9                    NA                  NA                  NA\n10                   NA                  NA                  NA\n11                   NA                  NA                  NA\n12                   NA                  NA                  NA\n13                   NA                  NA               0.023\n14                0.693               0.307               0.170\n15                0.672               0.328               0.098\n16                   NA                  NA                  NA\n17                   NA                  NA                  NA\n18                   NA                  NA                  NA\n19                   NA                  NA                  NA\n   TobStatNonCurrRespFreq QuitPctFrmrSrvCnt QuitAttmptSrvCnt CigCurrSmkerSrvCnt\n1                      NA                NA               NA                140\n2                      NA                NA               NA                469\n3                      NA                NA               NA                758\n4                      NA                NA               NA               1707\n5                      NA                NA               NA               1788\n6                      NA                NA               NA                211\n7                      NA                NA               NA               2896\n8                      NA                NA               NA               1231\n9                      NA                NA               NA                203\n10                     NA                NA               NA               2840\n11                     NA                NA               NA               1200\n12                     NA                NA               NA                 NA\n13                  0.977              1019              258               2492\n14                  0.830               951              217               1901\n15                  0.902              1970              475               4393\n16                     NA                NA               NA                 NA\n17                     NA                NA               NA                 NA\n18                     NA                NA               NA                213\n19                     NA                NA               NA               3961\n   CigFreqDailySrvCnt CigFreqSomeSrvCnt CigStatCurrSrvCnt CigStatFrmrSrvCnt\n1                  NA                NA                NA                NA\n2                  NA                NA                NA                NA\n3                  NA                NA                NA                NA\n4                  NA                NA                NA                NA\n5                  NA                NA                NA                NA\n6                  NA                NA                NA                NA\n7                  NA                NA                NA                NA\n8                  NA                NA                NA                NA\n9                  NA                NA                NA                NA\n10                 NA                NA                NA                NA\n11                 NA                NA                NA                NA\n12                 NA                NA                NA                NA\n13                339               339              2492              2492\n14                285               285              1901              1901\n15                624               624              4393              4393\n16                 NA                NA                NA                NA\n17                 NA                NA                NA                NA\n18                 NA                NA                NA                NA\n19                 NA                NA                NA                NA\n   CigStatNvrSrvCnt EcigCurrUseSrvCnt EcigFreqDailySrvCnt EcigFreqSomeSrvCnt\n1                NA               139                  NA                 NA\n2                NA               470                  NA                 NA\n3                NA               759                  NA                 NA\n4                NA              1713                  NA                 NA\n5                NA              1799                  NA                 NA\n6                NA               212                  NA                 NA\n7                NA              2902                  NA                 NA\n8                NA              1239                  NA                 NA\n9                NA               204                  NA                 NA\n10               NA              2847                  NA                 NA\n11               NA              1208                  NA                 NA\n12               NA                NA                  NA                 NA\n13             2492              2500                  76                 76\n14             1901              1910                  65                 65\n15             4393              4410                 141                141\n16               NA                NA                  NA                 NA\n17               NA                NA                  NA                 NA\n18               NA               213                  NA                 NA\n19               NA              3978                  NA                 NA\n   EcigStatCurrSrvCnt EcigStatFrmrSrvCnt EcigStatNvrSrvCnt TobCurrUseSrvCnt\n1                  NA                 NA                NA              140\n2                  NA                 NA                NA              470\n3                  NA                 NA                NA              756\n4                  NA                 NA                NA             1715\n5                  NA                 NA                NA             1799\n6                  NA                 NA                NA              213\n7                  NA                 NA                NA             2901\n8                  NA                 NA                NA             1239\n9                  NA                 NA                NA              205\n10                 NA                 NA                NA             2845\n11                 NA                 NA                NA             1208\n12                 NA                 NA                NA               NA\n13               2500               2500              2500             2503\n14               1910               1910              1910             1907\n15               4410               4410              4410             4410\n16                 NA                 NA                NA               NA\n17                 NA                 NA                NA               NA\n18                 NA                 NA                NA              214\n19                 NA                 NA                NA             3978\n   TobFreqDailySrvCnt TobFreqSomeSrvCnt TobStatCurrSrvCnt TobStatNonCurrSrvCnt\n1                  NA                NA                NA                   NA\n2                  NA                NA                NA                   NA\n3                  NA                NA                NA                   NA\n4                  NA                NA                NA                   NA\n5                  NA                NA                NA                   NA\n6                  NA                NA                NA                   NA\n7                  NA                NA                NA                   NA\n8                  NA                NA                NA                   NA\n9                  NA                NA                NA                   NA\n10                 NA                NA                NA                   NA\n11                 NA                NA                NA                   NA\n12                 NA                NA                NA                   NA\n13                 NA                NA              2503                 2503\n14                252               252              1907                 1907\n15                278               278              4410                 4410\n16                 NA                NA                NA                   NA\n17                 NA                NA                NA                   NA\n18                 NA                NA                NA                   NA\n19                 NA                NA                NA                   NA\n\n\nFiltering down to one Year and Location, so I can see how some of the aggregated values roll up and how they combine with other demographic variables. This gives me some insight into which questions were missing at what grains, and also allows me to check my merge to make sure it looks right. We finally have the data in a shape and layout that makes sense, and we see that its still kind of a mess. The intention was to maintain the separated demographic columns, and get each survey question into a column or columns. We accomplished that, however we can see that the demographics don’t stack in most cases. For example, I can see aggregated sample sizes for each Gender and the frequency for each question’s response, but I cannot see the age, education, or race makeup for each gender, or any combination of the demographic variables really. This was likely a deliberate choice for sample size constraints, nonetheless it complicates the next steps of this exercise. For a more in depth exercise, I would probably consider imputing the missing sample sizes or rates to get stratification by gender, age and race combined. However for the sake of this exercise, I will break each of the demographics into separate data sets since they have different questions considered. Once done, I will focus in on the Gender data set, since it looks like the gender demographic is the most complete across all questions.\nThe aggregated values by demographic aren’t necessary in the final version of any of these data sets, but we may need them to get the response volumes of some of the demographic values if there few responses. So, we can filter against all of the other demographic variables to only included their roll-up or overall value. First we will check for what those values are for each variable.\n\nunique(d4$Age) #'All Ages', also note some of the age bins are overlapping.\n\n[1] \"18 to 24 Years\"     \"18 to 44 Years\"     \"25 to 44 Years\"    \n[4] \"45 to 64 Years\"     \"65 Years and Older\" \"Age 20 and Older\"  \n[7] \"Age 25 and Older\"   \"All Ages\"          \n\nunique(d4$Gender) #'Overall'\n\n[1] \"Overall\" \"Female\"  \"Male\"   \n\nunique(d4$Race) #'All Races'\n\n[1] \"All Races\"                     \"African American\"             \n[3] \"American Indian/Alaska Native\" \"Asian/Pacific Islander\"       \n[5] \"Hispanic\"                      \"White\"                        \n\nunique(d4$Education) #'All Grades'\n\n[1] \"All Grades\"   \"&lt; 12th Grade\" \"&gt; 12th Grade\" \"12th Grade\"  \n\n\n\n# Age dataset, looking at rollup for all other variables except age, \n# then selecting relevant columns and filtering any last missing values\nAgeSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age != 'All Ages'\n              ,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\n# Gender Dataset. This time keeping the rollup value as well as the others,\n# but still filtering all other variables to their rollup\n# Also filtering out the national Median rows since its not the same grain as the others\n# finally removing last missing values. Will select relevant columns later.\nGndSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age == 'All Ages'\n              #,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  filter(LocationDesc != 'National Median (States and DC)',\n         !is.na(CigCurrSmkerRespFreq)\n         )\n\n# Race dataset, looking at rollup for all other variables except Race, \n# then selecting relevant columns and filtering any last missing values\nRaceSrv &lt;- d4 %&gt;% filter(1==1\n              ,Age == 'All Ages'\n              ,Gender == 'Overall'\n              #,Race == 'All Races'\n              ,Education == 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\n# Education dataset, looking at rollup for all other variables except Education and Age,\n#since age actually is startified with education (interestingly, not vice-versa)\n# then selecting relevant columns and filtering any last missing values\nEdSrv &lt;- d4 %&gt;% filter(1==1\n              #,Age == 'All Ages'\n              ,Gender == 'Overall'\n              ,Race == 'All Races'\n              ,Education != 'All Grades'\n              ) %&gt;% \n  select(\n    Year,\n    LocationDesc,\n    Age,\n    CigCurrSmkerRespFreq, \n    CigCurrSmkerSrvCnt, \n    TobCurrUseRespFreq, \n    TobCurrUseSrvCnt\n    ) %&gt;% \n  filter(!is.na(CigCurrSmkerRespFreq))\n\nNow to focus in on the gender data set. I did not select specific columns earlier because the treatment was a bit different than the other data sets. This data set in particular has a lot more opportunity for imputing and other transformations, but for the simplicity of this exercise we will select only a few interesting and complete columns. Also one last note, the Gender variables has three values, Male, Female, and Overall. Since the Response frequency is the true variable of interest, I’m going to leave the “Overall” values in. Under other circumstances I would remove these and calculate overall values only when they apply, that way there aren’t “repeated” observations in the data set. However, the response frequency variables are pre-processed and rounded, so it would only unnecessarily complicate things to try and calculate response counts to get a proper aggregation.\n\n#Selecting relevant columns for the remainder of the exercise\nGndSrv &lt;- GndSrv %&gt;% \n  select(\n    Year\n    ,LocationDesc\n    ,Gender\n    ,QuitAttmptRespFreq\n    ,CigStatCurrRespFreq\n    ,CigStatFrmrRespFreq\n    ,CigStatNvrRespFreq\n    )\n\nI have decided to focus in on the questions for Cigarette usage, namely status and if they have attempted to quit in the last year. I also kept Year and Location since those variables were the least problematic, and of course Gender since this is the Gender-specific dataset.\n\nGndSrv %&gt;% group_by(Gender) %&gt;% #grouped by general to see potential differences in mean.\n  summarize( # getting mean values for each of my questions\n    AvgQuitAttempt = mean(QuitAttmptRespFreq)\n    ,AvgCurentSmokers = mean(CigStatCurrRespFreq)\n    ,AvgFormerSmokers = mean(CigStatFrmrRespFreq)\n    ,AvgNeverSmoked = mean(CigStatNvrRespFreq)\n  )\n\n# A tibble: 3 × 5\n  Gender  AvgQuitAttempt AvgCurentSmokers AvgFormerSmokers AvgNeverSmoked\n  &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1 Female           0.526            0.163            0.215          0.622\n2 Male             0.510            0.203            0.283          0.514\n3 Overall          0.518            0.183            0.248          0.569\n\n\nLet’s start by getting some quick descriptive statistics. Notice that this table will get the average Rate for each of the questions across all states and timeframes. The number of combinations we have between categorical variables, especially since the Location variable has so many categories, complicates our ability to get a closer look. Faceting and framing will help with this, as we will see in a moment.\n\nbox1o &lt;- GndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_boxplot() +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\nbox1o\n\n\n\n\n\n\n\nhist1o &lt;- GndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\nhist1o\n\n\n\n\n\n\n\n\nthe boxplots should show the distribution of the Percent of Current Smokers across the different locations. I have filter to 2011, the earliest year in the data set, and 2019, the latest year. Here I wanted to see if there would be a large difference from the beginning of the timeframe to the end, and for the most part we can see that there is. It appears as though we have a lower rate of “Current” Smokers from beginning to end. the distribution is not particularly clean, but it also looks to be shifting left while keeping some outliers on the higher end to make for a bit of a right skew.\n\nGndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=QuitAttmptRespFreq)) + # Quit Attempt Rate observed\n  geom_boxplot() +\n  labs(x = 'Percent of Quit Attempts\\nAmong Current Smokers') +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\n\n\n\n\n\n\n\nGndSrv %&gt;% filter(Year %in% c(max(Year), min(Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=QuitAttmptRespFreq)) + # Quit Attempt Rate observed\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Quit Attempts\\nAmong Current Smokers', y = NULL) +\n  facet_grid(cols = vars(Year), rows = vars(Gender)) #want year and gender stratification\n\n\n\n\n\n\n\n\nGoing into the quit rate, it’s a bit of a different story here. There is maybe a little bit of a left shift in the Males and Overall, but it is far less pronounced than it was for the percent of current smokers. This makes sense, since it is the quit rate for current smokers, meaning we would expect it to either stay the same or potentially increase over time. This begs the question if the decrease in current smokers is coming from more people quitting or fewer people starting. The distribution is also all over the place, so there is a wider variance. This will be apparent in the next visual.\n\naniplot &lt;- GndSrv %&gt;% filter(Gender != 'Overall') %&gt;% # wanting Male and Female only\n  ggplot(aes(x=CigStatCurrRespFreq, y = QuitAttmptRespFreq)) + #plotting current smokers against quit attempt rate\n  geom_point(aes(frame = Year, color = Gender, ids = LocationDesc)) + #frame for animation, ids for hover text, color for comparison\n  labs(x = 'Percent of Current Smokers', y = 'Percent of Quit Attempts\\nAmong Current Smokers')\n\nWarning in geom_point(aes(frame = Year, color = Gender, ids = LocationDesc)):\nIgnoring unknown aesthetics: frame and ids\n\nggplotly(aniplot) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animaton, also slows down the slider when i click play\n\n\n\n\n\nThis was a pretty telling visual, so a few observations. First, there tend to be a higher rate of men who are current smokers over women. Second, the distribution is slowly but surely shifting left over time, it was not simply a fluke between 2011 and 2019 earlier. Finally, the wide variance in the percent of quit attempts is apparent, with the amount of vertical movement each point had in the visual. Still the distribution did not look to have any large shift up or down over time, which tells me the rate of people quitting is roughly the same, so more like the decrease in current smokers is happening because fewer people ever start smoking.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in later chunks\n  ggplot(aes(x = CigStatCurrRespFreq, y = fct_reorder(LocationDesc, CigStatCurrRespFreq), frame = Year)) + #fct_reorder orders location by Current smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of Current Smokers', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nMostly wanting to see if any outliers are more apparent here. Guam looks to have the highest rate of people who smoke, but they also have wide swings, likely due to a smaller sample size. Worth noting that the ranking of rates by state does not change significantly over time either.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in other chunks\n  ggplot(aes(x = CigStatFrmrRespFreq, y = fct_reorder(LocationDesc, CigStatFrmrRespFreq), frame = Year)) + #fct_reorder orders location by former smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of Former Smokers', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nThe rate of Former smokers does not look like it shares the trend. Again, if we saw an increase in the rates of people quitting we might expect the rate of Former Smokers to increase over time, but we don’t really see that much here.\n\naniplot2 &lt;- GndSrv %&gt;% filter(Gender == 'Overall') %&gt;% #Filtering to just overall, couldnt get stacked chart to work\n  select(Year, LocationDesc, CigStatCurrRespFreq, CigStatFrmrRespFreq, CigStatNvrRespFreq) %&gt;% # mostly for copy and paste in other chunks\n  ggplot(aes(x = CigStatNvrRespFreq, y = fct_reorder(LocationDesc, CigStatNvrRespFreq), frame = Year)) + #fct_reorder orders location by Never smoker rate\n  geom_col(position = 'identity') +\n  theme(axis.text.y = element_text(size=6, angle = 25)) +\n  labs(x = 'Percent of People Who Have Never Smoked', y = 'US Location')\n\nggplotly(aniplot2) %&gt;% animation_opts(1500, 1000) # feeds ggplot into animation, also slows down the slider when i click play\n\n\n\n\n\nHere we can see that increase in rate of people who have never smoked, though admittedly it still seems less pronounced than it was for the decrease in Current smokers. Still, it’s in line with the hypothesis that people are more likely are more likely to have never started smoking than they were in the past, and that is why we are seeing such a decrease in the rate of Current Smokers. Not to end on a darker note, but this is also likely a result of those who were Current Smokers in the earlier years dying off at a higher rate than non-smokers. This may suggest that younger people are much less likely to start smoking as well."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#generating-and-comparing-synthetic-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#generating-and-comparing-synthetic-data",
    "title": "CDC Data Exercise",
    "section": "Generating and comparing synthetic data",
    "text": "Generating and comparing synthetic data\n\nThis section contributed by Sean O’Sullivan\n\n# generating a reproducible set of synthetic data based on the characteristics\n# of the original GndSrv data frame\nset.seed(42)\nsynth &lt;- syn(GndSrv, method = c(\"\",\"\",\"\",\"norm\", \"norm\", \"norm\", \"norm\"))\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nsyn.GndSrv &lt;- as.data.frame(synth[3])\n\nIn order to generate some synthetic data that takes on the characteristics of the original data set we utilized the synthpop package’s syn() function and saved the returned data as a new data frame.\n\n# converting a few columns in both the original and synthetic data frames\n# to factor so that the summary function output can be better interpreted\nsyn.GndSrv_fac &lt;- as.data.frame(synth[3]) %&gt;%\n  mutate(syn.Year = as.factor(syn.Year),\n         syn.LocationDesc = as.factor(syn.LocationDesc),\n         syn.Gender = as.factor(syn.Gender))\nGndSrv_fac &lt;- GndSrv %&gt;%\n  mutate(Year = as.factor(Year),\n         LocationDesc = as.factor(LocationDesc),\n         Gender = as.factor(Gender))\n\n# outputting variable summaries for both data frames\nskim(GndSrv_fac)\n\n\nData summary\n\n\nName\nGndSrv_fac\n\n\nNumber of rows\n1428\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nYear\n0\n1\nFALSE\n9\n201: 159, 201: 159, 201: 159, 201: 159\n\n\nLocationDesc\n0\n1\nFALSE\n53\nAla: 27, Ala: 27, Ari: 27, Ark: 27\n\n\nGender\n0\n1\nFALSE\n3\nFem: 476, Mal: 476, Ove: 476\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nQuitAttmptRespFreq\n0\n1\n0.52\n0.05\n0.36\n0.48\n0.52\n0.54\n0.72\n▁▆▇▂▁\n\n\nCigStatCurrRespFreq\n0\n1\n0.18\n0.04\n0.06\n0.15\n0.18\n0.21\n0.36\n▁▇▇▂▁\n\n\nCigStatFrmrRespFreq\n0\n1\n0.25\n0.04\n0.10\n0.22\n0.25\n0.28\n0.35\n▁▂▆▇▂\n\n\nCigStatNvrRespFreq\n0\n1\n0.57\n0.07\n0.39\n0.52\n0.56\n0.61\n0.82\n▂▇▇▂▁\n\n\n\n\nskim(syn.GndSrv_fac)\n\n\nData summary\n\n\nName\nsyn.GndSrv_fac\n\n\nNumber of rows\n1428\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nsyn.Year\n0\n1\nFALSE\n9\n201: 159, 201: 159, 201: 159, 201: 159\n\n\nsyn.LocationDesc\n0\n1\nFALSE\n53\nAla: 27, Ala: 27, Ari: 27, Ark: 27\n\n\nsyn.Gender\n0\n1\nFALSE\n3\nFem: 476, Mal: 476, Ove: 476\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsyn.QuitAttmptRespFreq\n0\n1\n0.52\n0.05\n0.38\n0.48\n0.52\n0.55\n0.71\n▁▇▇▂▁\n\n\nsyn.CigStatCurrRespFreq\n0\n1\n0.18\n0.04\n0.04\n0.15\n0.18\n0.21\n0.33\n▁▅▇▃▁\n\n\nsyn.CigStatFrmrRespFreq\n0\n1\n0.25\n0.04\n0.11\n0.22\n0.25\n0.28\n0.36\n▁▂▇▆▁\n\n\nsyn.CigStatNvrRespFreq\n0\n1\n0.57\n0.07\n0.38\n0.52\n0.56\n0.62\n0.84\n▁▇▇▂▁\n\n\n\n\n# comparing overall distributions\n# subset just the numeric features\nGndSrv_num &lt;- GndSrv %&gt;%\n  select(where(is.numeric)) %&gt;% \n  select(-Year)\n\n# function for plotting multiple columns iteratively through the data frame\nhistfunco &lt;- function(colname) {\ncolname &lt;- sym(colname)\nplot &lt;- GndSrv_num %&gt;% \n  ggplot(aes(x = !!colname)) +\n  geom_histogram(aes(y = after_stat(density)), col =\"white\", fill = \"aquamarine2\", bins = 30) +\n  geom_density(col = \"aquamarine3\") +\n  ylab(NULL) +\n  theme(axis.text.y=element_blank(),\n  axis.ticks.y=element_blank())\n}\n\n# iterating through the numeric columns of the data frame with the above function and plotting the results\nhistso &lt;- lapply(colnames(GndSrv_num), FUN = histfunco)\n\n# subset just the numeric features\nsyn.GndSrv_num &lt;- syn.GndSrv %&gt;%\n  select(where(is.numeric)) %&gt;% \n  select(-syn.Year)\n\n# function for plotting multiple columns iteratively through the data frame\nhistfuncs &lt;- function(colname) {\ncolname &lt;- sym(colname)\nplot &lt;- syn.GndSrv_num %&gt;% \n  ggplot(aes(x = !!colname)) +\n  geom_histogram(aes(y = after_stat(density)), col =\"white\", fill = \"coral2\", bins = 30) +\n  geom_density(col = \"coral\") +\n  ylab(NULL) +\n  theme(axis.text.y=element_blank(),\n  axis.ticks.y=element_blank())\n}\n\n# iterating through the numeric columns of the data frame with the above function and plotting the results\nhistss &lt;- lapply(colnames(syn.GndSrv_num), FUN = histfuncs)\n\n#plot all plots\nwrap_plots(wrap_plots(histso, nrow = 1), wrap_plots(histss, nrow = 1), nrow = 2)\n\n\n\n\n\n\n\n\nComparing the overall distribution of the data in the original data frame and the new synthesized data frame we see that the overall shape of the data is very similar. Both data frames contain the same number of total observations, as well as the same composition of observations for each grouping within each factor (year, state, gender). Similarly we see nearly identical mean, standard deviation, and percentiles across each of the numeric variables. We can also see that the variable distributions when plotted are effectively visually indistinguishable overall.\n\n# comparing distribution plots for percent of current smokers by year and gender\nbox1s &lt;- syn.GndSrv %&gt;% filter(syn.Year %in% c(max(syn.Year), min(syn.Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=syn.CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_boxplot() +\n  labs(x = 'Percent of Current Smokers', title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(syn.Year), rows = vars(syn.Gender)) #want year and gender stratification\nwrap_plots(box1o, box1s, ncol = 1)\n\n\n\n\n\n\n\nhist1s &lt;- syn.GndSrv %&gt;% filter(syn.Year %in% c(max(syn.Year), min(syn.Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=syn.CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Synthetic Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(syn.Year), rows = vars(syn.Gender)) #want year and gender stratification\nwrap_plots(hist1o, hist1s, ncol = 1)\n\n\n\n\n\n\n\n\nHowever, when we examine the distributions of these features along combinations of Year and Gender we start to see some deviation from the underlying relationships at such a low level. They aren’t entirely dissimilar, but they differ significantly. This is perhaps because the synthpop package lacks enough data points for each combination of strata to create anything that isn’t simply a 1:1 copy of the original data.\nWhen attempting to use the syn.strata() function provided by the synthpop package we are able to better maintain relationships at a lower level by supplying a strata argument with the variable Year, but even the sample size for each strata of Year is too small for the function’s liking – as can be seen below.\n\n# generating a reproducible set of synthetic data based on the characteristics\n# of the original GndSrv data frame\nset.seed(42)\nsynth2 &lt;- syn.strata(GndSrv, strata = \"Year\", method = c(\"\",\"\",\"\",\"norm\", \"norm\", \"norm\", \"norm\"))\n\nNumber of observations in strata (original data):\n2011 2012 2013 2014 2015 2016 2017 2018 2019 \n 159  159  159  159  159  159  159  159  156 \nCAUTION: In the original data some strata (2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019) have limited numbers of observations.\nWe advise that there should be at least 170 observations (100 + 10 * no. of variables\nused in prediction).\n\nm = 1, strata = 2011\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 181 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2012\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 165 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2013\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 146 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2014\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 166 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2015\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 176 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2016\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 137 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2017\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 133 will be generated from original data of size 159.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2018\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (159) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nm = 1, strata = 2019\n-----------------------------------------------------\nCAUTION: Your data set has fewer observations (156) than we advise.\nWe suggest that there should be at least 170 observations\n(100 + 10 * no. of variables used in modelling the data).\nPlease check your synthetic data carefully with functions\ncompare(), utility.tab(), and utility.gen().\n\nSample(s) of size 165 will be generated from original data of size 156.\n\n\nVariable(s): LocationDesc, Gender have been changed for synthesis from character to factor.\n\nVariable(s): Year numeric but with only 1 or fewer distinct values turned into factor(s) for synthesis.\n\nVariable Year has only one value so its method has been changed to \"constant\".\nVariable Year removed as predictor because only one value.\n\nSynthesis\n-----------\n Year LocationDesc Gender QuitAttmptRespFreq CigStatCurrRespFreq CigStatFrmrRespFreq CigStatNvrRespFreq\n\nsyn.GndSrv2 &lt;- as.data.frame(synth2[3])\n\n# comparing distribution plots for percent of current smokers by year and gender\nbox2s &lt;- syn.GndSrv2 %&gt;% filter(syn.Year %in% c(max(syn.Year), min(syn.Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=syn.CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_boxplot() +\n  labs(x = 'Percent of Current Smokers', title = \"Original Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(syn.Year), rows = vars(syn.Gender)) #want year and gender stratification\nwrap_plots(box1o, box2s, ncol = 1)\n\n\n\n\n\n\n\nhist2s &lt;- syn.GndSrv2 %&gt;% filter(syn.Year %in% c(max(syn.Year), min(syn.Year))) %&gt;% #gets earliest and latest year\n  ggplot(aes(x=syn.CigStatCurrRespFreq)) + # Get current smoker rate\n  geom_histogram(bins = 30) +\n  labs(x = 'Percent of Current Smokers', y = NULL, title = \"Synthetic Data\") +\n  theme(plot.title = element_text(hjust = .5)) +\n  facet_grid(cols = vars(syn.Year), rows = vars(syn.Gender)) #want year and gender stratification\nwrap_plots(hist1o, hist2s, ncol = 1)"
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "This exercise will load, process, and explore a text dataset that consists of employee reviews of their current and former employers on LinkedIn. The dataset can be found from Kaggle here.\nStarting with loading our packages, tidyverse for general cleaning, jsonlite to bring in our Json file, and here to make directory referencing easier.\n\npacman::p_load(tidyverse, jsonlite, here, stringr, superml)\n\nNow we will load our data. Json files are not generally square or in a data frame format, but the fromJSON function makes this tremendously easy.\n\nemp_rev &lt;- fromJSON(here('data-exercise', 'employer-reviews.json'))\nhead(emp_rev)\n\n                      ReviewTitle\n1                      Productive\n2                       Stressful\n3 Good Company for Every employee\n4                      Productive\n5                  Non productive\n6                            Good\n                                                                                                                                                                                                                                                                                                                    CompleteReview\n1                                                                                                                                                              Good company, cool workplace, work load little bit higher. Clean environment, disciplined, good cantin, big campus, systematic workflow, lenient but reliable firm.\n2 1. Need to work on boss's whims and fancies 2. Priorities keep changing 3. No regards for work life balance 4. Completing the work before time is stressed too much than completing it well. Involves lots of reworking, blame games; etc. 5. No job boundaries, you will be asked to do any work depending on the requirements.\n3                                                                                                                                                   Good company for every Engineers dream, Full Mediclaim for entired family, Free transport services from company location to home, Township culture for employees,job security.\n4                                                                                                                                                                         I am just pass out bsc in chemistry Typical day at work Mangement Work place good The most enjoyable part of the job I learn more information in company\n5                                                                                                                                                                     Not so fun at work just blame games  Target people and less target at work Paid less  No increment Make you feel low Too much stress  No one understands you\n6                                                                                                                                                                      I working as laboratory technician form last one year in covid19 staff but we are not appreciate of any about awards we also here for work work and work ..\n                                                        URL Rating\n1 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    3.0\n2 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    3.0\n3 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    5.0\n4 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    5.0\n5 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    1.0\n6 https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews    3.0\n                                            ReviewDetails\n1     (Current Employee)  -  Ghansoli  -  August 30, 2021\n2               (Former Employee)  -   -  August 26, 2021\n3               (Former Employee)  -   -  August 17, 2021\n4              (Current Employee)  -   -  August 17, 2021\n5                (Former Employee)  -   -  August 9, 2021\n6 (Current Employee)  -  Dahej, Gujarat  -  July 22, 2021\n\nstr(emp_rev)\n\n'data.frame':   145209 obs. of  5 variables:\n $ ReviewTitle   : chr  \"Productive\" \"Stressful\" \"Good Company for Every employee\" \"Productive\" ...\n $ CompleteReview: chr  \"Good company, cool workplace, work load little bit higher. Clean environment, disciplined, good cantin, big cam\"| __truncated__ \"1. Need to work on boss's whims and fancies 2. Priorities keep changing 3. No regards for work life balance 4. \"| __truncated__ \"Good company for every Engineers dream, Full Mediclaim for entired family, Free transport services from company\"| __truncated__ \"I am just pass out bsc in chemistry Typical day at work Mangement Work place good The most enjoyable part of th\"| __truncated__ ...\n $ URL           : chr  \"https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews\" \"https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews\" \"https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews\" \"https://in.indeed.com/cmp/Reliance-Industries-Ltd/reviews\" ...\n $ Rating        : chr  \"3.0\" \"3.0\" \"5.0\" \"5.0\" ...\n $ ReviewDetails : chr  \"(Current Employee)  -  Ghansoli  -  August 30, 2021\" \"(Former Employee)  -   -  August 26, 2021\" \"(Former Employee)  -   -  August 17, 2021\" \"(Current Employee)  -   -  August 17, 2021\" ...\n\nsummary(emp_rev)\n\n ReviewTitle        CompleteReview         URL               Rating         \n Length:145209      Length:145209      Length:145209      Length:145209     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ReviewDetails     \n Length:145209     \n Class :character  \n Mode  :character  \n\n\nLooking at the columns, we will want to do some cleanup on some of the more categorical ones. Starting with the URL, this may contain information about the employer, which we can extract. First i want to confirm that all the urls start the same way.\n\nsubstr(emp_rev$URL, 1, 26) %&gt;% unique() #substring extracts first 26 characters,\n\n[1] \"https://in.indeed.com/cmp/\"\n\n#unique tells us all of the unique values in the substring'd column\nlength(unique(emp_rev$URL)) #tells us the number of potential company names\n\n[1] 7286\n\n\nNext, I will use some substrings and regex to extract the company name after the above url portion.\n\nd1 &lt;- emp_rev %&gt;% mutate(\n  CompNm = (substr(URL, 27, nchar(URL)) %&gt;% str_extract('.*(?=/)') %&gt;% str_replace_all('-',' '))\n)\n#substring removes the first part of the url, since its always the same at 27 characters\n#str_extract looks for and extracts the first set of characters before the \"/\"\n#str_replace_all removes all of the dashes and replaces them with spaces\nd1$CompNm %&gt;% unique()\n\n [1] \"Reliance Industries Ltd\"         \"Mphasis\"                        \n [3] \"Kpmg\"                            \"Yes Bank\"                       \n [5] \"Sutherland\"                      \"Marriott International, Inc.\"   \n [7] \"DHL\"                             \"Jio\"                            \n [9] \"Vodafoneziggo\"                   \"HP\"                             \n[11] \"Maersk\"                          \"Ride.swiggy\"                    \n[13] \"Jll\"                             \"Alstom\"                         \n[15] \"UnitedHealth Group\"              \"Tata Consultancy Services (tcs)\"\n[17] \"Capgemini\"                       \"Teleperformance\"                \n[19] \"Cognizant Technology Solutions\"  \"Mahindra & Mahindra Ltd\"        \n[21] \"L&T Technology Services Ltd.\"    \"Bharti Airtel Limited\"          \n[23] \"Indeed\"                          \"Hyatt\"                          \n[25] \"Icici Prudential Life Insurance\" \"Accenture\"                      \n[27] \"Honeywell\"                       \"Standard Chartered Bank\"        \n[29] \"Nokia\"                           \"Apollo Hospitals\"               \n[31] \"Tata Aia Life\"                   \"Hdfc Bank\"                      \n[33] \"Bosch\"                           \"Deloitte\"                       \n[35] \"Ey\"                              \"Microsoft\"                      \n[37] \"Barclays\"                        \"JPMorgan Chase\"                 \n[39] \"Muthoot Finance\"                 \"Wns Global Services\"            \n[41] \"Kotak Mahindra Bank\"             \"Infosys\"                        \n[43] \"Oracle\"                          \"Byju's\"                         \n[45] \"Deutsche Bank\"                   \"Hinduja Global Solutions\"       \n[47] \"Ericsson\"                        \"Axis Bank\"                      \n[49] \"IBM\"                             \"Concentrix\"                     \n[51] \"Wells Fargo\"                     \"Google\"                         \n[53] \"Dell Technologies\"               \"Facebook\"                       \n[55] \"Amazon.com\"                      \"Flipkart.com\"                   \n[57] \"American Express\"                \"Citi\"                           \n[59] \"HSBC\"                           \n\n\nThis is cheating a little bit, because I counted the number of characters in the first part of the URL manually, meaning this is not the most robust way to identify the company name, but observing our values it does not look like it caused any problems.\nNext let’s look at the Rating. When the file was read in it looks like it was read as a string, but it would be more useful to us as a number. We’ll start with a quick summary and completeness check.\n\nd1$Rating %&gt;% summary() #summary of variable, understand scope\n\n   Length     Class      Mode \n   145209 character character \n\nsum(d1$Rating=='') + sum(is.na(d1$Rating)) #counts empty and missing values\n\n[1] 0\n\nd1$Rating %&gt;% unique()\n\n[1] \"3.0\" \"5.0\" \"1.0\" \"4.0\" \"2.0\"\n\n\nSo the variable is a string, but does not contain any missing or null values. All of the values fall under the five-point scale, so it should be safe to convert to a number.\n\nd2 &lt;- d1 %&gt;% mutate(\n  Rating = as.numeric(Rating)\n) # converts the Rating variable to numeric and saves it to the same variable.\n\nOne last variable to look at, ReviewDetails. This looks to have three parts to it. The status of the employee, the location, and the date the review was done. I’m most interested in the status for this exercise, but let’s see if we can get all three\n\n#str_split() breaks up the column by the dashes\n#simplify = TRUE turns it into a matrix\n# dim() gives us the number of rows and columns of the matrix, expecting 3 cols\nstr_split(emp_rev$ReviewDetails, '-', simplify = TRUE) %&gt;% dim()\n\n[1] 145209      5\n\n\nThe intention was to split the variable by dashes to create three columns, however it looks like there are some values that contain a dash themselves. This causes two additional columns to appear, so we will have to make some adjustments.\n\n#check for number of columns\nstr_split(emp_rev$ReviewDetails, ' -  ', simplify = TRUE) %&gt;% dim() \n\n[1] 145209      3\n\n#check for number of unique employee statuses\nstr_split(emp_rev$ReviewDetails, ' -  ', simplify = TRUE)[,1] %&gt;% unique() %&gt;% head(10)\n\n [1] \"(Current Employee) \"                                   \n [2] \"(Former Employee) \"                                    \n [3] \"Training   (Former Employee) \"                         \n [4] \"Officer   (Former Employee) \"                          \n [5] \"Leader   (Current Employee) \"                          \n [6] \"health care   (Current Employee) \"                     \n [7] \"Good team worker   (Former Employee) \"                 \n [8] \"Officer   (Current Employee) \"                         \n [9] \"Sr.G.M.Engineering and projects .   (Former Employee) \"\n[10] \"Hospitality   (Former Employee) \"                      \n\n#check for number of empty values\nifelse(str_split(emp_rev$ReviewDetails, ' -  ', simplify = TRUE)[,3] =='',1,0) %&gt;% sum()\n\n[1] 0\n\n\nFortunately, the solution is easier than it first appeared. Originally I was going to approach the split by splitting from the left and the right for Employee Status and Review Date, then removing everything thats in the left and right for location. However, the dashes that split the different details would have two additional spaces after each, so if we include that in the split function we can get the result we are looking for.\nThe Employee Status section fo the Details field looks to have more than just the status for some observations. A quick check might be worth it to see if Employee Title would be worth pursuing.\n\n# splits the columns then checks the first column for the employee status values,\n#then counts those that don't fall into the status value only\nemp_rev %&gt;%\n  mutate(\n    Stat = str_split(ReviewDetails, ' -  ', simplify = TRUE)[,1],\n    Stat = ifelse(Stat %in% c('(Current Employee) ', '(Former Employee) '), 0, 1)\n      ) %&gt;% select(Stat) %&gt;% sum()\n\n[1] 523\n\n\nWith only 523 observations that fall outside of the Current or Former employee status, it’s relatively safe to ignore that part of the ReviewDetails field.\n\n#saves the split column into three new variables.\n#Review Date is tranformed into date format\n#Employee status uses str_extract to get the status vales only\n#Location uses trimws() to remove extrenuous blanks\nd3 &lt;- d2 %&gt;% mutate(\n  ReviewDate = (str_split(ReviewDetails, ' -  ', simplify = TRUE)[,3] %&gt;% \n                  parse_date_time('0m d, y')),\n  EmployeeStatus = (str_split(ReviewDetails, ' -  ', simplify = TRUE)[,1] %&gt;% \n                      str_extract('(Current Employee)|(Former Employee)')),\n  Location = (str_split(ReviewDetails, ' -  ', simplify = TRUE)[,2] %&gt;% \n                trimws())\n  )\n# checks for how many values actually have a location. \n#Primarily to check if the column is worth using\nd3$Location %&gt;% unique() %&gt;% length()\n\n[1] 3780\n\n#Checks to make sure only two values are in the status\nd3$EmployeeStatus %&gt;% unique() %&gt;% length()\n\n[1] 2\n\n\n\n#Null and empty checks for new columns. \n#DatNull does not check for empties because of date format limitation\nd3 %&gt;% summarize(\n  StatNull = sum(EmployeeStatus == '') + sum(is.na(EmployeeStatus)),\n  DatNull = sum(is.na(ReviewDate)),\n  LocNull = sum(Location=='') + sum(is.na(Location))\n)\n\n  StatNull DatNull LocNull\n1        0       0  129942\n\n\nLocation is a pretty empty field, so it can largely be ignored, otherwise our other two variables look great. From here we can move on to the Review text itself.\n\n#lower cases the full review text\nd3 &lt;- d3 %&gt;% mutate(\n  CompleteReview = tolower(CompleteReview)\n)\n\nBefore we do anything we do anything with the reviews, the dataset is huge, and since the next step involves creating a bag of words it would probably be a good idea to filter the dataset. We will pick two companies to filter to as our companies of interest. First let’s look at the number of reviews by company.\n\n#checks the count of reviews by company name\nd3 %&gt;% group_by(CompNm) %&gt;% \n  summarize(\n    cnt = n()\n  ) %&gt;% arrange(-cnt)\n\n# A tibble: 59 × 2\n   CompNm                            cnt\n   &lt;chr&gt;                           &lt;int&gt;\n 1 Tata Consultancy Services (tcs) 14441\n 2 IBM                             10820\n 3 Infosys                         10696\n 4 Accenture                       10137\n 5 Cognizant Technology Solutions   9626\n 6 Hdfc Bank                        6749\n 7 Capgemini                        5248\n 8 Amazon.com                       3385\n 9 L&T Technology Services Ltd.     3226\n10 Concentrix                       3162\n# ℹ 49 more rows\n\n\nLooking at the size, HP and Dell Technologies look pretty reasonable, so we can filter to those two and compare.\n\n#filters to reviews for HP and Dell Technologies, saves to new df\nd4 &lt;- d3 %&gt;% filter(CompNm %in% c('Dell Technologies', 'HP'))\n\nThe next step will create a ‘bag of words’ commonly used for machine learning, but we’re going to use it this time for to get summary information about scores based on the appearance of words.\n\n#initializes the class for CountVectorizer. \n#Only looking at top 100 most frequently used words\ncfv &lt;- CountVectorizer$new(max_features = 100)\n#Transforms the occurence of each word across all reviews into a vector\ncf_mat &lt;- cfv$fit_transform(d4$CompleteReview)\n#transposed for readability\nhead(cf_mat) %&gt;% t()\n\n              [,1] [,2] [,3] [,4] [,5] [,6]\nwork             1    1    1    1    1    0\ngood             1    0    0    4    1    0\nmanagement       0    0    0    1    0    0\ncompany          2    0    1    2    0    0\nplace            0    1    0    0    0    2\nteam             0    0    0    0    0    0\ngreat            0    1    0    0    0    0\nhp               0    1    2    0    1    0\nworking          0    0    0    0    0    1\ndell             0    0    0    0    0    0\njob              0    0    1    0    0    0\nculture          0    0    0    0    0    0\nlife             0    0    0    0    0    0\nenvironment      0    0    0    0    1    0\nlot              0    0    0    0    1    0\npart             0    0    0    0    0    0\nlearn            0    0    0    0    0    2\nbalance          0    0    0    0    0    0\nnew              0    0    0    0    0    0\nfun              0    0    0    0    1    0\nday              0    0    0    0    0    2\nexperience       0    0    0    0    1    1\nbest             0    0    0    0    0    1\ntime             0    0    0    0    0    0\nlearned          0    0    0    0    0    0\nco               0    0    0    0    0    0\nfriendly         1    0    0    2    0    0\nlearning         0    0    0    0    2    0\nemployees        0    0    0    1    0    0\npeople           0    0    0    0    0    0\nemployee         0    0    0    1    0    0\nprocess          0    0    0    0    0    0\nthings           0    0    0    0    0    0\ncan              0    0    0    2    0    0\nworkers          0    0    0    0    0    0\nnice             0    0    0    1    0    0\ns                0    1    0    0    0    0\ncustomer         1    0    0    0    0    1\nwill             0    0    0    1    0    0\nsupport          0    0    0    0    0    1\nalso             0    0    0    0    0    0\none              0    0    0    0    0    0\ncareer           0    1    0    0    0    0\nlike             0    0    0    1    0    0\nget              0    0    2    0    0    0\nmany             0    0    0    0    0    0\ngrowth           0    0    1    0    0    1\nworked           0    0    0    0    1    0\nwell             0    0    0    0    0    0\nskills           0    0    1    0    0    0\nlearnt           0    0    0    0    0    0\nsalary           1    0    0    0    0    0\nevery            0    0    0    0    1    0\nenjoyable        0    0    0    0    0    0\ntraining         0    0    0    0    0    0\nknowledge        0    0    1    0    0    1\ntechnical        0    0    1    0    0    0\nalways           0    0    0    0    0    0\ncustomers        0    0    0    0    0    0\nopportunities    0    0    0    0    0    0\nhardest          0    0    0    0    0    0\nsupportive       0    0    0    0    0    0\nexcellent        0    0    0    0    0    0\nbusiness         0    0    0    0    0    0\nopportunity      0    0    1    0    0    0\nreally           0    1    0    1    0    0\nyears            0    0    0    0    0    0\ndifferent        0    0    0    0    0    0\ngrow             0    0    0    0    0    0\nmanagers         0    0    0    0    0    1\nissues           0    0    0    0    0    0\nactivities       0    0    0    0    1    0\nmuch             0    0    0    0    0    0\ngot              1    0    0    0    0    0\nproject          0    0    0    1    0    0\nhelp             0    0    0    0    0    0\nfirst            0    0    0    0    0    0\nmanager          0    0    0    0    0    0\nclient           0    0    0    0    0    0\npressure         0    0    0    0    0    0\nprofessional     0    0    0    0    0    0\nflexible         0    0    0    0    0    0\nus               0    0    0    0    0    0\ntechnologies     0    0    0    0    0    0\nhelpful          0    0    0    0    0    0\norganization     0    0    0    0    0    0\nenjoyed          0    0    0    0    0    0\nhandling         0    0    0    0    0    0\nworkplace        0    0    0    0    0    0\nbenefits         1    0    0    0    0    0\ntechnology       0    0    0    0    0    0\nsales            0    0    0    0    0    0\noverall          0    0    0    0    1    0\nprojects         0    0    0    0    0    0\nservice          0    0    0    0    0    0\ntypical          0    0    0    0    0    0\namazing          0    0    0    0    0    0\nneed             0    0    0    0    0    0\nhome             0    0    0    0    0    0\nservices         0    0    0    0    0    0\n\n\nNow we combine the bag of words matrix to the dataframe to make summarizing a bit easier\n\n#combines bag of words with orginal data frame\nd5 &lt;- cbind(d4, cf_mat)\n\nWe can take a look at the average score for each word for both companies. Note that the average score is weighted by the number of appearances of a word, that is to say that if a word appears multiple times in a review the score will have a greater weight.\n\n#Multiples the rating by the appearance of each word, then sums that up for each word\n#Then it divides by the total number of appearances of that word\n((d5$Rating * d5[,10:109]) %&gt;% colSums())/(colSums(d5[,10:109]))\n\n         work          good    management       company         place \n     4.239728      4.152515      4.140500      4.208420      4.260095 \n         team         great            hp       working          dell \n     4.221481      4.352092      4.276878      4.249440      4.292874 \n          job       culture          life   environment           lot \n     4.180314      4.321168      4.286114      4.289959      4.220779 \n         part         learn       balance           new           fun \n     4.174620      4.230942      4.242068      4.264916      4.288538 \n          day    experience          best          time       learned \n     4.173973      4.273743      4.497076      4.168182      4.176380 \n           co      friendly      learning     employees        people \n     4.150769      4.305772      4.239370      4.270799      4.191736 \n     employee       process        things           can       workers \n     4.270408      4.169811      4.235741      4.199620      4.158397 \n         nice             s      customer          will       support \n     4.212644      4.127413      4.300797      4.062000      4.235772 \n         also           one        career          like           get \n     4.271967      4.256356      4.267094      4.221729      4.159251 \n         many        growth        worked          well        skills \n     4.176755      4.106436      4.270471      4.308458      4.300752 \n       learnt        salary         every     enjoyable      training \n     4.251256      3.877863      4.318421      4.171123      4.329640 \n    knowledge     technical        always     customers opportunities \n     4.189944      4.247887      4.219373      4.272206      4.262537 \n      hardest    supportive     excellent      business   opportunity \n     4.088496      4.281899      4.489552      4.247678      4.263492 \n       really         years     different          grow      managers \n     4.290657      4.163763      4.184397      4.370107      4.306859 \n       issues    activities          much           got       project \n     4.408397      4.310345      3.923077      4.315175      4.003922 \n         help         first       manager        client      pressure \n     4.360324      4.258621      4.210526      4.136564      4.053097 \n professional      flexible            us  technologies       helpful \n     4.355556      4.247748      4.325792      4.239819      4.303167 \n organization       enjoyed      handling     workplace      benefits \n     4.319444      4.280374      4.202830      4.241706      4.142180 \n   technology         sales       overall      projects       service \n     4.204762      4.328502      4.082927      4.113300      4.336634 \n      typical       amazing          need          home      services \n     4.080808      4.520202      3.953846      4.226804      4.300518 \n\n\nNow lets split up the data by company and see if there are any differences.\n\n#Standard filters saved as new data frames\nDell &lt;- d5 %&gt;% filter(CompNm == 'Dell Technologies')\nHP &lt;- d5 %&gt;% filter(CompNm == 'HP')\n\nWe can use the same logic as before to get the average score by word, but for each of our new dataframes for each company.\n\n#Finds the average score by word for Dell. Saves as dataframe\nDell_scores &lt;- ((Dell$Rating * Dell[,10:109]) %&gt;%\n                  colSums())/ (colSums(Dell[,10:109])) %&gt;% \n  as.data.frame()\n\n#Finds the average score by word for HP. Saves as dataframe\nHP_scores &lt;- ((HP$Rating * HP[,10:109]) %&gt;% \n     colSums())/(colSums(HP[,10:109])) %&gt;% \n  as.data.frame()\n\nDell_scores %&gt;% head()\n\n                  .\nwork       4.210822\ngood       4.118130\nmanagement 4.078498\ncompany    4.172018\nplace      4.226161\nteam       4.212121\n\nHP_scores %&gt;% head()\n\n                  .\nwork       4.264388\ngood       4.181271\nmanagement 4.189117\ncompany    4.238593\nplace      4.292148\nteam       4.229529\n\n\nNow we use these new data frames to check the difference in score for each word. This could be a potential way of identifying certain areas that employees of one company like or dislike more than employees of the other company.\n\n#determines the difference in score each word\nword_scores &lt;- Dell_scores - HP_scores\n#updates the name of the score difference so it can be referenced\ncolnames(word_scores) &lt;- c('score_diff')\n#sorts by the word score\nword_scores &lt;- word_scores %&gt;% arrange(-score_diff)\n#creates a new variable called word, easier to reference than row names\nword_scores$word &lt;- rownames(word_scores)\n\nhead(word_scores)\n\n          score_diff      word\ndell       0.7941176      dell\nfirst      0.2670235     first\nhp         0.2234513        hp\ndifferent  0.1577160 different\nhome       0.1491707      home\nalso       0.1454323      also\n\n\nUltimately we find the scores to not be so different, but we can still see words that tend to “lead” to higher scores than others. Interestingly, a review that includes either of the words Dell or HP would tend to be higher for Dell than HP,\n\n#plots the difference in score by word.\nword_scores %&gt;% head(sum(word_scores$score_diff&gt;0)) %&gt;% ggplot() +\n  geom_col(aes(y=fct_reorder(word, score_diff),x=score_diff)) +\n  labs(y='Word', x='Score Difference', title = 'Dell-HP Score Difference by Word') +\n  theme(axis.text.y = element_text(size=6, angle = 25))\n\n\n\n\n\n\n\n\nThe methodology used here was not terribly robust, so certainly more research could be done. For example using n-grams might get an idea if certain short phrases are more telling, or grouping known “problem” words for one company or another to see if they are mentioned in each others surveys. Other options would be filtering by a score range and seeing what words appear for more positive or negative results."
  },
  {
    "objectID": "HYLTIN-PII-project/code/eda-code/eda.html",
    "href": "HYLTIN-PII-project/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "Setup\n\n#load needed packages. make sure they are installed.\npacman::p_load(here, knitr, tidyverse, skimr, fpp2, tigris, plotly, viridis, transformr, htmlwidgets, gt, gtExtras, widgetframe)\ntheme_set(theme_minimal())\n\nLoad the data.\n\nd1 &lt;- readRDS(here('data','processed-data','processed-crime.rds'))\ntig_zips &lt;- zctas(cb=TRUE, starts_with = c(unique(d1$Zip.Code)), year = 2020)\n\nZCTAs can take several minutes to download.  To cache the data and avoid re-downloading in future R sessions, set `options(tigris_use_cache = TRUE)`\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary(d1)\n\n Incident.Number     Highest.Offense.Description Highest.Offense.Code\n Min.   :2.004e+04   Length:2461621              Min.   : 100        \n 1st Qu.:2.005e+10   Class :character            1st Qu.: 601        \n Median :2.011e+10   Mode  :character            Median :1199        \n Mean   :6.032e+10                               Mean   :1689        \n 3rd Qu.:2.017e+10                               3rd Qu.:2716        \n Max.   :2.024e+12                               Max.   :8905        \n                                                                     \n Family.Violence    Occurred.Date.Time               Occurred.Date       \n Length:2461621     Min.   :2003-01-01 00:00:00.00   Min.   :2003-01-01  \n Class :character   1st Qu.:2007-11-16 16:49:00.00   1st Qu.:2007-11-16  \n Mode  :character   Median :2012-05-28 23:09:00.00   Median :2012-05-28  \n                    Mean   :2012-11-25 19:50:28.18   Mean   :2012-11-25  \n                    3rd Qu.:2017-10-26 21:19:00.00   3rd Qu.:2017-10-26  \n                    Max.   :2024-06-01 23:46:00.00   Max.   :2024-06-01  \n                                                                         \n Occurred.Time     Report.Date.Time                  Report.Date        \n Length:2461621    Min.   :2002-11-29 05:30:00.00   Min.   :2002-11-29  \n Class1:hms        1st Qu.:2007-11-27 22:41:00.00   1st Qu.:2007-11-27  \n Class2:difftime   Median :2012-06-06 11:15:00.00   Median :2012-06-06  \n Mode  :numeric    Mean   :2012-12-04 16:45:39.02   Mean   :2012-12-04  \n                   3rd Qu.:2017-11-05 01:56:00.00   3rd Qu.:2017-11-05  \n                   Max.   :2024-06-02 01:20:00.00   Max.   :2024-06-02  \n                                                                        \n Report.Time       Location.Type        Address             Zip.Code    \n Length:2461621    Length:2461621     Length:2461621     Min.   :76574  \n Class1:hms        Class :character   Class :character   1st Qu.:78717  \n Class2:difftime   Mode  :character   Mode  :character   Median :78741  \n Mode  :numeric                                          Mean   :78732  \n                                                         3rd Qu.:78752  \n                                                         Max.   :78759  \n                                                                        \n Council.District  APD.Sector        APD.District           PRA           \n Min.   : 1.000   Length:2461621     Length:2461621     Length:2461621    \n 1st Qu.: 3.000   Class :character   Class :character   Class :character  \n Median : 4.000   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 4.965                                                           \n 3rd Qu.: 7.000                                                           \n Max.   :10.000                                                           \n NA's   :30699                                                            \n  Census.Tract      Clearance.Status   Clearance.Date       UCR.Category      \n Min.   :     1.0   Length:2461621     Min.   :2003-01-01   Length:2461621    \n 1st Qu.:    15.0   Class :character   1st Qu.:2008-04-07   Class :character  \n Median :    23.2   Mode  :character   Median :2012-10-17   Mode  :character  \n Mean   :   245.4                      Mean   :2013-03-14                     \n 3rd Qu.:   338.0                      3rd Qu.:2018-01-19                     \n Max.   :950800.0                      Max.   :2024-06-02                     \n NA's   :8822                          NA's   :348308                         \n Category.Description  X.coordinate      Y.coordinate         Latitude    \n Length:2461621       Min.   :      0   Min.   :       0   Min.   :30.01  \n Class :character     1st Qu.:3108421   1st Qu.:10057433   1st Qu.:30.23  \n Mode  :character     Median :3117292   Median :10073004   Median :30.28  \n                      Mean   :3075787   Mean   : 9946761   Mean   :30.29  \n                      3rd Qu.:3126595   3rd Qu.:10100561   3rd Qu.:30.35  \n                      Max.   :3231806   Max.   :10215496   Max.   :30.67  \n                                                           NA's   :32335  \n   Longitude        Location         Crime.Category    \n Min.   :-98.18   Length:2461621     Length:2461621    \n 1st Qu.:-97.76   Class :character   Class :character  \n Median :-97.73   Mode  :character   Mode  :character  \n Mean   :-97.73                                        \n 3rd Qu.:-97.70                                        \n Max.   :-97.37                                        \n NA's   :32335                                         \n\nhead(d1)\n\n  Incident.Number    Highest.Offense.Description Highest.Offense.Code\n1      2013851154 SEXUAL ASSAULT OF CHILD/OBJECT                 1707\n2     20161800084                RAPE OF A CHILD                  204\n3      2010701921                           RAPE                  200\n4     20071820003                           RAPE                  200\n5     20062192048       SEXUAL ASSAULT W/ OBJECT                 1700\n6     20033211543                           RAPE                  200\n  Family.Violence  Occurred.Date.Time Occurred.Date Occurred.Time\n1               Y 2009-01-01 00:01:00    2009-01-01      00:01:00\n2               Y 2016-06-28 01:05:00    2016-06-28      01:05:00\n3               Y 2010-03-04 19:15:00    2010-03-04      19:15:00\n4               N 2007-07-01 12:00:00    2007-07-01      12:00:00\n5               N 2006-08-07 22:28:00    2006-08-07      22:28:00\n6               Y 2003-11-17 14:00:00    2003-11-17      14:00:00\n     Report.Date.Time Report.Date Report.Time    Location.Type\n1 2013-03-26 16:56:00  2013-03-26    16:56:00 RESIDENCE / HOME\n2 2016-06-28 01:05:00  2016-06-28    01:05:00 RESIDENCE / HOME\n3 2010-03-11 17:06:00  2010-03-11    17:06:00 RESIDENCE / HOME\n4 2007-07-01 12:00:00  2007-07-01    12:00:00 RESIDENCE / HOME\n5 2006-08-07 22:28:00  2006-08-07    22:28:00 RESIDENCE / HOME\n6 2003-11-17 21:40:00  2003-11-17    21:40:00 RESIDENCE / HOME\n                   Address Zip.Code Council.District APD.Sector APD.District\n1      900 BLOCK E 32ND ST    78705                9         BA            1\n2 6900 BLOCK BRANCHWOOD DR    78744                2         FR            8\n3   400 BLOCK ANGEL OAK ST    78748                5         FR            2\n4     1700 BLOCK WOOTEN DR    78757                7         ID            7\n5    500 BLOCK E OLTORF ST    78704                9         DA            2\n6    7300 BLOCK DANJEAN DR    78745               NA         DA            6\n  PRA Census.Tract Clearance.Status Clearance.Date UCR.Category\n1 348         4.00                C     2013-04-11          11C\n2 530        24.41                C     2016-07-01          11A\n3 542        24.38                C     2010-03-18          11A\n4 247       405.00                O     2007-08-02          11A\n5 479        23.23                      2006-08-22          11C\n6 525      1728.00                O     2003-11-30          11A\n  Category.Description X.coordinate Y.coordinate Latitude Longitude Location\n1                 Rape            0            0       NA        NA         \n2                 Rape            0            0       NA        NA         \n3                 Rape            0            0       NA        NA         \n4                 Rape            0            0       NA        NA         \n5                 Rape            0            0       NA        NA         \n6                 Rape            0            0       NA        NA         \n  Crime.Category\n1           RAPE\n2           RAPE\n3           RAPE\n4           RAPE\n5           RAPE\n6           RAPE\n\nstr(d1)\n\n'data.frame':   2461621 obs. of  28 variables:\n $ Incident.Number            : num  2.01e+09 2.02e+10 2.01e+09 2.01e+10 2.01e+10 ...\n $ Highest.Offense.Description: chr  \"SEXUAL ASSAULT OF CHILD/OBJECT\" \"RAPE OF A CHILD\" \"RAPE\" \"RAPE\" ...\n $ Highest.Offense.Code       : int  1707 204 200 200 1700 200 902 2703 200 2006 ...\n $ Family.Violence            : chr  \"Y\" \"Y\" \"Y\" \"N\" ...\n $ Occurred.Date.Time         : POSIXct, format: \"2009-01-01 00:01:00\" \"2016-06-28 01:05:00\" ...\n $ Occurred.Date              : Date, format: \"2009-01-01\" \"2016-06-28\" ...\n $ Occurred.Time              : 'hms' num  00:01:00 01:05:00 19:15:00 12:00:00 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ Report.Date.Time           : POSIXct, format: \"2013-03-26 16:56:00\" \"2016-06-28 01:05:00\" ...\n $ Report.Date                : Date, format: \"2013-03-26\" \"2016-06-28\" ...\n $ Report.Time                : 'hms' num  16:56:00 01:05:00 17:06:00 12:00:00 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ Location.Type              : chr  \"RESIDENCE / HOME\" \"RESIDENCE / HOME\" \"RESIDENCE / HOME\" \"RESIDENCE / HOME\" ...\n $ Address                    : chr  \"900 BLOCK E 32ND ST\" \"6900 BLOCK BRANCHWOOD DR\" \"400 BLOCK ANGEL OAK ST\" \"1700 BLOCK WOOTEN DR\" ...\n $ Zip.Code                   : int  78705 78744 78748 78757 78704 78745 78702 78759 78705 78741 ...\n $ Council.District           : int  9 2 5 7 9 NA 3 10 9 3 ...\n $ APD.Sector                 : chr  \"BA\" \"FR\" \"FR\" \"ID\" ...\n $ APD.District               : chr  \"1\" \"8\" \"2\" \"7\" ...\n $ PRA                        : chr  \"348\" \"530\" \"542\" \"247\" ...\n $ Census.Tract               : num  4 24.4 24.4 405 23.2 ...\n $ Clearance.Status           : chr  \"C\" \"C\" \"C\" \"O\" ...\n $ Clearance.Date             : Date, format: \"2013-04-11\" \"2016-07-01\" ...\n $ UCR.Category               : chr  \"11C\" \"11A\" \"11A\" \"11A\" ...\n $ Category.Description       : chr  \"Rape\" \"Rape\" \"Rape\" \"Rape\" ...\n $ X.coordinate               : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Y.coordinate               : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Latitude                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Longitude                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Location                   : chr  \"\" \"\" \"\" \"\" ...\n $ Crime.Category             : chr  \"RAPE\" \"RAPE\" \"RAPE\" \"RAPE\" ...\n\nskim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n2461621\n\n\nNumber of columns\n28\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n12\n\n\nDate\n3\n\n\ndifftime\n2\n\n\nnumeric\n9\n\n\nPOSIXct\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHighest.Offense.Description\n0\n1\n3\n48\n0\n436\n0\n\n\nFamily.Violence\n0\n1\n1\n1\n0\n2\n0\n\n\nLocation.Type\n0\n1\n7\n47\n0\n47\n0\n\n\nAddress\n0\n1\n8\n74\n0\n246951\n0\n\n\nAPD.Sector\n0\n1\n2\n5\n0\n14\n0\n\n\nAPD.District\n0\n1\n1\n2\n0\n21\n0\n\n\nPRA\n0\n1\n1\n4\n0\n742\n0\n\n\nClearance.Status\n0\n1\n0\n1\n615856\n4\n0\n\n\nUCR.Category\n0\n1\n0\n3\n1550375\n17\n0\n\n\nCategory.Description\n0\n1\n0\n18\n1550375\n8\n0\n\n\nLocation\n0\n1\n0\n27\n32335\n219842\n0\n\n\nCrime.Category\n0\n1\n4\n29\n0\n33\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Date\n0\n1.00\n2003-01-01\n2024-06-01\n2012-05-28\n7823\n\n\nReport.Date\n0\n1.00\n2002-11-29\n2024-06-02\n2012-06-06\n7825\n\n\nClearance.Date\n348308\n0.86\n2003-01-01\n2024-06-02\n2012-10-17\n7814\n\n\n\nVariable type: difftime\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Time\n0\n1\n0 secs\n86340 secs\n14:25:00\n1440\n\n\nReport.Time\n0\n1\n0 secs\n86340 secs\n14:06:00\n1440\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nIncident.Number\n0\n1.00\n6.031558e+10\n2.896224e+11\n20035.00\n2.005329e+10\n2.010505e+10\n2.017186e+10\n2.024242e+12\n▇▁▁▁▁\n\n\nHighest.Offense.Code\n0\n1.00\n1.689080e+03\n1.218280e+03\n100.00\n6.010000e+02\n1.199000e+03\n2.716000e+03\n8.905000e+03\n▇▅▁▁▁\n\n\nZip.Code\n0\n1.00\n7.873243e+04\n2.510000e+01\n76574.00\n7.871700e+04\n7.874100e+04\n7.875200e+04\n7.875900e+04\n▁▁▁▁▇\n\n\nCouncil.District\n30699\n0.99\n4.960000e+00\n2.840000e+00\n1.00\n3.000000e+00\n4.000000e+00\n7.000000e+00\n1.000000e+01\n▅▇▃▃▅\n\n\nCensus.Tract\n8822\n1.00\n2.453700e+02\n3.363970e+03\n1.00\n1.500000e+01\n2.324000e+01\n3.380000e+02\n9.508000e+05\n▇▁▁▁▁\n\n\nX.coordinate\n0\n1.00\n3.075787e+06\n3.551571e+05\n0.00\n3.108421e+06\n3.117292e+06\n3.126595e+06\n3.231806e+06\n▁▁▁▁▇\n\n\nY.coordinate\n0\n1.00\n9.946761e+06\n1.147895e+06\n0.00\n1.005743e+07\n1.007300e+07\n1.010056e+07\n1.021550e+07\n▁▁▁▁▇\n\n\nLatitude\n32335\n0.99\n3.029000e+01\n8.000000e-02\n30.01\n3.023000e+01\n3.028000e+01\n3.035000e+01\n3.067000e+01\n▁▇▇▂▁\n\n\nLongitude\n32335\n0.99\n-9.773000e+01\n5.000000e-02\n-98.18\n-9.776000e+01\n-9.773000e+01\n-9.770000e+01\n-9.737000e+01\n▁▁▇▂▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Date.Time\n0\n1\n2003-01-01 00:00:00\n2024-06-01 23:46:00\n2012-05-28 23:09:00\n1738386\n\n\nReport.Date.Time\n0\n1\n2002-11-29 05:30:00\n2024-06-02 01:20:00\n2012-06-06 11:15:00\n2169726\n\n\n\n\n\n\nsummary(tig_zips)\n\n  ZCTA5CE20          AFFGEOID20          GEOID20             NAME20         \n Length:68          Length:68          Length:68          Length:68         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    LSAD20             ALAND20             AWATER20                 geometry \n Length:68          Min.   :  1475441   Min.   :       0   MULTIPOLYGON :68  \n Class :character   1st Qu.: 22687804   1st Qu.:       0   epsg:4269    : 0  \n Mode  :character   Median : 41116414   Median :  257508   +proj=long...: 0  \n                    Mean   : 98762692   Mean   : 1867545                     \n                    3rd Qu.:110644620   3rd Qu.: 1117435                     \n                    Max.   :521452503   Max.   :26097562                     \n\nhead(tig_zips)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -98.27747 ymin: 30.02595 xmax: -97.21823 ymax: 30.49132\nGeodetic CRS:  NAD83\n     ZCTA5CE20     AFFGEOID20 GEOID20 NAME20 LSAD20   ALAND20 AWATER20\n477      78653 860Z200US78653   78653  78653     Z5 272611245   872369\n1478     78610 860Z200US78610   78610  78610     Z5 250909063   918933\n1884     78722 860Z200US78722   78722  78722     Z5   3510327        0\n5355     78757 860Z200US78757   78757  78757     Z5  12823683        0\n6063     78620 860Z200US78620   78620  78620     Z5 454800649  1116065\n6064     78621 860Z200US78621   78621  78621     Z5 460284787  2487757\n                           geometry\n477  MULTIPOLYGON (((-97.61061 3...\n1478 MULTIPOLYGON (((-98.01582 3...\n1884 MULTIPOLYGON (((-97.7274 30...\n5355 MULTIPOLYGON (((-97.75528 3...\n6063 MULTIPOLYGON (((-98.27626 3...\n6064 MULTIPOLYGON (((-97.50169 3...\n\nstr(tig_zips)\n\nClasses 'sf' and 'data.frame':  68 obs. of  8 variables:\n $ ZCTA5CE20 : chr  \"78653\" \"78610\" \"78722\" \"78757\" ...\n $ AFFGEOID20: chr  \"860Z200US78653\" \"860Z200US78610\" \"860Z200US78722\" \"860Z200US78757\" ...\n $ GEOID20   : chr  \"78653\" \"78610\" \"78722\" \"78757\" ...\n $ NAME20    : chr  \"78653\" \"78610\" \"78722\" \"78757\" ...\n $ LSAD20    : chr  \"Z5\" \"Z5\" \"Z5\" \"Z5\" ...\n $ ALAND20   : num  2.73e+08 2.51e+08 3.51e+06 1.28e+07 4.55e+08 ...\n $ AWATER20  : num  872369 918933 0 0 1116065 ...\n $ geometry  :sfc_MULTIPOLYGON of length 68; first list element: List of 2\n  ..$ :List of 1\n  .. ..$ : num [1:6, 1:2] -97.6 -97.6 -97.6 -97.6 -97.6 ...\n  ..$ :List of 1\n  .. ..$ : num [1:448, 1:2] -97.6 -97.6 -97.6 -97.6 -97.6 ...\n  ..- attr(*, \"class\")= chr [1:3] \"XY\" \"MULTIPOLYGON\" \"sfg\"\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA NA\n  ..- attr(*, \"names\")= chr [1:7] \"ZCTA5CE20\" \"AFFGEOID20\" \"GEOID20\" \"NAME20\" ...\n - attr(*, \"tigris\")= chr \"zcta\"\n\nskim(tig_zips)\n\nWarning: Couldn't find skimmers for class: sfc_MULTIPOLYGON, sfc; No\nuser-defined `sfl` provided. Falling back to `character`.\n\n\n\nData summary\n\n\nName\ntig_zips\n\n\nNumber of rows\n68\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nZCTA5CE20\n0\n1\n5\n5\n0\n68\n0\n\n\nAFFGEOID20\n0\n1\n14\n14\n0\n68\n0\n\n\nGEOID20\n0\n1\n5\n5\n0\n68\n0\n\n\nNAME20\n0\n1\n5\n5\n0\n68\n0\n\n\nLSAD20\n0\n1\n2\n2\n0\n1\n0\n\n\ngeometry\n0\n1\n830\n10835\n0\n68\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nALAND20\n0\n1\n98762692\n125048235\n1475441\n22687804\n41116413.5\n110644621\n521452503\n▇▁▁▁▁\n\n\nAWATER20\n0\n1\n1867545\n4648935\n0\n0\n257508.5\n1117435\n26097562\n▇▁▁▁▁\n\n\n\n\n\nImportant to note that the dataset has several date fields, and that alone will be ripe with opportunity for exploration. However I want to take a look at some of the other variables first, to see if maybe isolating to one of the other categories, or including other categorical variables in the time series could be useful. First off is Zip Code, because location is often a major factor in determining the amount and type of crime that would occur.\n\nd1 %&gt;% \n  mutate(years = year(Occurred.Date),\n         years = as.character(years)\n         ) %&gt;% \n  group_by(years, Zip.Code) %&gt;% \n  summarize(\n    count = n()\n  ) %&gt;% as.data.frame() %&gt;% pivot_wider(names_from = years, values_from = count)\n\n`summarise()` has grouped output by 'years'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 69 × 23\n   Zip.Code `2003` `2004` `2005` `2006` `2007` `2008` `2009` `2010` `2011`\n      &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1    76574      1      4      1     NA      2      6     NA      1     NA\n 2    78610      1     17     14     10      5      8      9      6      5\n 3    78612      1     NA      1      3     NA      2     NA     NA     NA\n 4    78613    379    435    301    343    338    470    461    442    431\n 5    78617    748    756    722    744    840   1035   1038   1084   1063\n 6    78620      2      1      5      2      3      1      1     NA     NA\n 7    78634      2     NA     NA      1      1      1     NA     NA     NA\n 8    78641     11      3      6      9      5     11      1      3     NA\n 9    78642      1     NA     NA     NA     NA      1     NA     NA     NA\n10    78645      4      1      5      3      4      4     NA      1     NA\n# ℹ 59 more rows\n# ℹ 13 more variables: `2012` &lt;int&gt;, `2013` &lt;int&gt;, `2014` &lt;int&gt;, `2015` &lt;int&gt;,\n#   `2016` &lt;int&gt;, `2017` &lt;int&gt;, `2018` &lt;int&gt;, `2019` &lt;int&gt;, `2020` &lt;int&gt;,\n#   `2021` &lt;int&gt;, `2022` &lt;int&gt;, `2023` &lt;int&gt;, `2024` &lt;int&gt;\n\n\nLayering in the year gives an interesting view, because it gives us the opportunity to see major increases in certain locations over time. However, there are a lot of zip Codes and a lot of years in this dataset. A visualization may help with this, my first thought being a chloropeth plot with a slider for the year value. Other quicker solutions might be looking at the individual years as observations of sorts, then looking at summary statistics. More to come here.\n\nd1 %&gt;% \n  mutate(years = year(Occurred.Date)\n         #years = as.character(years)\n         ) %&gt;% \n  #filter(years == '2023' | years == '2022') %&gt;% \n  #group_by(Family.Violence, Highest.Offense.Description, years) %&gt;% \n  group_by(Highest.Offense.Description, years) %&gt;% \n  summarize(cnt = n(), .groups = 'drop') %&gt;% \n  #group_by(Highest.Offense.Description, years) %&gt;% \n  #mutate(tot = sum(cnt)) %&gt;% \n  #relocate(tot, .after = years) %&gt;% \n  mutate(Highest.Offense.Description = as.factor(Highest.Offense.Description)) %&gt;%\n  as.data.frame() %&gt;%\n  arrange(years) %&gt;% \n  pivot_wider(names_from = years, values_from = cnt) \n\n# A tibble: 436 × 23\n   Highest.Offense.Description  `2003` `2004` `2005` `2006` `2007` `2008` `2009`\n   &lt;fct&gt;                         &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1 ABUSE OF OFFICIAL CAPACITY        1      5      2      2      4      2      1\n 2 AGG ASLT W/MOTOR VEH FAM/DA…     25     41     37     55     52     54     50\n 3 AGG ASSAULT                     718    809    805    810    849    971    885\n 4 AGG ASSAULT FAM/DATE VIOLEN…    426    446    497    564    610    628    605\n 5 AGG ASSAULT ON PUBLIC SERVA…     26     25     16     25     14     11     24\n 6 AGG ASSAULT WITH MOTOR VEH      119    147    104     88    109    121    102\n 7 AGG FORCED SODOMY                 5      2      2      2      3      3      3\n 8 AGG FORCED SODOMY OF CHILD       36     34     24     23     11     15      1\n 9 AGG KIDNAPPING                    7      6      7      6      8      9      9\n10 AGG PERJURY                       1     NA     NA     NA     NA     NA     NA\n# ℹ 426 more rows\n# ℹ 15 more variables: `2010` &lt;int&gt;, `2011` &lt;int&gt;, `2012` &lt;int&gt;, `2013` &lt;int&gt;,\n#   `2014` &lt;int&gt;, `2015` &lt;int&gt;, `2016` &lt;int&gt;, `2017` &lt;int&gt;, `2018` &lt;int&gt;,\n#   `2019` &lt;int&gt;, `2020` &lt;int&gt;, `2021` &lt;int&gt;, `2022` &lt;int&gt;, `2023` &lt;int&gt;,\n#   `2024` &lt;int&gt;\n\n\nSimilar to our zip code variable here, but there are far too many crime descriptions to use. Grouping some together may work, I don’t think we have enough information in the dataset overall to do any sort of classification modeling however, so it may amount to a rules based classification using regular expressions. Other variables may still help with this, such as the UCF classification variables.\n\nd1 %&gt;% \n  mutate(\n    Category.Description = if_else(\n      Category.Description == '', 'NA/Unknown', Category.Description\n      )\n    ) %&gt;% \n  group_by(Category.Description, Highest.Offense.Description) %&gt;% \n  summarize(\n    cnt = n()\n    ) %&gt;% \n  pivot_wider(\n    names_from = Category.Description, values_from = cnt\n    )\n\n`summarise()` has grouped output by 'Category.Description'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 436 × 9\n   Highest.Offense.Description `Aggravated Assault` `Auto Theft` Burglary Murder\n   &lt;chr&gt;                                      &lt;int&gt;        &lt;int&gt;    &lt;int&gt;  &lt;int&gt;\n 1 AGG ASLT ENHANC STRANGL/SU…                 1098           NA       NA     NA\n 2 AGG ASLT STRANGLE/SUFFOCATE                 7712           NA       NA     NA\n 3 AGG ASLT W/MOTOR VEH FAM/D…                  772           NA       NA     NA\n 4 AGG ASSAULT                                18634           NA       NA     NA\n 5 AGG ASSAULT BY PUBLIC SERV…                   10           NA       NA     NA\n 6 AGG ASSAULT FAM/DATE VIOLE…                 9603           NA       NA     NA\n 7 AGG ASSAULT ON PEACE OFFIC…                   70           NA       NA     NA\n 8 AGG ASSAULT ON PUBLIC SERV…                  329           NA       NA     NA\n 9 AGG ASSAULT WITH MOTOR VEH                  1823           NA       NA     NA\n10 ARSON WITH BODILY INJURY                      15           NA       NA     NA\n# ℹ 426 more rows\n# ℹ 4 more variables: `NA/Unknown` &lt;int&gt;, Rape &lt;int&gt;, Robbery &lt;int&gt;,\n#   Theft &lt;int&gt;\n\n\nThe UCF classifiers are much more consumable, but there are more incidents that are not classified than are. That said, this might be a good start for a rules based classification. Many of the unclassified incidents would not fit very nicely into the groups anyway, so some new categories may need to be made. I may or may not come back to the re-classification part, but ultimately the genesis of this project was with reports on the amount of homicide in the city, so the already existing descriptions may be enough to drill down into just those.\nreminder to myself to save figures from this file to bring into manuscript and presentation later.\n\n\nData exploration through figures\nTime is a variable that is much easier to interpret with a plot, so let’s start there. Time series first.\n\ndayline &lt;- d1 %&gt;% group_by(Occurred.Date) %&gt;% \n  summarize(\n  cnt = n()\n) %&gt;% as.data.frame() %&gt;% \n  ggplot(mapping = aes(x=Occurred.Date, y=cnt)) +\n  geom_line() +\n  labs(title = 'Austin Crime Over Time', x = 'Time of Occurence (by day)', y = 'Count') +\n  theme(panel.background = element_rect(fill = 'white',\n                                        color = 'white'),\n        plot.background = element_rect(fill = 'white',\n                                        color = 'white')\n        )\n\ndayline\n\n\n\n\n\n\n\nggsave(here('results', 'figures', 'static-plots', 'crime-by-day.png'), dayline)\n\nSaving 7 x 5 in image\n\n\nI expected aggregating by day to actually look worse than this. There is a clear trend, upwards at first but then starts decreasing around 2008 or so. Seeing this I am tempted to looking into time series modeling. There may be a seasonal pattern in some of those spikes, but that may be something that can be resolved with different methods. I’m tempted to go ARIMA, because frankly that’s the one I’m familiar with, but I’ll have to do some reading up on my options. An interesting idea might be to fit the model, make predictions, and then see where I landed compared to the actual data before the class ends.\n\nmonthline &lt;- d1 %&gt;% filter(Occurred.Date &lt;= '2024-03-31') %&gt;%\n  mutate(Occurred.Date = trunc.Date(Occurred.Date, 'months')) %&gt;% \n  group_by(Occurred.Date) %&gt;% \n  summarize(\n  cnt = n()\n) %&gt;% as.data.frame() %&gt;% \n  ggplot(mapping = aes(x=Occurred.Date, y=cnt)) +\n  geom_line() +\n  labs(title = 'Austin Crime Over Time', x = 'Time of Occurence (by month)', y = 'Count') +\n  theme(panel.background = element_rect(fill = 'white',\n                                        color = 'white'),\n        plot.background = element_rect(fill = 'white',\n                                        color = 'white')\n        )\n\nmonthline\n\n\n\n\n\n\n\nggsave(here('results', 'figures', 'static-plots', 'crime-by-month.png'), monthline)\n\nSaving 7 x 5 in image\n\n\nMonthly looks even better, again further lending itself to the idea of a time series model. The downward spikes are more prevalent here and may cause problems themselves as well, but again the seasonality may be able to be resolved with things like logs or differencing. As a note, in this and the last plot I filter out 2024, mostly due to a large drop at the end from the incomplete month, but also for the aforementioned idea of comparing predictions to actual results.\nA potentially quick way to identify “peak” months for crime would be via a bar chart aggregating all occurrences in each month together. I was hoping to see some quick trends here, but I see surprisingly little here. The highest month is May, potentially because of school letting out/ graduations for UT Austin, but I wouldnt say the difference is large enough compared to other months to really point to any one reason. This may be another one to layer in with time, like a chart with a slider for the year to see shifts in crime frequency by month over time.\n\nmurdline &lt;- d1 %&gt;% filter(Occurred.Date &lt;= '2024-03-31', Crime.Category == 'MURDER') %&gt;%\n  mutate(Occurred.Date = trunc.Date(Occurred.Date, 'months')) %&gt;% \n  group_by(Occurred.Date) %&gt;% \n  summarize(\n  cnt = n()\n) %&gt;% as.data.frame() %&gt;% \n  ggplot(mapping = aes(x=Occurred.Date, y=cnt)) +\n  geom_line() +\n  labs(title = 'Austin Murder Over Time', x = 'Time of Occurence (by month)', y = 'Count') +\n  theme(panel.background = element_rect(fill = 'white',\n                                        color = 'white'),\n        plot.background = element_rect(fill = 'white',\n                                        color = 'white')\n        )\n\nmurdline\n\n\n\n\n\n\n\nggsave(here('results', 'figures', 'static-plots', 'murder-by-time.png'), murdline)\n\nSaving 7 x 5 in image\n\n\nADD COMMMENTARY LATER: NUMBER OF FIRST QUARTER HOMICIDES MATCHES ARTICLE FOR 2020 AND 2023, BUT NOT 2024. ARTICLE LIKELY NOT COUNTING INTOX MANSLAUGHTER OR JUSTIFIED HOMICIDE. KEPT THOSE BECAUSE FBI CATEGORIZATION COUNTS MANSLAUGHTER.\n\nd2 &lt;- d1 %&gt;% \n  mutate(\n    Occurred.Month = month(Occurred.Date, label = TRUE),\n    Occurred.Day = weekdays(Occurred.Date),\n    Occurred.Year = year(Occurred.Date),\n    Occur.Report.Diff = Report.Date - Occurred.Date\n  )\nd2 %&gt;% \n  #filter(Occurred.Year == 2023) %&gt;%\n  ggplot(aes(x=Occurred.Month)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nNow a Zip Code bar chart. Really a bit of a graphical representation of what we saw earlier, though I filtered it to the two most recent complete years. Again, looking for big swings, and simultaneously taking a quick peak at the Family Violence indicator. Surprisingly low volume there, which leads me to believe it just doesn’t get captured appropriately every time, especially when comparing with the descriptions themselves, which we will see in a moment. Going back to the zip codes, there might be an opportunity to do something like binning the zip codes with high medium and low crime areas, or something of the sort.\n\nd1 %&gt;% \n  mutate(years = year(Occurred.Date),\n         years = as.character(years)\n         ) %&gt;% \n  filter(years == '2003' | years == '2022') %&gt;% \n  group_by(Family.Violence, Zip.Code, years) %&gt;% \n  summarize(cnt = n(), .groups = 'drop') %&gt;% \n  group_by(Zip.Code, years) %&gt;% \n  mutate(tot = sum(cnt)) %&gt;% \n  relocate(tot, .after = years) %&gt;% \n  mutate(Zip.Code = as.character(Zip.Code)) %&gt;%\n  as.data.frame() %&gt;% \n  arrange(desc(tot)) %&gt;% head(100) %&gt;% \n  ggplot(aes(fill=Family.Violence, x= reorder(Zip.Code,cnt), y=cnt)) +\n  geom_bar(position = 'stack', stat='identity') +\n  facet_wrap(~years) +\n  coord_flip()\n\n\n\n\n\n\n\n\nNow a bar chart by crime description. This is similar to the last plot, but a little more disparate here. There are so many more crime descriptions than zip codes, so they are much more easily spread thin. That said, that actually makes the top five crime descriptions stand out a little more. So going back to the idea of creating new classifications, that may be a good idea to make sure those 5 are appropriately captured. And again, the family violence indicator is captured almost entirely by one description, yet the top description is Family disturbance. An easy explanation may be because the description is of the Highest Offense in the incident, so when there is physical family violence, it is often the highest offense.\n\nd1 %&gt;% \n  mutate(years = year(Occurred.Date),\n         years = as.character(years)\n         ) %&gt;% \n  filter(years == '2023' | years == '2022') %&gt;% \n  group_by(Family.Violence, Highest.Offense.Description, years) %&gt;% \n  summarize(cnt = n(), .groups = 'drop') %&gt;% \n  group_by(Highest.Offense.Description, years) %&gt;% \n  mutate(tot = sum(cnt)) %&gt;% \n  relocate(tot, .after = years) %&gt;% \n  mutate(Highest.Offense.Description = as.factor(Highest.Offense.Description)) %&gt;%\n  as.data.frame() %&gt;% \n  arrange(desc(tot)) %&gt;% head(50) %&gt;% \n  ggplot(aes(fill=Family.Violence, x= reorder(Highest.Offense.Description,cnt), y=cnt)) +\n  geom_bar(position = 'stack', stat='identity') +\n  facet_wrap(~years) +\n  coord_flip() \n\n\n\n\n\n\n\n\n\nyearlyCrime &lt;- d1 %&gt;% mutate(\n  Zip.Char = as.character(Zip.Code),\n  years = year(Occurred.Date),\n  years = as.character(years)\n  ) %&gt;% filter(years != 2024) %&gt;% \n  group_by(Zip.Char, years) %&gt;% \n  summarize(crim.count  = n()) %&gt;% \n  ungroup() %&gt;% \n  as.data.frame()\n\n`summarise()` has grouped output by 'Zip.Char'. You can override using the\n`.groups` argument.\n\n  #filter(count &gt;= 500) %&gt;%\n  #filter(Zip.Char == '78741') %&gt;%\n  \n\nallYrsZips &lt;- expand.grid(years = unique(yearlyCrime$years), Zip.Char = unique(yearlyCrime$Zip.Char))\n\nyearlyCrimeAll &lt;- left_join(allYrsZips, yearlyCrime, by = join_by(Zip.Char, years))  %&gt;% \n  inner_join(tig_zips, by = join_by(Zip.Char == ZCTA5CE20)) %&gt;% \n  mutate(crim.count = ifelse(is.na(crim.count), 0, crim.count))\n\n\ncpeth &lt;- yearlyCrimeAll %&gt;% filter(crim.count &gt;= 50) %&gt;% \n  ggplot() +\n  geom_sf(aes(geometry = geometry, fill = crim.count, frame = years, text = paste(Zip.Char, '&lt;br&gt;', crim.count))) +\n  scale_fill_gradient2(low = \"#E0144C\", mid = '#FFFFFF', high = \"#000067\", midpoint = -5000, \n                       trans = 'reverse') +\n  theme_classic() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.text.y=element_blank(),\n        line = element_blank(),\n        plot.title = element_text(hjust = 0, size = 15)) +\n  labs(fill = 'Crime Count',\n       title = 'Crime by Location Over Time')\n\nWarning in layer_sf(geom = GeomSf, data = data, mapping = mapping, stat = stat,\n: Ignoring unknown aesthetics: frame and text\n\ncplotly &lt;- cpeth %&gt;% ggplotly() %&gt;% \n  layout(hoverlabel = text) %&gt;% \n  animation_opts(1500, 1) %&gt;% \n  animation_slider(currentvalue = list(visible = FALSE)) %&gt;% \n  animation_button(x = 0, xanchor = \"left\", y = 0, yanchor = \"bottom\")\ncplotly\n\n\n\n\nsaveWidget(frameableWidget(cplotly), here('results', 'figures', 'html-widgets', 'crime-location.html'))\n\n\nyearlyMurder &lt;- d1 %&gt;% mutate(\n  Zip.Char = as.character(Zip.Code),\n  years = year(Occurred.Date),\n  years = as.character(years)\n  ) %&gt;% filter(years != 2024, Crime.Category == 'MURDER') %&gt;% \n  group_by(Zip.Char, years) %&gt;% \n  summarize(murder.count  = n()) %&gt;% \n  ungroup() %&gt;% \n  as.data.frame()\n\n`summarise()` has grouped output by 'Zip.Char'. You can override using the\n`.groups` argument.\n\nyearlyMurderAll &lt;- left_join(allYrsZips, yearlyMurder, by = join_by(Zip.Char, years))  %&gt;% \n  inner_join(tig_zips, by = join_by(Zip.Char == ZCTA5CE20)) %&gt;% \n  mutate(murder.count = ifelse(is.na(murder.count), 0, murder.count))\n\n\ncpeth2 &lt;- yearlyMurderAll %&gt;% #filter(murder.count &gt;= 1) %&gt;% \n  ggplot() +\n  geom_sf(aes(geometry = geometry, fill = murder.count, frame = years)) +\n  #geom_sf_label(aes(label = paste(Zip.Char, '&lt;br&gt;', murder.count))) +\n  scale_fill_gradient2(low = \"#E0144C\", mid = '#FFFFFF', high = \"#000067\", \n                       trans = 'reverse') +\n  theme_classic() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.text.y=element_blank(),\n        line = element_blank(),\n        plot.title = element_text(hjust = 0, size = 15)) +\n  labs(fill = 'Murder Count',\n       title = 'Murder by Location Over Time')\n\nWarning in layer_sf(geom = GeomSf, data = data, mapping = mapping, stat = stat,\n: Ignoring unknown aesthetics: frame\n\ncplotly2 &lt;- cpeth2 %&gt;% ggplotly() %&gt;% \n  layout(hoverlabel = text) %&gt;% \n  animation_opts(1500, 1) %&gt;% \n  animation_slider(currentvalue = list(visible = FALSE)) %&gt;% \n  animation_button(x = 0, xanchor = \"left\", y = 0, yanchor = \"bottom\")\ncplotly2\n\n\n\n\n\n\ncatYearly &lt;- d1 %&gt;% mutate(\n  years = year(Occurred.Date)\n  #years = as.character(years)\n  ) %&gt;% filter(years != 2024) %&gt;% \n  group_by(Crime.Category, years) %&gt;% \n  summarize(crim.count  = n()) %&gt;% \n  ungroup() %&gt;% \n  as.data.frame()\n\n`summarise()` has grouped output by 'Crime.Category'. You can override using\nthe `.groups` argument.\n\nallYrsCats &lt;- expand.grid(years = unique(catYearly$years), Crime.Category = unique(catYearly$Crime.Category))\n\nyearlyAllCats &lt;- left_join(allYrsCats, catYearly, by = join_by(Crime.Category, years))  %&gt;% \n  mutate(crim.count = ifelse(is.na(crim.count), 0, crim.count))\n\ncatbar &lt;- yearlyAllCats %&gt;%  \n  ggplot() +\n  geom_bar(aes(y= fct_reorder(Crime.Category, crim.count), x = crim.count, frame = years), stat = 'identity') +\n  labs(x = 'Count of Offense', y = 'Offense Description', title = 'Frequncy of Crim by Offense Category')\n\nWarning in geom_bar(aes(y = fct_reorder(Crime.Category, crim.count), x =\ncrim.count, : Ignoring unknown aesthetics: frame\n\n  #scale_x_continuous(transform = 'log')\ncatbar\n\n\n\n\n\n\n\nfig &lt;- plot_ly(yearlyAllCats,\n  x = ~crim.count,\n  y = ~fct_reorder(Crime.Category, crim.count),\n  type = 'bar',\n  showlegend = F,\n  frame = ~years,\n  marker = list(color = '#000067')\n)\n\ncatbarly &lt;- fig %&gt;% \n  layout(\n    yaxis = list(\n      title = '', tickangle = -30, tickfont = list(size = 8)\n      ), \n    xaxis = list(\n      title = 'Count of Occurence' \n    ),\n    title = list(\n      text = 'Yearly Occurence of Crime by Category',\n      y = 0.99, x = 0.1, xanchor = 'left', yanchor = 'top',\n      font = list(\n        size = 18\n      )\n      )\n    )\n\ncatbarly\n\n\n\n\nsaveWidget(frameableWidget(catbarly), here('results', 'figures', 'html-widgets', 'crime-cat.html'))\n\nFirst table is intended to act as a translation key between description and category, but also serves well to illustrate the motivation behind the categorizations.\n\nRecatTable &lt;- d1 %&gt;% \n  filter(year(Occurred.Date) &lt; 2024) %&gt;% \n  mutate(\n  Occur.Years = trunc.Date(Occurred.Date, 'years')\n  ) %&gt;% \n  group_by(Crime.Category, Highest.Offense.Description, Occur.Years) %&gt;% \n  summarize(\n    Count = n(),\n    .groups = 'drop'\n  ) %&gt;% \n  arrange(by = Occur.Years) %&gt;%\n  group_by(Crime.Category, Highest.Offense.Description) %&gt;% \n  summarize(\n    mean = mean(Count),\n    sd = sd(Count),\n    sum = sum(Count),\n    .groups = 'drop'\n  ) %&gt;% \n  gt() %&gt;% \n  tab_header(\n    title = md('**Crime Recategorization Key**')\n    ) %&gt;% \n  cols_label(\n    Crime.Category = md('**New Category**'),\n    Highest.Offense.Description = md('**Original Description**'),\n    mean = md('**Mean Occurrence Across Years**'),\n    sd = md('**Standard Deviation of Yearly Occurrence**'),\n    sum = md('**Total Occurrences**')\n  ) %&gt;% \n  fmt_number(columns = mean:sd, decimals = 1) %&gt;% \n  opt_interactive()\n\ngtsave(RecatTable, 'recat-table.html', here('results', 'figures', 'html-widgets'))\n#saveWidget(frameableWidget(RecatTable), here('results', 'figures', 'html-widgets', 'recat-table.html'))\nRecatTable\n\n\n\n\nCrime Recategorization Key\n\n\n\n\n\n\n\n\nPlot highlights the advantages in the categorization, with inclusion of sparklines for clarity on movement over time.\n\nnewCatTable &lt;- d1 %&gt;%\n  filter(year(Occurred.Date) &lt; 2024) %&gt;% \n  mutate(\n  Occur.Years = trunc.Date(Occurred.Date, 'years')\n  ) %&gt;% \n  group_by(Crime.Category, Occur.Years) %&gt;% \n  summarize(\n    Count = n(),\n    .groups = 'drop'\n  ) %&gt;% \n  arrange(by = Occur.Years) %&gt;%\n  group_by(Crime.Category) %&gt;% \n  summarize(\n    mean = mean(Count),\n    sd = sd(Count),\n    list = list(Count),\n    .groups = 'drop'\n  ) %&gt;% \n  gt() %&gt;% \n  tab_header(\n    title = md('**Crime Categories Over Time**')\n    ) %&gt;% \n  cols_label(\n    Crime.Category = md('**Crime Category**'),\n    mean = md('**Mean Occurrence Across Years**'),\n    sd = md('**Standard Deviation of Yearly Occurrence**'),\n    list = md('**Change Over Time**')\n  ) %&gt;% \n  fmt_number(columns = mean:sd, decimals = 1) %&gt;% \n  gtExtras::gt_plt_sparkline(column = list, type = 'ref_iqr', same_limit = FALSE) %&gt;% \n  tab_footnote('Note: Sparklines have independent axes, comparisons should not be considered across different categories')\ngtsave(newCatTable, 'new-cat-table.html', here('results', 'figures', 'html-widgets'))\n\nWarning in geom_segment(aes(x = min(.data$x), y = stats::median(.data$y), : All aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = min(.data$x), y = stats::median(.data$y), : All aesthetics have length 1, but the data has 15 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = min(.data$x), y = stats::median(.data$y), : All aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = min(.data$x), y = stats::median(.data$y), : All aesthetics have length 1, but the data has 8 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = min(.data$x), y = stats::median(.data$y), : All aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\nnewCatTable\n\nWarning in geom_segment(aes(x = min(.data$x), y = stats::median(.data$y), : All aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = min(.data$x), y = stats::median(.data$y), : All aesthetics have length 1, but the data has 15 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = min(.data$x), y = stats::median(.data$y), : All aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = min(.data$x), y = stats::median(.data$y), : All aesthetics have length 1, but the data has 8 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = min(.data$x), y = stats::median(.data$y), : All aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 21 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCrime Categories Over Time\n\n\nCrime Category\nMean Occurrence Across Years\nStandard Deviation of Yearly Occurrence\nChange Over Time\n\n\n\n\nABUSE OF OFFICE\n6.4\n3.9\n\n\n\n   2.0\n\n\n\nAGGRAVATED ASSAULT\n2,029.0\n411.7\n\n\n\n   2.7K\n\n\n\nALCOHOL RELATED\n4,876.4\n1,366.1\n\n\n\n   2.3K\n\n\n\nAUTO THEFT\n2,822.5\n1,233.1\n\n\n\n   6.8K\n\n\n\nBRIBERY\n1.3\n0.8\n\n\n\n   2.0\n\n\n\nBURGLARY\n6,135.7\n1,565.7\n\n\n\n   4.5K\n\n\n\nCRIMINAL CONSPIRACY\n29.8\n16.1\n\n\n\n   17.0\n\n\n\nCRIMINAL MISCHIEF\n7,827.0\n1,735.4\n\n\n\n   5.9K\n\n\n\nDISORDERLY CONDUCT\n1,159.0\n254.1\n\n\n\n   1.1K\n\n\n\nDRUG RELATED\n6,058.4\n1,811.2\n\n\n\n   3.1K\n\n\n\nFINANCIAL CRIME\n1,458.5\n571.2\n\n\n\n   879.0\n\n\n\nFRAUD\n2,805.2\n459.4\n\n\n\n   2.0K\n\n\n\nGAMBLING RELATED\n40.7\n26.8\n\n\n\n   3.0\n\n\n\nGENERAL DISTURBANCE\n14,422.1\n2,150.4\n\n\n\n   11.4K\n\n\n\nHARASSMENT\n3,513.2\n1,037.9\n\n\n\n   2.0K\n\n\n\nHARM OF VULNERABLE PERSONS\n1,173.2\n321.2\n\n\n\n   1.0K\n\n\n\nJUSTIFIED HOMICIDE\n5.4\n3.1\n\n\n\n   6.0\n\n\n\nLARGE SCALE THREAT\n1,061.8\n194.2\n\n\n\n   1.0K\n\n\n\nLITTERING\n132.8\n104.5\n\n\n\n   11.0\n\n\n\nMURDER\n40.0\n16.2\n\n\n\n   65.0\n\n\n\nNON-COMPLIANCE\n5,744.0\n2,966.8\n\n\n\n   1.5K\n\n\n\nOTHER\n920.3\n174.7\n\n\n\n   766.0\n\n\n\nPROTECTIVE ORDER\n980.9\n372.2\n\n\n\n   491.0\n\n\n\nRAPE\n741.7\n150.3\n\n\n\n   480.0\n\n\n\nROBBERY\n1,082.8\n201.3\n\n\n\n   889.0\n\n\n\nSEX OFFENSE\n687.4\n119.0\n\n\n\n   877.0\n\n\n\nSEX OFFENSE INVOLVING A CHILD\n290.6\n84.4\n\n\n\n   171.0\n\n\n\nSIMPLE ASSAULT\n10,780.3\n812.8\n\n\n\n   8.9K\n\n\n\nTHEFT\n32,048.1\n4,129.8\n\n\n\n   23.6K\n\n\n\nTRESPASSING\n2,846.7\n1,236.5\n\n\n\n   2.5K\n\n\n\nUNLAWFUL RESTRAINT\n59.5\n11.5\n\n\n\n   89.0\n\n\n\nVOCO\n3,251.1\n2,688.0\n\n\n\n   266.0\n\n\n\nWEAPON RELATED\n548.4\n109.8\n\n\n\n   777.0\n\n\n\n\nNote: Sparklines have independent axes, comparisons should not be considered across different categories"
  },
  {
    "objectID": "HYLTIN-PII-project/code/processing-code/processingfile.html",
    "href": "HYLTIN-PII-project/code/processing-code/processingfile.html",
    "title": "Cleaning up Crime",
    "section": "",
    "text": "Processing script\n\nSetup\nLoad needed packages.\nPacman::p_load makes sure they are installed before auto-loading.\ntidyverse for general data processing/cleaning.\nlubridate for cleaning/re-coding date fields.\nskimr for nice visualization of data.\nhere to set paths.\n\n#install.packages(pacman)\npacman::p_load(tidyverse,lubridate,skimr,here, stringr)\n\n\n\n\nData loading\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"data\",\"raw-data\",\"raw-crime-060424.csv\")\nrawdata &lt;- read.csv(data_location, TRUE)\n\n\n\nCheck data\nFirst we can look at the codebook, which can be found here. I will recreate it here as it currently exists, and I will have a copy of the pdf attachment codebook included in the repo as well. However, please note if this analysis is ever repeated that careful consideration should be taken to ensure field have not been changed, removed, or redefined.\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nIncident Number\nIncident report number\n\n\nHighest Offense Description\nDescription\n\n\nHighest Offense Code\nCode\n\n\nFamily Violence\nIncident involves family violence? Y = yes, N = no\n\n\nOccurred Date Time\nDate and time (combined) incident occurred\n\n\nOccurred Date\nDate the incident occurred\n\n\nOccurred Time\nTime the incident occurred\n\n\nReport Date Time\nDate and time (combined) incident was reported\n\n\nReport Date\nDate the incident was reported\n\n\nReport Time\nTime the incident was reported\n\n\nLocation Type\nGeneral description of the premise where the incident occurred\n\n\nAddress\nIncident location\n\n\nZip code\nZip code where incident occurred\n\n\nCouncil District\nAustin city council district where the incident occurred\n\n\nAPD Sector\nAPD sector where incident occurred\n\n\nAPD District\nAPD district where incident occurred\n\n\nPRA\nAPD police reporting area where incident occurred\n\n\nCensus Tract\nCensus tract where incident occurred\n\n\nClearance Status\nHow/whether crime was solved (see Clearance lookup)\n\n\nClearance Date\nDate crime was solved\n\n\nUCR Category\nCode for the most serious crimes identified by the FBI as part of its Uniform Crime Reporting program\n\n\nCategory Description\nDescription for the most serious crimes identified by the FBI as part of its Uniform Crime Reporting program\n\n\nX-coordinate\nX-coordinate where the incident occurred\n\n\nY-coordinate\nY-coordinate where incident occurred\n\n\nLatitude\nLatitude where incident occurred\n\n\nLongitude\nLongitude where the incident occurred\n\n\nLocation\n3rd party generated spatial column\n\n\n\n\n\n\nClearance lookup\n\n\n\n\n\nC\nCleared by Arrest\n\n\nO\nCleared by Exception\n\n\nN\nNot cleared\n\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 2,470,867\nColumns: 27\n$ Incident.Number             &lt;dbl&gt; 2006471156, 20045044338, 2006960811, 20138…\n$ Highest.Offense.Description &lt;chr&gt; \"FAMILY DISTURBANCE\", \"TAMPERING WITH ID N…\n$ Highest.Offense.Code        &lt;int&gt; 3400, 2719, 3400, 1707, 204, 200, 200, 170…\n$ Family.Violence             &lt;chr&gt; \"N\", \"N\", \"N\", \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N…\n$ Occurred.Date.Time          &lt;chr&gt; \"02/16/2006 02:25:00 PM\", \"09/14/2004 03:3…\n$ Occurred.Date               &lt;chr&gt; \"02/16/2006\", \"09/14/2004\", \"04/06/2006\", …\n$ Occurred.Time               &lt;int&gt; 1425, 1532, 1029, 1, 105, 1915, 1200, 2228…\n$ Report.Date.Time            &lt;chr&gt; \"02/16/2006 02:25:00 PM\", \"09/14/2004 03:3…\n$ Report.Date                 &lt;chr&gt; \"02/16/2006\", \"09/14/2004\", \"04/06/2006\", …\n$ Report.Time                 &lt;int&gt; 1425, 1532, 1029, 1656, 105, 1706, 1200, 2…\n$ Location.Type               &lt;chr&gt; \"RESIDENCE / HOME\", \"\", \"RESIDENCE / HOME\"…\n$ Address                     &lt;chr&gt; \"7000 DECKER 1422\", \"3301 CR 100\", \"5005 W…\n$ Zip.Code                    &lt;int&gt; NA, NA, NA, 78705, 78744, 78748, 78757, 78…\n$ Council.District            &lt;int&gt; NA, NA, NA, 9, 2, 5, 7, 9, NA, NA, NA, NA,…\n$ APD.Sector                  &lt;chr&gt; \"\", \"\", \"\", \"BA\", \"FR\", \"FR\", \"ID\", \"DA\", …\n$ APD.District                &lt;chr&gt; \"\", \"\", \"\", \"1\", \"8\", \"2\", \"7\", \"2\", \"6\", …\n$ PRA                         &lt;chr&gt; \"\", \"\", \"\", \"348\", \"530\", \"542\", \"247\", \"4…\n$ Census.Tract                &lt;dbl&gt; NA, NA, NA, 4.00, 24.41, 24.38, 405.00, 23…\n$ Clearance.Status            &lt;chr&gt; \"\", \"N\", \"N\", \"C\", \"C\", \"C\", \"O\", \"\", \"N\",…\n$ Clearance.Date              &lt;chr&gt; \"\", \"09/14/2004\", \"05/01/2006\", \"04/11/201…\n$ UCR.Category                &lt;chr&gt; \"\", \"\", \"\", \"11C\", \"11A\", \"11A\", \"11A\", \"1…\n$ Category.Description        &lt;chr&gt; \"\", \"\", \"\", \"Rape\", \"Rape\", \"Rape\", \"Rape\"…\n$ X.coordinate                &lt;int&gt; NA, NA, NA, 0, 0, 0, 0, 0, NA, NA, 0, 0, 0…\n$ Y.coordinate                &lt;int&gt; NA, NA, NA, 0, 0, 0, 0, 0, NA, NA, 0, 0, 0…\n$ Latitude                    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Longitude                   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Location                    &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n\nsummary(rawdata)\n\n Incident.Number     Highest.Offense.Description Highest.Offense.Code\n Min.   :2.004e+04   Length:2470867              Min.   : 100        \n 1st Qu.:2.005e+10   Class :character            1st Qu.: 601        \n Median :2.011e+10   Mode  :character            Median :1199        \n Mean   :6.022e+10                               Mean   :1691        \n 3rd Qu.:2.017e+10                               3rd Qu.:2716        \n Max.   :2.024e+12                               Max.   :8905        \n                                                                     \n Family.Violence    Occurred.Date.Time Occurred.Date      Occurred.Time \n Length:2470867     Length:2470867     Length:2470867     Min.   :   0  \n Class :character   Class :character   Class :character   1st Qu.: 805  \n Mode  :character   Mode  :character   Mode  :character   Median :1425  \n                                                          Mean   :1319  \n                                                          3rd Qu.:1930  \n                                                          Max.   :2400  \n                                                          NA's   :60    \n Report.Date.Time   Report.Date         Report.Time   Location.Type     \n Length:2470867     Length:2470867     Min.   :   0   Length:2470867    \n Class :character   Class :character   1st Qu.: 915   Class :character  \n Mode  :character   Mode  :character   Median :1406   Mode  :character  \n                                       Mean   :1329                     \n                                       3rd Qu.:1849                     \n                                       Max.   :2359                     \n                                       NA's   :1                        \n   Address             Zip.Code     Council.District  APD.Sector       \n Length:2470867     Min.   :    0   Min.   : 1.00    Length:2470867    \n Class :character   1st Qu.:78717   1st Qu.: 3.00    Class :character  \n Mode  :character   Median :78741   Median : 4.00    Mode  :character  \n                    Mean   :78731   Mean   : 4.96                      \n                    3rd Qu.:78752   3rd Qu.: 7.00                      \n                    Max.   :78759   Max.   :10.00                      \n                    NA's   :9180    NA's   :39936                      \n APD.District           PRA             Census.Tract      Clearance.Status  \n Length:2470867     Length:2470867     Min.   :     1.0   Length:2470867    \n Class :character   Class :character   1st Qu.:    15.0   Class :character  \n Mode  :character   Mode  :character   Median :    23.2   Mode  :character  \n                                       Mean   :   253.5                     \n                                       3rd Qu.:   338.0                     \n                                       Max.   :960100.0                     \n                                       NA's   :17241                        \n Clearance.Date     UCR.Category       Category.Description  X.coordinate      \n Length:2470867     Length:2470867     Length:2470867       Min.   :-32621852  \n Class :character   Class :character   Class :character     1st Qu.:  3108417  \n Mode  :character   Mode  :character   Mode  :character     Median :  3117292  \n                                                            Mean   :  3075713  \n                                                            3rd Qu.:  3126591  \n                                                            Max.   : 38895240  \n                                                            NA's   :5089       \n  Y.coordinate         Latitude       Longitude        Location        \n Min.   :       0   Min.   : 0.00   Min.   :-99.16   Length:2470867    \n 1st Qu.:10057416   1st Qu.:30.23   1st Qu.:-97.76   Class :character  \n Median :10073004   Median :30.28   Median :-97.73   Mode  :character  \n Mean   : 9944021   Mean   :30.29   Mean   :-97.73                     \n 3rd Qu.:10100561   3rd Qu.:30.35   3rd Qu.:-97.70                     \n Max.   :41158776   Max.   :42.17   Max.   :  0.00                     \n NA's   :5082       NA's   :38139   NA's   :38139                      \n\nhead(rawdata)\n\n  Incident.Number    Highest.Offense.Description Highest.Offense.Code\n1      2006471156             FAMILY DISTURBANCE                 3400\n2     20045044338       TAMPERING WITH ID NUMBER                 2719\n3      2006960811             FAMILY DISTURBANCE                 3400\n4      2013851154 SEXUAL ASSAULT OF CHILD/OBJECT                 1707\n5     20161800084                RAPE OF A CHILD                  204\n6      2010701921                           RAPE                  200\n  Family.Violence     Occurred.Date.Time Occurred.Date Occurred.Time\n1               N 02/16/2006 02:25:00 PM    02/16/2006          1425\n2               N 09/14/2004 03:32:00 PM    09/14/2004          1532\n3               N 04/06/2006 10:29:00 AM    04/06/2006          1029\n4               Y 01/01/2009 12:01:00 AM    01/01/2009             1\n5               Y 06/28/2016 01:05:00 AM    06/28/2016           105\n6               Y 03/04/2010 07:15:00 PM    03/04/2010          1915\n        Report.Date.Time Report.Date Report.Time    Location.Type\n1 02/16/2006 02:25:00 PM  02/16/2006        1425 RESIDENCE / HOME\n2 09/14/2004 03:32:00 PM  09/14/2004        1532                 \n3 04/06/2006 10:29:00 AM  04/06/2006        1029 RESIDENCE / HOME\n4 03/26/2013 04:56:00 PM  03/26/2013        1656 RESIDENCE / HOME\n5 06/28/2016 01:05:00 AM  06/28/2016         105 RESIDENCE / HOME\n6 03/11/2010 05:06:00 PM  03/11/2010        1706 RESIDENCE / HOME\n                   Address Zip.Code Council.District APD.Sector APD.District\n1         7000 DECKER 1422       NA               NA                        \n2              3301 CR 100       NA               NA                        \n3        5005 W FRANCES PL       NA               NA                        \n4      900 BLOCK E 32ND ST    78705                9         BA            1\n5 6900 BLOCK BRANCHWOOD DR    78744                2         FR            8\n6   400 BLOCK ANGEL OAK ST    78748                5         FR            2\n  PRA Census.Tract Clearance.Status Clearance.Date UCR.Category\n1               NA                                             \n2               NA                N     09/14/2004             \n3               NA                N     05/01/2006             \n4 348         4.00                C     04/11/2013          11C\n5 530        24.41                C     07/01/2016          11A\n6 542        24.38                C     03/18/2010          11A\n  Category.Description X.coordinate Y.coordinate Latitude Longitude Location\n1                                NA           NA       NA        NA         \n2                                NA           NA       NA        NA         \n3                                NA           NA       NA        NA         \n4                 Rape            0            0       NA        NA         \n5                 Rape            0            0       NA        NA         \n6                 Rape            0            0       NA        NA         \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n2470867\n\n\nNumber of columns\n27\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n16\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHighest.Offense.Description\n0\n1\n3\n48\n0\n436\n0\n\n\nFamily.Violence\n0\n1\n1\n1\n0\n3\n0\n\n\nOccurred.Date.Time\n0\n1\n0\n22\n137\n1744152\n0\n\n\nOccurred.Date\n0\n1\n10\n10\n0\n7823\n0\n\n\nReport.Date.Time\n0\n1\n0\n22\n1\n2176754\n0\n\n\nReport.Date\n0\n1\n10\n10\n0\n7825\n0\n\n\nLocation.Type\n0\n1\n0\n47\n18945\n47\n0\n\n\nAddress\n0\n1\n0\n100\n12\n252579\n0\n\n\nAPD.Sector\n0\n1\n0\n6\n3359\n65\n0\n\n\nAPD.District\n0\n1\n0\n5\n4055\n71\n0\n\n\nPRA\n0\n1\n0\n4\n5635\n762\n0\n\n\nClearance.Status\n0\n1\n0\n1\n618707\n5\n0\n\n\nClearance.Date\n0\n1\n0\n10\n350297\n7815\n0\n\n\nUCR.Category\n0\n1\n0\n3\n1557816\n17\n0\n\n\nCategory.Description\n0\n1\n0\n18\n1557816\n8\n0\n\n\nLocation\n0\n1\n0\n27\n38139\n221001\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nIncident.Number\n0\n1.00\n6.022168e+10\n2.893008e+11\n20035.00\n2.005330e+10\n2.010505e+10\n2.017187e+10\n2.024242e+12\n▇▁▁▁▁\n\n\nHighest.Offense.Code\n0\n1.00\n1.691360e+03\n1.219150e+03\n100.00\n6.010000e+02\n1.199000e+03\n2.716000e+03\n8.905000e+03\n▇▅▁▁▁\n\n\nOccurred.Time\n60\n1.00\n1.319010e+03\n7.176300e+02\n0.00\n8.050000e+02\n1.425000e+03\n1.930000e+03\n2.400000e+03\n▆▃▆▇▇\n\n\nReport.Time\n1\n1.00\n1.329500e+03\n6.544400e+02\n0.00\n9.150000e+02\n1.406000e+03\n1.849000e+03\n2.359000e+03\n▅▅▇▇▇\n\n\nZip.Code\n9180\n1.00\n7.873064e+04\n3.763500e+02\n0.00\n7.871700e+04\n7.874100e+04\n7.875200e+04\n7.875900e+04\n▁▁▁▁▇\n\n\nCouncil.District\n39936\n0.98\n4.960000e+00\n2.840000e+00\n1.00\n3.000000e+00\n4.000000e+00\n7.000000e+00\n1.000000e+01\n▅▇▃▃▅\n\n\nCensus.Tract\n17241\n0.99\n2.535000e+02\n4.194780e+03\n1.00\n1.500000e+01\n2.324000e+01\n3.380000e+02\n9.601000e+05\n▇▁▁▁▁\n\n\nX.coordinate\n5089\n1.00\n3.075713e+06\n3.978973e+05\n-32621852.00\n3.108417e+06\n3.117292e+06\n3.126591e+06\n3.889524e+07\n▁▁▇▁▁\n\n\nY.coordinate\n5082\n1.00\n9.944021e+06\n1.160975e+06\n0.00\n1.005742e+07\n1.007300e+07\n1.010056e+07\n4.115878e+07\n▁▇▁▁▁\n\n\nLatitude\n38139\n0.98\n3.029000e+01\n2.000000e-01\n0.00\n3.023000e+01\n3.028000e+01\n3.035000e+01\n4.217000e+01\n▁▁▁▇▁\n\n\nLongitude\n38139\n0.98\n-9.773000e+01\n4.700000e-01\n-99.16\n-9.776000e+01\n-9.773000e+01\n-9.770000e+01\n0.000000e+00\n▇▁▁▁▁\n\n\n\n\nunique(rawdata$Location.Type)\n\n [1] \"RESIDENCE / HOME\"                               \n [2] \"\"                                               \n [3] \"OTHER / UNKNOWN\"                                \n [4] \"FIELD / WOODS\"                                  \n [5] \"HOTEL / MOTEL / ETC.\"                           \n [6] \"GOVERNMENT / PUBLIC BUILDING\"                   \n [7] \"DEPARTMENT / DISCOUNT STORE\"                    \n [8] \"COMMERCIAL / OFFICE BUILDING\"                   \n [9] \"GROCERY / SUPERMARKET\"                          \n[10] \"PARK / PLAYGROUND\"                              \n[11] \"CONVENIENCE STORE\"                              \n[12] \"LIQUOR STORE\"                                   \n[13] \"CHURCH / SYNAGOGUE / TEMPLE / MOSQUE\"           \n[14] \"CYBERSPACE\"                                     \n[15] \"CAMP / CAMPGROUND\"                              \n[16] \"CONSTRUCTION SITE\"                              \n[17] \"DAYCARE FACILITY\"                               \n[18] \"COMMUNITY CENTER\"                               \n[19] \"SCHOOL - ELEMENTARY / SECONDARY\"                \n[20] \"AUTO DEALERSHIP NEW / USED\"                     \n[21] \"SHOPPING MALL\"                                  \n[22] \"BANK / SAVINGS & LOAN\"                          \n[23] \"SCHOOL - COLLEGE / UNIVERSITY\"                  \n[24] \"AMUSEMENT PARK\"                                 \n[25] \"HWY / ROAD / ALLEY/ STREET/ SIDEWALK\"           \n[26] \"ABANDONED/CONDEMNED STRUCTURE\"                  \n[27] \"INDUSTRIAL SITE\"                                \n[28] \"PARKING /DROP LOT/ GARAGE\"                      \n[29] \"AIR / BUS / TRAIN TERMINAL\"                     \n[30] \"DRUG STORE / DOCTOR'S OFFICE / HOSPITAL\"        \n[31] \"JAIL / PRISON/PENITENTIARY/CORRECTIONS FACILITY\"\n[32] \"RESTAURANT\"                                     \n[33] \"BAR / NIGHTCLUB\"                                \n[34] \"LAKE / WATERWAY/BEACH\"                          \n[35] \"SPECIALTY  STORE\"                               \n[36] \"SERVICE/ GAS STATION\"                           \n[37] \"SCHOOL/COLLEGE\"                                 \n[38] \"SHELTER-MISSION / HOMELESS\"                     \n[39] \"ATM SEPARATE FROM BANK\"                         \n[40] \"GAMBLING FACILITY / CASINO / RACE TRACK\"        \n[41] \"RENTAL STORAGE FACILITY\"                        \n[42] \"MILITARY INSTALLATION\"                          \n[43] \"ARENA / STADIUM / FAIRGROUNDS / COLISEUM\"       \n[44] \"FARM FACILITY\"                                  \n[45] \"TRIBAL LANDS\"                                   \n[46] \"REST AREA\"                                      \n[47] \"DOCK / WHARF / FREIGHT / MODAL TERMINAL\"        \n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\n\n\nDates and Times\nWe have seven time related fields in this dataset: Occurred.Date.Time, Occurred.Date, Occurred.Time, Report.Date.Time, Report.Date, Report.Time, and Clearance.Date.\n- The Date-Time and Date variables were read in as strings, and the two Time variables were read in as numeric. So that we can treat these variables as actual representations of a point in time, we will need to re-code them into date, time, and date-time values.\n- Many of the variables have missing or empty values where they shouldn’t. For example, there are some instances where the Occurred.Date.Time variable is empty, but the Occurred.Date and Occurred.Time values are not.\n- Finally, there are some instances where the time value does not make any sense. Values like 60 or 70 are impossible to occur and somewhat vague, so special consideration should be taken to account for these as well.\n\nd1 &lt;- rawdata %&gt;% mutate(\n  test = (str_sub(as.character(Occurred.Time), -2, -1)\n          %&gt;% as.numeric()),\n  Occurred.Date = mdy(Occurred.Date)\n  ,Report.Date = mdy(Report.Date)\n  ,Occurred.Date.Time = parse_date_time(Occurred.Date.Time,'%m/%d/%Y %I:%M:%S %p')\n  ,Report.Date.Time = parse_date_time(Report.Date.Time,'%m/%d/%Y %I:%M:%S %p')\n  ,Clearance.Date = mdy(Clearance.Date)\n  ) %&gt;% mutate(\n    Occurred.Time = if_else(\n      is.na(Occurred.Time) | test &gt;= 60, \n      if_else(\n        Occurred.Date == Report.Date,\n        if_else(\n          is.na(Report.Time), \n          2359, Report.Time\n          ), 2359\n        ), if_else(\n          Occurred.Time == 2400, \n          2359, Occurred.Time\n          ) \n      ) %&gt;% as.character()\n    ) %&gt;% mutate(\n      Occurred.Time = paste0('000',Occurred.Time,'00') %&gt;% \n        str_sub(.,-6,-1) %&gt;% \n        parse_time('%H%M%S')\n      ,Occurred.Date.Time = if_else(\n        is.na(Occurred.Date.Time), \n        ymd_hms(paste(Occurred.Date, Occurred.Time)), Occurred.Date.Time\n        )\n      ,Report.Time = if_else(\n        is.na(Report.Time), 2359, Report.Time) %&gt;%\n        as.character() %&gt;% \n        paste0('000',.,'00') %&gt;% \n        str_sub(.,-6,-1) %&gt;% \n        parse_time('%H%M%S')\n      ,Report.Date.Time = if_else(\n        is.na(Report.Date.Time),\n        ymd_hms(paste(Report.Date, Report.Time)), Report.Date.Time\n      )\n      ) %&gt;% select(-test)\nskim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n2470867\n\n\nNumber of columns\n27\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n11\n\n\nDate\n3\n\n\ndifftime\n2\n\n\nnumeric\n9\n\n\nPOSIXct\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHighest.Offense.Description\n0\n1\n3\n48\n0\n436\n0\n\n\nFamily.Violence\n0\n1\n1\n1\n0\n3\n0\n\n\nLocation.Type\n0\n1\n0\n47\n18945\n47\n0\n\n\nAddress\n0\n1\n0\n100\n12\n252579\n0\n\n\nAPD.Sector\n0\n1\n0\n6\n3359\n65\n0\n\n\nAPD.District\n0\n1\n0\n5\n4055\n71\n0\n\n\nPRA\n0\n1\n0\n4\n5635\n762\n0\n\n\nClearance.Status\n0\n1\n0\n1\n618707\n5\n0\n\n\nUCR.Category\n0\n1\n0\n3\n1557816\n17\n0\n\n\nCategory.Description\n0\n1\n0\n18\n1557816\n8\n0\n\n\nLocation\n0\n1\n0\n27\n38139\n221001\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Date\n0\n1.00\n2003-01-01\n2024-06-01\n2012-05-29\n7823\n\n\nReport.Date\n0\n1.00\n2002-11-29\n2024-06-02\n2012-06-07\n7825\n\n\nClearance.Date\n350297\n0.86\n2003-01-01\n2024-06-02\n2012-10-18\n7814\n\n\n\nVariable type: difftime\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Time\n0\n1\n0 secs\n86340 secs\n14:25:00\n1440\n\n\nReport.Time\n0\n1\n0 secs\n86340 secs\n14:06:00\n1440\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nIncident.Number\n0\n1.00\n6.022168e+10\n2.893008e+11\n20035.00\n2.005330e+10\n2.010505e+10\n2.017187e+10\n2.024242e+12\n▇▁▁▁▁\n\n\nHighest.Offense.Code\n0\n1.00\n1.691360e+03\n1.219150e+03\n100.00\n6.010000e+02\n1.199000e+03\n2.716000e+03\n8.905000e+03\n▇▅▁▁▁\n\n\nZip.Code\n9180\n1.00\n7.873064e+04\n3.763500e+02\n0.00\n7.871700e+04\n7.874100e+04\n7.875200e+04\n7.875900e+04\n▁▁▁▁▇\n\n\nCouncil.District\n39936\n0.98\n4.960000e+00\n2.840000e+00\n1.00\n3.000000e+00\n4.000000e+00\n7.000000e+00\n1.000000e+01\n▅▇▃▃▅\n\n\nCensus.Tract\n17241\n0.99\n2.535000e+02\n4.194780e+03\n1.00\n1.500000e+01\n2.324000e+01\n3.380000e+02\n9.601000e+05\n▇▁▁▁▁\n\n\nX.coordinate\n5089\n1.00\n3.075713e+06\n3.978973e+05\n-32621852.00\n3.108417e+06\n3.117292e+06\n3.126591e+06\n3.889524e+07\n▁▁▇▁▁\n\n\nY.coordinate\n5082\n1.00\n9.944021e+06\n1.160975e+06\n0.00\n1.005742e+07\n1.007300e+07\n1.010056e+07\n4.115878e+07\n▁▇▁▁▁\n\n\nLatitude\n38139\n0.98\n3.029000e+01\n2.000000e-01\n0.00\n3.023000e+01\n3.028000e+01\n3.035000e+01\n4.217000e+01\n▁▁▁▇▁\n\n\nLongitude\n38139\n0.98\n-9.773000e+01\n4.700000e-01\n-99.16\n-9.776000e+01\n-9.773000e+01\n-9.770000e+01\n0.000000e+00\n▇▁▁▁▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Date.Time\n0\n1\n2003-01-01 00:00:00\n2024-06-01 23:46:00\n2012-05-29 19:00:00\n1744224\n\n\nReport.Date.Time\n0\n1\n2002-11-29 05:30:00\n2024-06-02 01:20:00\n2012-06-07 09:47:00\n2176754\n\n\n\n\n\nThis should address all of the time-based variables. I may come back and update some of this, solely because I made some more analytic decisions regarding the empty values for Occurred.Date.Time. The logic is that if the time is missing, the occurrence would have had to have happened at least before the report time, though if the report occurred on a different day we assume the occurrence would have to be at least by midnight the same day. The number of empties, once the three “Occurred” variables are coalesced where appropriate, are relatively low, so I don’t expect this to have significant effect on any results either way.\n\n\nCategorical Variables\nIn the initial looks at the dataset a few columns stand out as potential factor variables.\n- Family Violence is a mostly complete field, though a minor oddity to it is the existence of a third factor when the codebook says it should only be two.\n- APD Sector, APD District, and PRA may be the best values to serve as a categorical representation of location, since it has relatively few missing variables. Zip code may also work and be more consumable to the average person, however it does have more missing than the others mentioned. Lastly, X and Y Coordinates, though not naturally categorical, may be able to be used to create categorical groupings considering their completeness.\n- Location Type is another good categorical variable. There are a lot of categories, but with one like this it may be easy to combine some of them. Empty values are climbing, but still less than 1% of the data.\n- Highest Offense code has 396 distinct values, which would be far too many for categorical use, but again may be able to be grouped. - Clearance Status sounds like a very interesting variable to use, however it is one of the variables with the highest number of empties/missing. Furthermore, it’s confounded by Clearance Date, which has fewer (but still a lot of) missing and empty values. This is one that may take some outside research, on what constitutes a “clearance” and what it means in relation to a clearance date. - UCR Category and Description also sound like very interesting variables, but are the worst offenders in terms of missing data. This makes sense from info from the codebook, that it is reserved for designation of the most serious crimes by the FBI. In a way this could even mean the null values are also valuable, because they very well may indicate less serious crimes.\n\nd2&lt;- d1 %&gt;% \n  mutate(\n    Family.Violence = toupper(Family.Violence)\n    ,Location.Type = if_else(\n      Location.Type == '',\n      'Unknown', Location.Type\n      )\n    ) %&gt;% \n  filter(!is.na(Zip.Code) & !(Zip.Code == 0) & !is.na(X.coordinate) & !is.na(Y.coordinate) & PRA != '',Clearance.Status != 9)\nskim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n2461621\n\n\nNumber of columns\n27\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n11\n\n\nDate\n3\n\n\ndifftime\n2\n\n\nnumeric\n9\n\n\nPOSIXct\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHighest.Offense.Description\n0\n1\n3\n48\n0\n436\n0\n\n\nFamily.Violence\n0\n1\n1\n1\n0\n2\n0\n\n\nLocation.Type\n0\n1\n7\n47\n0\n47\n0\n\n\nAddress\n0\n1\n8\n74\n0\n246951\n0\n\n\nAPD.Sector\n0\n1\n2\n5\n0\n14\n0\n\n\nAPD.District\n0\n1\n1\n2\n0\n21\n0\n\n\nPRA\n0\n1\n1\n4\n0\n742\n0\n\n\nClearance.Status\n0\n1\n0\n1\n615856\n4\n0\n\n\nUCR.Category\n0\n1\n0\n3\n1550375\n17\n0\n\n\nCategory.Description\n0\n1\n0\n18\n1550375\n8\n0\n\n\nLocation\n0\n1\n0\n27\n32335\n219842\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Date\n0\n1.00\n2003-01-01\n2024-06-01\n2012-05-28\n7823\n\n\nReport.Date\n0\n1.00\n2002-11-29\n2024-06-02\n2012-06-06\n7825\n\n\nClearance.Date\n348308\n0.86\n2003-01-01\n2024-06-02\n2012-10-17\n7814\n\n\n\nVariable type: difftime\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Time\n0\n1\n0 secs\n86340 secs\n14:25:00\n1440\n\n\nReport.Time\n0\n1\n0 secs\n86340 secs\n14:06:00\n1440\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nIncident.Number\n0\n1.00\n6.031558e+10\n2.896224e+11\n20035.00\n2.005329e+10\n2.010505e+10\n2.017186e+10\n2.024242e+12\n▇▁▁▁▁\n\n\nHighest.Offense.Code\n0\n1.00\n1.689080e+03\n1.218280e+03\n100.00\n6.010000e+02\n1.199000e+03\n2.716000e+03\n8.905000e+03\n▇▅▁▁▁\n\n\nZip.Code\n0\n1.00\n7.873243e+04\n2.510000e+01\n76574.00\n7.871700e+04\n7.874100e+04\n7.875200e+04\n7.875900e+04\n▁▁▁▁▇\n\n\nCouncil.District\n30699\n0.99\n4.960000e+00\n2.840000e+00\n1.00\n3.000000e+00\n4.000000e+00\n7.000000e+00\n1.000000e+01\n▅▇▃▃▅\n\n\nCensus.Tract\n8822\n1.00\n2.453700e+02\n3.363970e+03\n1.00\n1.500000e+01\n2.324000e+01\n3.380000e+02\n9.508000e+05\n▇▁▁▁▁\n\n\nX.coordinate\n0\n1.00\n3.075787e+06\n3.551571e+05\n0.00\n3.108421e+06\n3.117292e+06\n3.126595e+06\n3.231806e+06\n▁▁▁▁▇\n\n\nY.coordinate\n0\n1.00\n9.946761e+06\n1.147895e+06\n0.00\n1.005743e+07\n1.007300e+07\n1.010056e+07\n1.021550e+07\n▁▁▁▁▇\n\n\nLatitude\n32335\n0.99\n3.029000e+01\n8.000000e-02\n30.01\n3.023000e+01\n3.028000e+01\n3.035000e+01\n3.067000e+01\n▁▇▇▂▁\n\n\nLongitude\n32335\n0.99\n-9.773000e+01\n5.000000e-02\n-98.18\n-9.776000e+01\n-9.773000e+01\n-9.770000e+01\n-9.737000e+01\n▁▁▇▂▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nOccurred.Date.Time\n0\n1\n2003-01-01 00:00:00\n2024-06-01 23:46:00\n2012-05-28 23:09:00\n1738386\n\n\nReport.Date.Time\n0\n1\n2002-11-29 05:30:00\n2024-06-02 01:20:00\n2012-06-06 11:15:00\n2169726\n\n\n\n\nprint(count(d2)/count(d1))\n\n         n\n1 0.996258\n\n\nThe categorical variables look good for now. Up until this point little exploration has been done and there may be other re-codes or filters that would be worth including for other variables. Likely once I am further into the analysis I will return to this step to make more adjustments, but the changes I’ve made should provide a good starting point.\n\n\nString Variables\nThere are a few string variables that may be worth using for the analysis. Some of these I covered in the categorical variables, but for those that have excessive number of unique values a categorical value may not be practical.\n- Highest Offense Description has hundreds of unique values, but since this is essentially a description of the crimes committed there are likely commonalities that could lead to groupings. I can imagine more variables stemming from this as well, such as if the description contains certain keywords I could create indicator variables based on that presence. - Address is essentially another location based variable, and considering the number of them that have been cleaned for that purpose already it seems unlikely to need more. Still, the specificity offered by the address may be insightful, especially if i start drilling down into particular areas that suffer from the most crime. Potentially Doing something like a regular expression to extract a street name would be useful.\n- Location Type does not have an unreasonable number of unique values, so this variable may better fit as a categorical only. Still the max length of some of the values lends itself to be that there may be more worth extracting from the field.\nParsing out these string variables will likely be more of a cognitive load than the others would, so I will hold off on doing so for now. Once I am further along with the EDA I will have a better idea of what I want to be looking for within these variables anyway.\n\n\nAnd We’re Back\nThe string variable Highest.Offense.Description was just too rich to pass up. It would make for an excellent categorical variable however there are over 400 categories going off the variable as-is. UCR.Category is a very nice alternative with only a few levels to the category, but the field is mostly empty due to the nature of the variable. It represent an FBI Categorization known as Uniform Crime Reporting (UCR) for the most serious of crimes. So I decided to take the matter into my own hands. Number one, I would create a new field that categorizes the crime myself, based on categorizations in the UCR program. If the observation had values in the UCR field, I would take that one, if it did not, I impute a category based on the description in the Highest Offense Description field. This was mostly done through brute force, with string detection and research in two places, the FBI UCR archive (2019 was not the only year that had information on the categorizations but it was the most in a quick reference format), and the Texas Capitol Statutes website. I feel the need to address this up front, I am absolutely not a criminologist, these categorizations are solely for the sake of making my analysis easier. I think of this as grouping the crime descriptions from the perspective of a layman, me, because ultimately the average person observing crime statistics will roughly as little knowledge about them as I do. An expert could and would do a much better job than I did, but that wasn’t readily available so I worked with what I had. Famous last words of an analyst.\n\nunique(d2$Category.Description) %&gt;% as.data.frame()\n\n                   .\n1               Rape\n2                   \n3           Burglary\n4              Theft\n5 Aggravated Assault\n6            Robbery\n7         Auto Theft\n8             Murder\n\nunique(d2[c('Category.Description','Highest.Offense.Description')]) %&gt;% as.data.frame()\n\n        Category.Description                      Highest.Offense.Description\n1                       Rape                   SEXUAL ASSAULT OF CHILD/OBJECT\n2                       Rape                                  RAPE OF A CHILD\n3                       Rape                                             RAPE\n5                       Rape                         SEXUAL ASSAULT W/ OBJECT\n7                                              ASSAULT  CONTACT-SEXUAL NATURE\n8                                                                  HARASSMENT\n10                                              DEPENDENT AND NEGLECTED CHILD\n12                                                          INDECENT EXPOSURE\n13                                             INDECENCY WITH A CHILD/CONTACT\n17                  Burglary                      BURG OF RES - SEXUAL NATURE\n21                                                        ASSAULT WITH INJURY\n22                                             FAILURE TO REG AS SEX OFFENDER\n25                      Rape                                         AGG RAPE\n27                                                    STATUTORY RAPE OF CHILD\n28                     Theft                                THEFT FROM PERSON\n30                  Burglary                            BURGLARY OF RESIDENCE\n33                      Rape                   AGG SEXUAL ASSAULT CHILD/OBJEC\n42                                              INDECENCY WITH CHILD/EXPOSURE\n47                     Theft                                            THEFT\n51                                                            INJURY TO CHILD\n54                      Rape                              AGG RAPE OF A CHILD\n62                     Theft                                 THEFT OF BICYCLE\n65                                                         ASSAULT BY CONTACT\n80                                                                RETALIATION\n86                      Rape                       AGG FORCED SODOMY OF CHILD\n95                                             DISCLOSE/PROMO INTIMATE VISUAL\n98                                                          HARASSMENT ONLINE\n103                                            ASSAULT W/INJURY-FAM/DATE VIOL\n105                                                 INVASIVE VISUAL RECORDING\n127                     Rape                                    FORCED SODOMY\n144                                                         CRIMINAL TRESPASS\n159                                                     FALSE ALARM OR REPORT\n162                                                        TERRORISTIC THREAT\n163                                                        FAMILY DISTURBANCE\n191                                                          ENTICING A CHILD\n192                                                            IDENTITY THEFT\n203                                                             FRAUD - OTHER\n220                     Rape                      AGG SEXUAL ASSAULT W OBJECT\n257                                                      ONLINE IMPERSONATION\n277                                                        DATING DISTURBANCE\n291                                                INJURY DISABLED INDIVIDUAL\n300       Aggravated Assault                    AGG ASSAULT FAM/DATE VIOLENCE\n325                                               CUSTODY ARREST TRAFFIC WARR\n371                                                       DISTURBANCE - OTHER\n410                     Rape                                AGG FORCED SODOMY\n411                                                       FAILURE TO IDENTIFY\n448                                             ASSAULT BY CONTACT FAM/DATING\n466                     Rape                       EXPIRED-ATT SEXUAL ASSAULT\n468                                                         CRIMINAL MISCHIEF\n491       Aggravated Assault                                      AGG ASSAULT\n495                                                              PROSTITUTION\n512                                                                KIDNAPPING\n524                     Rape                          CONT SEX ABUSE OF CHILD\n528                                                                   PROWLER\n529                                            CHILD ENDANGERMENT- ABANDONMEN\n584                                            FELONY ENHANCEMENT/ASSLT W/INJ\n598                                                 SEXTING DEPICTING A MINOR\n610                                                           PUBLIC LEWDNESS\n628                                               VIOL CITY ORDINANCE - OTHER\n639                  Robbery                               ROBBERY BY ASSAULT\n668                                                   POSSESSION OF MARIJUANA\n692                                                   HARBORING RUNAWAY CHILD\n703                                                       WEAPON VIOL - OTHER\n706                                                WARRANT ARREST NON TRAFFIC\n736                  Robbery                                ROBBERY BY THREAT\n844                                            VIOL OF COURT ORDER-NON EPO-PO\n890                                                        UNLAWFUL RESTRAINT\n937                                              POSS CONTROLLED SUB/NARCOTIC\n984                                                 URINATING IN PUBLIC PLACE\n1048                                            INCEST-PROHIBITED SEX CONDUCT\n1073                                               APPLIC TO REVOKE PROBATION\n1080                                           INJURY TO CHILD (CARE/CUSTODY)\n1108                                                         INDECENT ASSAULT\n1168                 Robbery                        AGG ROBBERY/DEADLY WEAPON\n1176                                             POSS/PROMO CHILD PORNOGRAPHY\n1193                                           UNLAWFUL RESTRAINT FAM/DAT VIO\n1222                                                 VIOL OF PROTECTIVE ORDER\n1225                                                             DOC EXPOSURE\n1322                    Rape                   EXPIRED-ATT SEXUAL ASULT CHILD\n1351      Aggravated Assault                      AGG ASLT STRANGLE/SUFFOCATE\n1465                                                                 STALKING\n1548                                             ASSAULT BY THREAT FAM/DATING\n1625                                                        ASSAULT BY THREAT\n1656                 Robbery                           AGG ROBBERY BY ASSAULT\n1665      Aggravated Assault                        SERIOUS INJURY TO A CHILD\n1775                                              FAMILY DISTURBANCE/PARENTAL\n1868                    Rape                      EXPIRED-ATT RAPE OF A CHILD\n1900                                           IMPROPER CONTACT-SEX ASLT VICT\n1945                                                   VIOL STATE LAW - OTHER\n1962                                            BURG OF RES - FAM/DATING ASLT\n1964                                           TERRORISTIC THREAT-FAM/DAT VIO\n1980                Burglary                           BURGLARY NON RESIDENCE\n2002              Auto Theft                                       AUTO THEFT\n2005                    Rape                                 EXPIRED-ATT RAPE\n2051                                             IMPERSONATING PUBLIC SERVANT\n2096                    Rape                           FORCED SODOMY OF CHILD\n2158      Aggravated Assault                                   DEADLY CONDUCT\n2276                                                ASSAULT ON PUBLIC SERVANT\n2373                                                         THEFT OF SERVICE\n2580                                           INTER EMERG PHONECALL FAM/DATE\n2664                  Murder                                           MURDER\n2708                                            FALSE REPORT TO PEACE OFFICER\n2745                   Theft                      BREACH OF COMPUTER SECURITY\n2893                                                POSS OF PROHIBITED WEAPON\n2912                                                PROMOTION OF PROSTITUTION\n3066                                                 INJURY TO ELDERLY PERSON\n3080                                             VIOL PO / SEXUAL ASLT VICTIM\n3096                                           INJ TO ELDERLY   FAM/DATE VIOL\n3206                                                 SLEEPING IN PUBLIC PLACE\n3222                                                  CRED CARD ABUSE - OTHER\n3396                                                  IMMIGRATION HOLD/ARREST\n3455                                                                      DWI\n3459                                                           POSS DANG DRUG\n3491                                               POSS OF DRUG PARAPHERNALIA\n3518                   Theft                             THEFT BY SHOPLIFTING\n3570                                                   EXPIRED-ATT KIDNAPPING\n3590                                              VOCO - ALCOHOL  CONSUMPTION\n3622                                                 UNLAWFUL CARRYING WEAPON\n3652                                                    DOC OFFENSIVE GESTURE\n3699      Aggravated Assault                   AGG ASLT ENHANC STRANGL/SUFFOC\n3710                                           INJ TO DISABLED  FAM/DATE VIOL\n3712                                               RESISTING ARREST OR SEARCH\n3758                                                                  DWI 2ND\n3772                                                         DAMAGE CITY PROP\n3778                                               VOCO SOLICITATION PROHIBIT\n3790                                               DOC WINDOW PEEPING - HOTEL\n3795                                             TAMPERING WITH CONSUMER PROD\n3811                    Rape                   EXPIRED-ATT AGG SEXUAL ASSAULT\n3854                                                           AGG KIDNAPPING\n3883                   Theft                                        ATT THEFT\n3959                                                           EVADING / FOOT\n4001                                           POSS OF ALCOHOL - AGE 17 TO 20\n4017                                                                LITTERING\n4032                                            DWI - DRUG RECOGNITION EXPERT\n4056                                                           PROBATION VIOL\n4108                                              CRIMINAL TRESPASS/TRANSIENT\n4110                                                                 GRAFFITI\n4126                   Theft                                  THEFT FROM AUTO\n4211                                           DOC DISPLAY GUN/DEADLY PUB PLC\n4304                   Theft                              BURGLARY OF VEHICLE\n4356                                           ONLINE SOLICITATION OF A MINOR\n4364                                                             DOC FIGHTING\n4528                                           THEFT OF TELECOMMUNICATION SRV\n4779                                                  CRIMINAL TRESPASS/HOTEL\n4988                                                 POSS OF FIREARM BY FELON\n5223                    Rape                   EXPIREDATT AGG SEX ASSLT CHILD\n5332                                              SEXUAL PERFORMANCE BY CHILD\n5368                                                     DOC ABUSIVE LANGUAGE\n5563      Aggravated Assault                   AGG ASLT W/MOTOR VEH FAM/DAT V\n5615                                                                  PERJURY\n5683      Aggravated Assault                       AGG ASSAULT WITH MOTOR VEH\n6231                                                  CHILD CUSTODY INTERFERE\n6655                                                              PAROLE VIOL\n6719                   Theft                    BURGLARY OF VEH-NO SUSPECT/FU\n6768                                                  COMPELLING PROSTITUTION\n7190                                                      VIOL OF PARK CURFEW\n7358                   Theft                                       MAIL THEFT\n7917                Burglary                        ATT BURGLARY OF RESIDENCE\n8816                                                          CAMPING IN PARK\n8898                                                POSS CONTROLLED SUB/OTHER\n9195                                           VIOL OF EMERG PROTECTIVE ORDER\n9461                    Rape                   EXPIRED-ATTAGGFORCESODOMYCHILD\n9912                    Rape                        EXPIRED-ATT FORCED SODOMY\n10386                                                      FEDERAL VIOL/OTHER\n10547                                                        DEBIT CARD ABUSE\n11327                  Theft                              THEFT FROM BUILDING\n11435                                                 DEL OF ALCOHOL TO MINOR\n11524                                                     ESCAPE FROM CUSTODY\n11941                                                   CRIMINAL SOLICITATION\n12205                                                   DISRUPTION OF CLASSES\n12219                                                  DUI - AGE 16 AND UNDER\n13051                                          OBSCENE DISPLAY - DISTRIBUTION\n13504                                                  AGG KIDNAPPING FAM VIO\n13841                                          THEFT- APPROPRIATE STOLEN PROP\n15376                                          EXPIRED-INJURY CHILD - ELDERLY\n15909                                           AGG PROMOTION OF PROSTITUTION\n15983                                                     OFFICIAL OPPRESSION\n16260                                                                GAMBLING\n16906                                          DISTRIB HARMFUL MATERIAL MINOR\n17752                                          SEXTING/TRANSMIT SEXUAL PHOTOS\n18640                 Murder                                   CAPITAL MURDER\n19374                                                         FORGERY - OTHER\n19490                                             DEL CONTROLLED SUB/NARCOTIC\n19958                                                                   ARSON\n20521                                                           DEL MARIJUANA\n20606                                              EXPIRED-ATT AGG KIDNAPPING\n21721                                                  HINDERING APPREHENSION\n22160                                                 SMUGGLING ILLEGAL ALIEN\n22325                                          CRASH/FAIL STOP AND RENDER AID\n22491                   Rape                                       AGG SODOMY\n22765                                               VIOL CITY ORDINANCE - DOG\n23056                                           DOC DISCHARGE GUN - PUB PLACE\n23243                                                     DOC ABUSE OR THREAT\n23249                                               EVADING / VEHICLE PURSUIT\n23512                                                      KIDNAPPING FAM VIO\n23561                                                  SOLICITATION - BEGGING\n23583                                              LIQUOR LAW VIOLATION/OTHER\n23627                                             DEL CONTROLLED SUB/SYN NARC\n23679                                                      THEFT BY EXTORTION\n23714                                          CRIMINAL SOLICITATION OF MINOR\n23980                                            DRIVING WHILE INTOX / FELONY\n24012                  Theft                           THEFT OF LICENSE PLATE\n24057                                                               OBSCENITY\n24077                                                POSS SYNTHETIC MARIJUANA\n24408                                              VIOL CITY ORDINANCE - TAXI\n24414                                                     FORGERY AND PASSING\n24417                                                TAMPERING WITH ID NUMBER\n24814                                          ASSAULT OF PREGNANT WM-FAM/DAT\n24865                                            CRIMINAL MISCHIEF-NO SUSPECT\n24868                                                         EVADING VEHICLE\n24928                                          HARASSMENT OF A PUBLIC SERVANT\n24935                                            CRIMINAL TRESPASS/IN VEHICLE\n24936                                                  CIVIL DISTURBANCE/DEMO\n25002                                               ARSON-VIOL CITY ORDINANCE\n25034                                            POSS CONTROLLED SUB/SYN NARC\n25040                  Theft                                 THEFT OF TRAILER\n25076               Burglary    BURGLARY OF SHED/DETACHED GARAGE/STORAGE UNIT\n25331                                                      DUI - AGE 17 TO 20\n25717                                                      CRUELTY TO ANIMALS\n25849                                                ASSAULT ON PEACE OFFICER\n26052                                            VOCO AMPLIFIED MUSIC/VEHICLE\n26066                  Theft                        THEFT CATALYTIC CONVERTER\n26145                                              VIOL CIVIL RIGHTS PRISONER\n26152                                                   DWI - CHILD PASSENGER\n26258                                                   DWI  .15 BAC OR ABOVE\n26337                                                            ABUSE OF 911\n26381                                          TELECOMMUNICATION CRIMES/OTHER\n26412                                            DOC DISCHARGE GUN - PUB ROAD\n26927                                            VIOL CITY ORDINANCE - CURFEW\n27128                                                     DAMAGE CITY VEHICLE\n27312                                           VIOL CITY ORDINANCE - BOOTING\n27423     Aggravated Assault                     DEADLY CONDUCT FAM/DATE VIOL\n27461                                              CRED CARD ABUSE BY FORGERY\n28052                                               RENTAL CAR/FAIL TO RETURN\n28240                                           VIOL CITY ORDINANCE -FIREWORK\n28361                                           VIOL CITY ORDINANCE - WRECKER\n28403                                                  AIRPORT - FEDERAL VIOL\n28481                                             FAIL TO REGISTER- MINOR VIC\n28583                                             EXPLOSIVE ORDNANCE DISPOSAL\n28585                                                        PROTECTIVE ORDER\n28674                                          AIRPORT PLACES WEAPON PROHIBIT\n28824                                            THEFT BY FALSE PRETEXT/BUNCO\n28908                                                         RECKLESS DAMAGE\n29167     Aggravated Assault                   TAKE WEAPON FRM POLICE OFFICER\n29171                  Theft                              THEFT OF AUTO PARTS\n29230                                                 ARSON-CRIMINAL MISCHIEF\n29629                                          FALSE STATEMENT -OBTAIN CREDIT\n29814                                              INTERFERENCE PUBLIC DUTIES\n29925                                          FRAUD FILING FINANCE STATEMENT\n30076                  Theft                                  PURSE SNATCHING\n30136                                                DEL CONTROLLED SUB/OTHER\n30205                                               VIOL OF CAMPING ORDINANCE\n30282                  Theft                                   THEFT OF METAL\n30305                                           FORGERY- CERTIFICATE OF TITLE\n30343                                                 VIOL OF BOND CONDITIONS\n30501                                              CRASH/INTOXICATION ASSAULT\n30508                                                 PURCHASING PROSTITUTION\n30705                                             VIOL CITY ORDINANCE - SOUND\n30759                                                 TAMPERING WITH EVIDENCE\n30799                                                  DOC UNREASONABLE NOISE\n31046                  Theft                        THEFT-NO SUSPECT/FOLLOWUP\n31126                                          TERRORISTIC THREAT-MASS CASLTY\n31266     Aggravated Assault                     AGG ASSAULT ON PEACE OFFICER\n31574                                                          COUNTERFEITING\n31867                  Theft                         THEFT OF HEAVY EQUIPMENT\n32145                                          INTERFERE W PO SERVICE ANIMALS\n32227                  Theft                      BURGLARY OF COIN-OP MACHINE\n32522                                                POSS CRIMINAL INSTRUMENT\n33666                                          INJ/CHILD FV (NO CARE/CUSTODY)\n33676                                            FAIL TO REGISTER - ADULT VIC\n33693                                           EXPLOITATION OF CHILD/ELDERLY\n33902                                          INTERFERING W/EMERG PHONE CALL\n33950                                              IDENTITY THEFT-TAX RETURNS\n33976                                                   THEFT BY EMBEZZLEMENT\n34179                                               DOC CREATING NOXIOUS ODOR\n34224                                          OBTAIN CONTROLLED SUB BY FRAUD\n34564                  Theft                                   POCKET PICKING\n34631                                                        RECKLESS CONDUCT\n34965                                                     UCW LICENSE PREMISE\n35540                                                        MONEY LAUNDERING\n35710                   Rape            AGG SEXUAL ASSAULT CHILD/OBJECT-SUPER\n35864     Aggravated Assault                   INJURY TO ELDERLY PERSON - SBI\n36025                                             AIRPORT - CRIMINAL TRESPASS\n36402                                                      FRAUD-CARD SKIMMER\n37746                                                       FORGERY BY MAKING\n38099                                                        DEL OF DANG DRUG\n38652                                                CRASH/INTOX MANSLAUGHTER\n39024                  Theft                          MISAPPLY FIDUCIARY PROP\n39161                                                      NUISANCE ABATEMENT\n39184                                          ILLUMIN AIRCRAFT INTENSE LIGHT\n40523                                               OBTAIN DANG DRUG BY FRAUD\n41467                                                             BOMB THREAT\n42654                                               BOATING WHILE INTOXICATED\n42680             Auto Theft                                   ATT AUTO THEFT\n42889                                          SECURING EXEC-DOC BY DECEPTION\n43023                                                   FORGERY BY ALTERATION\n43408                                              SIT AND LIE ORDINANCE VIOL\n43627                                             ASSAULT OF A PREGNANT WOMAN\n44409                                                      JUSTIFIED HOMICIDE\n44424                                                           ATTACK BY DOG\n44686                                                    VIOL STAY AWAY ORDER\n45553                                                 DEL SYNTHETIC MARIJUANA\n47850                                             DUMPING REFUSE NEAR HIGHWAY\n47910                                                                 BRIBERY\n48227                                               FORGERY OF IDENTIFICATION\n48685                                                             BANK KITING\n49180     Aggravated Assault                    AGG ASSAULT ON PUBLIC SERVANT\n49548                                          FRAUD DESTRUCTION OF A WRITING\n51266                                           MAKING TOBACCO AVAIL TO MINOR\n51544                                               TAMPERING WITH GOV RECORD\n53269                                                  CRIMES AGAINST ELDERLY\n53304                                                HINDER SECURED CREDITORS\n54181                                          CRED CARD ABUSE - EXPIR-CANCEL\n55456                                                     FALSE REPORT TO CPS\n56265     Aggravated Assault                             ARSON-DEADLY CONDUCT\n56891                                                     MISREP AGE BY MINOR\n56985                                               VIOL TEMP EX PARTE  ORDER\n57576                                                          INHALANT ABUSE\n57670                                                 THEFT BY PUBLIC SERVANT\n57814                                                  EXPIRED-EVADING ARREST\n57839                                          POSS OF ALCOHOL-AGE 16 & UNDER\n58218                                               AMPLIFIED MUSIC / VEHICLE\n58378               Burglary                       ATT BURGLARY NON RESIDENCE\n59205                                           VIOL CITY ORDINANCE - AIRPORT\n59611                                             MANF CONTROLLED SUB - OTHER\n59661                                                   ARSON-RECKLESS DAMAGE\n60847                                           LOCATION RESTRICT KNIFE CLS C\n63226                                               UNLAWFUL CARRY-LIC HOLDER\n64148                                            POSSESSION OF FORGED WRITING\n64331                                           VIOL CITY ORDINANCE - SMOKING\n64399                                                      CRASH/MANSLAUGHTER\n64458                                             TOBACCO VIOL - UNDER AGE 17\n65945                                             ENGAGING IN ORGANIZED CRIME\n67008                                          ILLEGAL LABELLING OF RECORDING\n67041                                                TRADEMARK COUNTERFEITING\n67144                                                             PIGEON DROP\n67719                                          EXPIRED-ATT OBT CONT SUB FRAUD\n70706                                            DESECRATION VENERATED OBJECT\n70923                                             TOBACCO VIOL - UNDER AGE 21\n73666                                                         SEXUAL COERCION\n76276     Aggravated Assault                       EXPIRED-ATT CAPITAL MURDER\n77057                                           VIOL OF OBSTRUCTION ORDINANCE\n77143                                                  DANG DRUG VIOL - OTHER\n77216                                             CONTROLLED SUB VIOL - OTHER\n78918                                            AIRPORT - BREACH OF SECURITY\n79097                                                    BWI-EXPIRED USE 2110\n79676                                                    VIOL GLASS CONTAINER\n79852                                                 DISPOSAL OF SOLID WASTE\n80198                  Theft                          THEFT OF CRYPTOCURRENCY\n85362                                                 FIREARMS ON SCHOOL PROP\n87299                                                         ABUSE OF CORPSE\n87690                                           DELIVERY OF PRESCRIPTION FORM\n87849                                           MANF CONTROLLED SUB- SYN NARC\n88498                                                   TOBACCO VIOL - AGE 17\n88709                                                                 TRUANCY\n89131                                          PRACTICE MEDICINE W/OUT LICENS\n91263                                              ABUSE OF OFFICIAL CAPACITY\n93209                                                 CRASH/CRIM NEG HOMICIDE\n98690                                          TRAFFICKING OF PERSONS FOR SEX\n100368                                                      CONTEMPT OF COURT\n112247                                                 TAMPERING WITH WITNESS\n115039    Aggravated Assault                    AGG ASSAULT BY PUBLIC SERVANT\n116693                                                DOMESTIC VIOLENCE/ALARM\n134059                                         VIOL CITY ORDINANCE -  GAME RM\n138822                                         VIOL CITY ORDINANCE - TITLE 10\n141940                                             ASSAULT - SCHOOL PERSONNEL\n144428                                                   HINDERING PROCEEDING\n145979                                              SALE OR PURCHASE OF CHILD\n147966                                                 ABANDONED REFRIGERATOR\n148738               Robbery                   EXPIRED-ATT ROBBERY BY ASSAULT\n154995                 Theft                               THEFT/TILL TAPPING\n158333                                                           CRASH/MURDER\n165476                                                            AGG PERJURY\n167605                                          FALSE CALLER ID DISPLAY/SPOOF\n168830                                             DISRUPTIVE ACTS AT SCHOOLS\n173594                                                  DRINKING AFTER CURFEW\n185099                                           FAIL DISPLAY HANDGUN LICENSE\n190951                                                                   RIOT\n195046                                            FALSE ID AS A PEACE OFFICER\n204682                                                  UNLAWFUL INTERCEPTION\n212998                                         EXP-VIOL CITY ORDINANCE - TAXI\n215138                                                     GAMBLING PROMOTION\n215956                                         EXPIRED-ATT OB DANG DRUG FRAUD\n216043                                         VIOL STATE MASSAGE REGULATIONS\n216157               Robbery                     EXPIRED-ATT AGG ROBBERY/WEAP\n220163                                                MISUSE OF OFFICIAL INFO\n224363                                            COMMUNICATING GAMBLING INFO\n228331                Murder                                     MANSLAUGHTER\n231475                Murder DRUG/MURDER BY PROVIDING DRUG RESULTING IN DEATH\n237163                                                 KEEPING GAMBLING PLACE\n237969                                                  VIOL WATER SAFETY ACT\n242106                                                    CRIMINAL CONSPIRACY\n247680                                                         CHILD GROOMING\n256156                                             COERCION OF PUBLIC SERVANT\n257860                                                             BESTIALITY\n264432                                            VIOL OF AGGRESSIVE CONFRONT\n264771                               TAMPER WITH ELECTRONIC MONITORING DEVICE\n281399                                                VIOL OF PRISONERS RIGHT\n299919                                                                   AWOL\n307134    Aggravated Assault                         ARSON WITH BODILY INJURY\n337178                                         POSS OF GAMBLING PARAPHERNALIA\n351108                                          CRIM NEG HOMICIDE/NON TRAFFIC\n362194                                              POSS OF PRESCRIPTION FORM\n408277                                           CONTRIBUTE DELINQUENCY MINOR\n408542    Aggravated Assault                               EXPIRED-ATT MURDER\n467966                                            BAIL JUMPING/FAIL TO APPEAR\n509687                                               LOITERING ON SCHOOL PROP\n515367    Aggravated Assault                   EXPIRED-ATT/TAKE WEAP FROM OFF\n526932                                                                 HAZING\n536773                                                         AIDING SUICIDE\n537135                                                              DESERTION\n596320                                                  AIRPORT - BOMB THREAT\n614563                                             POSS OF GAMBLING EQUIPMENT\n616672                                                    CRIMINAL NONSUPPORT\n636557                                             SALE OF LIQ WITHOUT PERMIT\n638675                                                              ATT ARSON\n661938                                          DISRUPTING MEETING/PROCESSION\n694010                                                    OFFICIAL MISCONDUCT\n755386                                                        FICTITIOUS NAME\n764399                                                        STAY AWAY ORDER\n783564                                                     COMMERCIAL BRIBERY\n789923                                                   SALE OF DXM TO MINOR\n797008                                            ATTACK ON ASSISTANCE ANIMAL\n957830                                          ILLEGAL TRANSPORTATION OF LIQ\n1091931                                                BOMB THREAT - AIRCRAFT\n1106322   Aggravated Assault                         EXPIRED - DEADLY ASSAULT\n1250526                                         FAILURE TO REPORT CHILD ABUSE\n1359652                                           VIOL CITY ORD-COVID 19 MASK\n1367472                           EXPIRED-DO NOT USE-ASSAULT ON PEACE OFFICER\n1571922                                            POSS OF LIQ ON SCHOOL PROP\n1685972                                              LOITERING IN PUBLIC PARK\n1840897                                            SALE OF LIQ IN PROHIB AREA\n1881271                                              MANF SYNTHETIC MARIJUANA\n1921201                                         EXPIRED-SOLICITATION OF CHILD\n1984778                                                GIFT TO PUBLIC SERVANT\n2027212                                              CRASH/NEGLIGENT HOMICIDE\n\n\n\nUCRCats &lt;- d2 %&gt;% filter(Category.Description != '') %&gt;% select(Category.Description) %&gt;% unique()\nrow.names(UCRCats) &lt;- NULL\nUCRCats &lt;- UCRCats[c(7,1,4,5,6,3,2),]\nrow.names(UCRCats) &lt;- NULL\n\ndefCats &lt;- rbind(CHILD = str_flatten(c('CHILD', 'MINOR', 'TRUANCY', 'NONSUPPORT'), collapse = '|') # Sex Offense involving a Child, Child Abuse/ Endangerment, Injury/Harm of Vulnerable Persons*\n                 ,HARASSMENT = str_flatten(c('HARASSMENT', 'PROWLER', 'STALKING'), collapse = '|') # General Harassment\n                 ,'SEX OFFENSE'= str_flatten(c('SEX', 'INVASIVE', 'INTIMATE', 'PROSTITUTION', 'OBSC', 'ADULT', 'BESTIALITY', 'INDECEN', 'ENTIC', 'PORN', 'SOLICITATION OF', 'GROOMING'), collapse = '|') # Sexual Offense\n                 ,'DISORDERLY CONDUCT' = str_flatten(c('DOC', 'LEWD', 'URINATING', 'RECKLESS CONDUCT', 'ILLUMIN', 'CORPSE', 'ANIMAL', 'RIOT', 'PROCESSION'), collapse = '|') #Disorderly Conduct\n                 ,VOCO = str_flatten(c('VOCO', 'ORDINANCE', 'CURFEW', 'CAMP', 'AMPLIFIED', 'GLASS', 'CITY ORD'), collapse = '|') # Violation of City Ordinance\n                 ,'SIMPLE ASSAULT' = str_flatten(c('ASSAULT','ASLT$', 'ASSLT', 'PROD', 'ATTACK BY DOG', 'VIOLENCE', 'HAZING', 'SUICIDE', 'CONFRONT'), collapse = '|') # Simple Assault/ ASSAULTIVE\n                 ,TRESPASSING = str_flatten(c('TRESPASS', 'LOITER', 'SLEEP', 'BEG'), collapse = '|') # TRESPASSING\n                 ,'LARGE SCALE THREAT' = str_flatten(c('THREAT'), collapse = '|') # Large Scale Threat\n                 ,'GENERAL DISTURBANCE' = str_flatten(c('DISTURBANCE'), collapse = '|') # General Disturbance\n                 ,'DRUG RELATED' = str_flatten(c('DRUG', 'SUB', 'MARIJUANA', 'INHALANT', 'TOBACCO', 'PRESCRIPTION'), collapse = '|') #Drug Related\n                 ,'FRAUD' = str_flatten(c('FRAUD', 'FALSE', 'IMPERSON', 'FORGE', 'COUNTERFEITING', 'TAMPER', 'LABEL', 'PIGEON', 'MEDICINE', 'NAME'), collapse = '|') #Fraud, Unlawful Deception\n                 ,'ALCOHOL RELATED' = str_flatten(c('ALCOHOL', 'LIQ', 'DWI', 'DUI', 'WHILE INTOX', 'BWI'), collapse = '|') # Alcohol Related\n                 ,'HARM OF VULNERABLE PERSONS' = str_flatten(c('INJ', 'ELDER'), collapse = '|') #Injury/Harm of Vulnerable Persons*\n                 ,'NON-COMPLIANCE' = str_flatten(c('WARR', 'BAIL', 'IDENTIFY', 'PROBATION', 'INTER', '911', 'ARREST', 'EVAD', 'PERJURY', 'PAROLE','RETALIATION', 'ESCAPE', 'HINDERING', 'BOND', 'CONTEMPT'), collapse = '|') # Warrant, Non Compliance\n                 ,'CRIMINAL MISCHIEF' = str_flatten(c('MISCHIEF', 'ARSON', 'DAMAGE', 'GRAFFITI', 'DESECRATION'), collapse = '|') # CRIMINAL MISCHIEF\n                 ,'UNLAWFUL RESTRAINT' = str_flatten(c('KIDNAP', 'RESTRAINT'), collapse = '|') # Unlawful Restraint\n                 ,'WEAPON RELATED' = str_flatten(c('WEAPON', 'FIREARM', 'EXPLOSIVE', 'UCW', 'KNIFE', 'CARRY', 'HANDGUN'), collapse = '|') # WEAPON RELATED\n                 ,'PROTECTIVE ORDER' = str_flatten(c('ORDER'), collapse = '|') #PROTECTIVE/ RESTRAINING ORDER\n                 ,'FINANCIAL CRIME' = str_flatten(c('CRED', 'DEBIT', 'MONEY', 'BANK'), collapse = '|') # FINANCIAL CRIME\n                 ,LITTERING = str_flatten(c('LITTER', 'DUMP', 'WASTE', 'REFRIGERATOR'), collapse = '|') #LITTERING\n                 ,'CRIMINAL CONSPIRACY' = str_flatten(c('CRIMINAL SOLICITATION', 'ABATE', 'ORGANIZED', 'CONSPIR'), collapse = '|') # CRIMINAL CONSPIRACY\n                 ,'ABUSE OF OFFICE' = str_flatten(c('OFFICIAL', 'PRISONER'), collapse = '|') # ABUSE OF OFFICE\n                 ,'GAMBLING RELATED' = str_flatten(c('GAMBL'), collapse = '|') # GAMBLING RELATED\n                 ,THEFT = str_flatten(c('RENTAL CAR'), collapse = '|')# THEFT\n                 ,MURDER = str_flatten(c('MANSLAUGHTER', 'HOMICIDE'), collapse = '|') # MURDER (MANSLAUGHTER INCLUDED BECAUSE THAT'S HOW FBI CATS)\n                 ,BRIBERY = str_flatten(c('BRIBE', 'COERCION OF PUBLIC SERVANT', 'GIFT'), collapse = '|') # BRIBERY\n                 ,OTHER = str_flatten(c('IMMIGRATION', 'ALIEN', 'OTHER', 'DISRUPTION OF CLASSES', 'AIRPORT', 'INSTRUMENT','SCHOOLS', 'MASSAGE', 'WATER', 'AWOL', 'DESERTION', 'RENDER'), collapse = '|')# OTHER\n                 )%&gt;% as.data.frame()\ndefCats$Cat &lt;- rownames(defCats)\nrownames(defCats) &lt;- NULL  \ncolnames(defCats) &lt;- c('strSearch', 'Cats')\n\nd3 &lt;- d2 %&gt;% \n    mutate(Crime.Category = if_else(Category.Description == '', \n                                    str_extract(Highest.Offense.Description, str_flatten(toupper(UCRCats), collapse = '|')), \n                                    toupper(Category.Description)\n                                    )\n          ,Crime.Category = if_else(is.na(Crime.Category),\n                                    case_when(\n                                      str_detect(Highest.Offense.Description,defCats[1,1]) == TRUE ~ defCats[1,2],\n                                      str_detect(Highest.Offense.Description,defCats[2,1]) == TRUE ~ defCats[2,2],\n                                      str_detect(Highest.Offense.Description,defCats[3,1]) == TRUE ~ defCats[3,2],\n                                      str_detect(Highest.Offense.Description,defCats[4,1]) == TRUE ~ defCats[4,2],\n                                      str_detect(Highest.Offense.Description,defCats[5,1]) == TRUE ~ defCats[5,2],\n                                      str_detect(Highest.Offense.Description,defCats[6,1]) == TRUE ~ defCats[6,2],\n                                      str_detect(Highest.Offense.Description,defCats[7,1]) == TRUE ~ defCats[7,2],\n                                      str_detect(Highest.Offense.Description,defCats[8,1]) == TRUE ~ defCats[8,2],\n                                      str_detect(Highest.Offense.Description,defCats[9,1]) == TRUE ~ defCats[9,2],\n                                      str_detect(Highest.Offense.Description,defCats[10,1]) == TRUE ~ defCats[10,2],\n                                      str_detect(Highest.Offense.Description,defCats[11,1]) == TRUE ~ defCats[11,2],\n                                      str_detect(Highest.Offense.Description,defCats[12,1]) == TRUE ~ defCats[12,2],\n                                      str_detect(Highest.Offense.Description,defCats[13,1]) == TRUE ~ defCats[13,2],\n                                      str_detect(Highest.Offense.Description,defCats[14,1]) == TRUE ~ defCats[14,2],\n                                      str_detect(Highest.Offense.Description,defCats[15,1]) == TRUE ~ defCats[15,2],\n                                      str_detect(Highest.Offense.Description,defCats[16,1]) == TRUE ~ defCats[16,2],\n                                      str_detect(Highest.Offense.Description,defCats[17,1]) == TRUE ~ defCats[17,2],\n                                      str_detect(Highest.Offense.Description,defCats[18,1]) == TRUE ~ defCats[18,2],\n                                      str_detect(Highest.Offense.Description,defCats[19,1]) == TRUE ~ defCats[19,2],\n                                      str_detect(Highest.Offense.Description,defCats[20,1]) == TRUE ~ defCats[20,2],\n                                      str_detect(Highest.Offense.Description,defCats[21,1]) == TRUE ~ defCats[21,2],\n                                      str_detect(Highest.Offense.Description,defCats[22,1]) == TRUE ~ defCats[22,2],\n                                      str_detect(Highest.Offense.Description,defCats[23,1]) == TRUE ~ defCats[23,2],\n                                      str_detect(Highest.Offense.Description,defCats[24,1]) == TRUE ~ defCats[24,2],\n                                      str_detect(Highest.Offense.Description,defCats[25,1]) == TRUE ~ defCats[25,2],\n                                      str_detect(Highest.Offense.Description,defCats[26,1]) == TRUE ~ defCats[26,2],\n                                      str_detect(Highest.Offense.Description,defCats[27,1]) == TRUE ~ defCats[27,2],\n                                      .default = Crime.Category\n                                    ),\n                                    Crime.Category\n          )\n          ,Crime.Category = if_else(Crime.Category == 'CHILD',\n                                    case_when(\n                                      str_detect(Highest.Offense.Description,defCats[3,1]) == TRUE ~ 'SEX OFFENSE INVOLVING A CHILD',\n                                      .default = 'HARM OF VULNERABLE PERSONS'\n                                    ),\n                                    Crime.Category\n                                    )\n          ,Crime.Category = if_else(Highest.Offense.Description == 'JUSTIFIED HOMICIDE', 'JUSTIFIED HOMICIDE', Crime.Category)\n          )\nd3 %&gt;% select(Highest.Offense.Description, Category.Description, Crime.Category) %&gt;% head(100)\n\n       Highest.Offense.Description Category.Description\n1   SEXUAL ASSAULT OF CHILD/OBJECT                 Rape\n2                  RAPE OF A CHILD                 Rape\n3                             RAPE                 Rape\n4                             RAPE                 Rape\n5         SEXUAL ASSAULT W/ OBJECT                 Rape\n6                             RAPE                 Rape\n7   ASSAULT  CONTACT-SEXUAL NATURE                     \n8                       HARASSMENT                     \n9                             RAPE                 Rape\n10   DEPENDENT AND NEGLECTED CHILD                     \n11        SEXUAL ASSAULT W/ OBJECT                 Rape\n12               INDECENT EXPOSURE                     \n13  INDECENCY WITH A CHILD/CONTACT                     \n14                 RAPE OF A CHILD                 Rape\n15  INDECENCY WITH A CHILD/CONTACT                     \n16                 RAPE OF A CHILD                 Rape\n17     BURG OF RES - SEXUAL NATURE             Burglary\n18  INDECENCY WITH A CHILD/CONTACT                     \n19                            RAPE                 Rape\n20                            RAPE                 Rape\n21             ASSAULT WITH INJURY                     \n22  FAILURE TO REG AS SEX OFFENDER                     \n23                            RAPE                 Rape\n24                            RAPE                 Rape\n25                        AGG RAPE                 Rape\n26                            RAPE                 Rape\n27         STATUTORY RAPE OF CHILD                     \n28               THEFT FROM PERSON                Theft\n29  INDECENCY WITH A CHILD/CONTACT                     \n30           BURGLARY OF RESIDENCE             Burglary\n31  ASSAULT  CONTACT-SEXUAL NATURE                     \n32                            RAPE                 Rape\n33  AGG SEXUAL ASSAULT CHILD/OBJEC                 Rape\n34  FAILURE TO REG AS SEX OFFENDER                     \n35  INDECENCY WITH A CHILD/CONTACT                     \n36               INDECENT EXPOSURE                     \n37                 RAPE OF A CHILD                 Rape\n38                            RAPE                 Rape\n39  INDECENCY WITH A CHILD/CONTACT                     \n40  SEXUAL ASSAULT OF CHILD/OBJECT                 Rape\n41                            RAPE                 Rape\n42   INDECENCY WITH CHILD/EXPOSURE                     \n43  FAILURE TO REG AS SEX OFFENDER                     \n44  FAILURE TO REG AS SEX OFFENDER                     \n45  INDECENCY WITH A CHILD/CONTACT                     \n46             ASSAULT WITH INJURY                     \n47                           THEFT                Theft\n48  FAILURE TO REG AS SEX OFFENDER                     \n49        SEXUAL ASSAULT W/ OBJECT                 Rape\n50         STATUTORY RAPE OF CHILD                     \n51                 INJURY TO CHILD                     \n52   DEPENDENT AND NEGLECTED CHILD                     \n53        SEXUAL ASSAULT W/ OBJECT                 Rape\n54             AGG RAPE OF A CHILD                 Rape\n55  FAILURE TO REG AS SEX OFFENDER                     \n56                            RAPE                 Rape\n57  INDECENCY WITH A CHILD/CONTACT                     \n58  AGG SEXUAL ASSAULT CHILD/OBJEC                 Rape\n59  SEXUAL ASSAULT OF CHILD/OBJECT                 Rape\n60  SEXUAL ASSAULT OF CHILD/OBJECT                 Rape\n61  FAILURE TO REG AS SEX OFFENDER                     \n62                THEFT OF BICYCLE                Theft\n63  FAILURE TO REG AS SEX OFFENDER                     \n64  SEXUAL ASSAULT OF CHILD/OBJECT                 Rape\n65              ASSAULT BY CONTACT                     \n66  AGG SEXUAL ASSAULT CHILD/OBJEC                 Rape\n67                 RAPE OF A CHILD                 Rape\n68  AGG SEXUAL ASSAULT CHILD/OBJEC                 Rape\n69         STATUTORY RAPE OF CHILD                     \n70  INDECENCY WITH A CHILD/CONTACT                     \n71  SEXUAL ASSAULT OF CHILD/OBJECT                 Rape\n72        SEXUAL ASSAULT W/ OBJECT                 Rape\n73  INDECENCY WITH A CHILD/CONTACT                     \n74  AGG SEXUAL ASSAULT CHILD/OBJEC                 Rape\n75                            RAPE                 Rape\n76  FAILURE TO REG AS SEX OFFENDER                     \n77  FAILURE TO REG AS SEX OFFENDER                     \n78  FAILURE TO REG AS SEX OFFENDER                     \n79  FAILURE TO REG AS SEX OFFENDER                     \n80                     RETALIATION                     \n81  INDECENCY WITH A CHILD/CONTACT                     \n82                            RAPE                 Rape\n83                 RAPE OF A CHILD                 Rape\n84  AGG SEXUAL ASSAULT CHILD/OBJEC                 Rape\n85  INDECENCY WITH A CHILD/CONTACT                     \n86      AGG FORCED SODOMY OF CHILD                 Rape\n87  FAILURE TO REG AS SEX OFFENDER                     \n88                        AGG RAPE                 Rape\n89  INDECENCY WITH A CHILD/CONTACT                     \n90                            RAPE                 Rape\n91  INDECENCY WITH A CHILD/CONTACT                     \n92        SEXUAL ASSAULT W/ OBJECT                 Rape\n93                            RAPE                 Rape\n94  FAILURE TO REG AS SEX OFFENDER                     \n95  DISCLOSE/PROMO INTIMATE VISUAL                     \n96        SEXUAL ASSAULT W/ OBJECT                 Rape\n97  AGG SEXUAL ASSAULT CHILD/OBJEC                 Rape\n98               HARASSMENT ONLINE                     \n99                            RAPE                 Rape\n100                           RAPE                 Rape\n                   Crime.Category\n1                            RAPE\n2                            RAPE\n3                            RAPE\n4                            RAPE\n5                            RAPE\n6                            RAPE\n7                     SEX OFFENSE\n8                      HARASSMENT\n9                            RAPE\n10     HARM OF VULNERABLE PERSONS\n11                           RAPE\n12                    SEX OFFENSE\n13  SEX OFFENSE INVOLVING A CHILD\n14                           RAPE\n15  SEX OFFENSE INVOLVING A CHILD\n16                           RAPE\n17                       BURGLARY\n18  SEX OFFENSE INVOLVING A CHILD\n19                           RAPE\n20                           RAPE\n21                 SIMPLE ASSAULT\n22                    SEX OFFENSE\n23                           RAPE\n24                           RAPE\n25                           RAPE\n26                           RAPE\n27                           RAPE\n28                          THEFT\n29  SEX OFFENSE INVOLVING A CHILD\n30                       BURGLARY\n31                    SEX OFFENSE\n32                           RAPE\n33                           RAPE\n34                    SEX OFFENSE\n35  SEX OFFENSE INVOLVING A CHILD\n36                    SEX OFFENSE\n37                           RAPE\n38                           RAPE\n39  SEX OFFENSE INVOLVING A CHILD\n40                           RAPE\n41                           RAPE\n42  SEX OFFENSE INVOLVING A CHILD\n43                    SEX OFFENSE\n44                    SEX OFFENSE\n45  SEX OFFENSE INVOLVING A CHILD\n46                 SIMPLE ASSAULT\n47                          THEFT\n48                    SEX OFFENSE\n49                           RAPE\n50                           RAPE\n51     HARM OF VULNERABLE PERSONS\n52     HARM OF VULNERABLE PERSONS\n53                           RAPE\n54                           RAPE\n55                    SEX OFFENSE\n56                           RAPE\n57  SEX OFFENSE INVOLVING A CHILD\n58                           RAPE\n59                           RAPE\n60                           RAPE\n61                    SEX OFFENSE\n62                          THEFT\n63                    SEX OFFENSE\n64                           RAPE\n65                 SIMPLE ASSAULT\n66                           RAPE\n67                           RAPE\n68                           RAPE\n69                           RAPE\n70  SEX OFFENSE INVOLVING A CHILD\n71                           RAPE\n72                           RAPE\n73  SEX OFFENSE INVOLVING A CHILD\n74                           RAPE\n75                           RAPE\n76                    SEX OFFENSE\n77                    SEX OFFENSE\n78                    SEX OFFENSE\n79                    SEX OFFENSE\n80                 NON-COMPLIANCE\n81  SEX OFFENSE INVOLVING A CHILD\n82                           RAPE\n83                           RAPE\n84                           RAPE\n85  SEX OFFENSE INVOLVING A CHILD\n86                           RAPE\n87                    SEX OFFENSE\n88                           RAPE\n89  SEX OFFENSE INVOLVING A CHILD\n90                           RAPE\n91  SEX OFFENSE INVOLVING A CHILD\n92                           RAPE\n93                           RAPE\n94                    SEX OFFENSE\n95                    SEX OFFENSE\n96                           RAPE\n97                           RAPE\n98                     HARASSMENT\n99                           RAPE\n100                          RAPE\n\nd3 %&gt;% dim()\n\n[1] 2461621      28\n\nd3 %&gt;% group_by(Crime.Category, Category.Description) %&gt;% summarize(\n  cnt = n()\n)\n\n`summarise()` has grouped output by 'Crime.Category'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 36 × 3\n# Groups:   Crime.Category [33]\n   Crime.Category      Category.Description    cnt\n   &lt;chr&gt;               &lt;chr&gt;                 &lt;int&gt;\n 1 ABUSE OF OFFICE     \"\"                      135\n 2 AGGRAVATED ASSAULT  \"Aggravated Assault\"  43636\n 3 ALCOHOL RELATED     \"\"                   103340\n 4 AUTO THEFT          \"Auto Theft\"          62114\n 5 BRIBERY             \"\"                       20\n 6 BURGLARY            \"Burglary\"           130503\n 7 CRIMINAL CONSPIRACY \"\"                      633\n 8 CRIMINAL MISCHIEF   \"\"                   166706\n 9 DISORDERLY CONDUCT  \"\"                    24842\n10 DRUG RELATED        \"\"                   128900\n# ℹ 26 more rows\n\nd3 %&gt;% filter(!is.na(Crime.Category)) %&gt;%\n  #,Category.Description == '') %&gt;% \n  #select(Crime.Category, Highest.Offense.Description) %&gt;% \n  group_by(Crime.Category) %&gt;%  #, Highest.Offense.Description) %&gt;% \n  summarize(\n    cnt = n()\n  ) %&gt;% \n  arrange(by = cnt)\n\n# A tibble: 33 × 2\n   Crime.Category                  cnt\n   &lt;chr&gt;                         &lt;int&gt;\n 1 BRIBERY                          20\n 2 JUSTIFIED HOMICIDE               44\n 3 ABUSE OF OFFICE                 135\n 4 CRIMINAL CONSPIRACY             633\n 5 GAMBLING RELATED                861\n 6 MURDER                          862\n 7 UNLAWFUL RESTRAINT             1274\n 8 LITTERING                      2800\n 9 SEX OFFENSE INVOLVING A CHILD  6147\n10 WEAPON RELATED                11867\n# ℹ 23 more rows\n\n\n\n\nTidied Up\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d3\n\n\n\n\nSave data\nFinally, we save the clean data as RDS file.\n\nsave_data_location &lt;- here(\"data\",\"processed-data\",\"processed-crime.rds\")\nsaveRDS(processeddata, file = save_data_location)"
  },
  {
    "objectID": "HYLTIN-PII-project/data/processed-data/readme.html",
    "href": "HYLTIN-PII-project/data/processed-data/readme.html",
    "title": "processed-data",
    "section": "",
    "text": "processed-data\nThis folder contains data that has been processed and cleaned by code.\nAny files located in here are based on the raw data and can be re-created running the various processing/cleaning code scripts in the code folder.\nCode Book is included here, copied from the source code book.\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nIncident Number\nIncident report number\n\n\nHighest Offense Description\nDescription\n\n\nHighest Offense Code\nCode\n\n\nFamily Violence\nIncident involves family violence? Y = yes, N = no\n\n\nOccurred Date Time\nDate and time (combined) incident occurred\n\n\nOccurred Date\nDate the incident occurred\n\n\nOccurred Time\nTime the incident occurred\n\n\nReport Date Time\nDate and time (combined) incident was reported\n\n\nReport Date\nDate the incident was reported\n\n\nReport Time\nTime the incident was reported\n\n\nLocation Type\nGeneral description of the premise where the incident occurred\n\n\nAddress\nIncident location\n\n\nZip code\nZip code where incident occurred\n\n\nCouncil District\nAustin city council district where the incident occurred\n\n\nAPD Sector\nAPD sector where incident occurred\n\n\nAPD District\nAPD district where incident occurred\n\n\nPRA\nAPD police reporting area where incident occurred\n\n\nCensus Tract\nCensus tract where incident occurred\n\n\nClearance Status\nHow/whether crime was solved (see Clearance lookup)\n\n\nClearance Date\nDate crime was solved\n\n\nUCR Category\nCode for the most serious crimes identified by the FBI as part of its Uniform Crime Reporting program\n\n\nCategory Description\nDescription for the most serious crimes identified by the FBI as part of its Uniform Crime Reporting program\n\n\nX-coordinate\nX-coordinate where the incident occurred\n\n\nY-coordinate\nY-coordinate where incident occurred\n\n\nLatitude\nLatitude where incident occurred\n\n\nLongitude\nLongitude where the incident occurred\n\n\nLocation\n3rd party generated spatial column\n\n\n\n\n\n\nClearance lookup\n\n\n\n\n\nC\nCleared by Arrest\n\n\nO\nCleared by Exception\n\n\nN\nNot cleared"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website and data analysis profile",
    "section": "",
    "text": "Pardon the Dust\nThis site is intended to serve as a repository for different analytic exercises and assignments done over the course of my education in the Data Analytics Masters Program at UTSA and beyond. This means that this site will be perpetually under construction. Here you can expect to find:\n\nAnalyses\nVisualizations\nApplications of statistical methods\nThe slow spiral of a man drowning in the sea of his own imposter syndrome\nInsights, hopefully\n\nPlease use the Menu Bar at the top to peruse to your liking.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalytics is a combination of both art and science. Incidentally, robots are already better than us at both, but here’s to trying."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at C:/Users/Hylti/OneDrive/Documents/School/Practicum II/williamhyltin-P2-portfolio\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  factor                   2     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique\n1 Gender                0             1 FALSE          3\n2 Generation            0             1 FALSE          4\n  top_counts                    \n1 M: 4, F: 3, O: 2              \n2 Gen: 5, Gen: 2, Bab: 1, Mil: 1\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n3 Salary                0             1  95.4 36.6  44  70  81 133  144 ▂▇▂▁▆\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\np5 = mydata %&gt;% ggplot(aes(x=Generation, y=Height)) + geom_boxplot()\nplot(p5)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-generation-boxplot.png\")\nggsave(filename = figure_file, plot=p5) \n\nSaving 7 x 5 in image\n\n\n\np6 = mydata %&gt;% ggplot(aes(x=Weight, y=Salary)) + geom_point()\nplot(p6)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"salary-weight-scatter.png\")\nggsave(filename = figure_file, plot=p6) \n\nSaving 7 x 5 in image\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(here) #to set paths\n\nhere() starts at C:/Users/Hylti/OneDrive/Documents/School/Practicum II/williamhyltin-P2-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "William Hyltin Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  }
]