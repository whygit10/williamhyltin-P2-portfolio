---
title: "DA 6813 Case Study 2 "
author: "Will Hytlin, Holly Millazo and Tim Harrison"
date: today
format: 
  html:
    code-fold: true # Folds code by default
    number-sections: true # Numbered sections
    df-print: paged # Nice table format
    fig-align: center # Centers figures
    fig-width: 6 # Adjust figure size
    fig-height: 4
    reference-location: section # Valid value for citation references
    page-layout: article
---

```{r, echo=FALSE}
pacman::p_load(MASS, tidyverse, e1071, here, readxl, skimr, corrplot, patchwork)

raw_train <- read_xlsx(here('case-study-2', 'BBBC-Train.xlsx'))
raw_test <- read_xlsx(here('case-study-2', 'BBBC-Test.xlsx'))
```

```{r, echo=FALSE}
train1 <- raw_train %>%
  select(-Observation) %>%
  mutate(
    Choice = as.factor(Choice),
    Gender = as.factor(Gender)
  )

test1 <- raw_test %>%
  select(-Observation) %>%
  mutate(
    Choice = as.factor(Choice),
    Gender = as.factor(Gender)
  )

combined <- rbind(train1, test1)
```

```{r, echo=FALSE}
combox <- lapply(colnames(select_if(combined, is.numeric)),
       function(col) {
        ggplot(combined,
                aes(y = .data[[col]], x = .data$Choice)) + geom_boxplot() + ggtitle(col)
       }
)
```

```{r, echo=FALSE, warning=FALSE}
combbar <- lapply(colnames(select_if(combined, startsWith(names(combined), 'P_'))),
       function(col) {
        ggplot(combined,
                aes(x = .data[[col]], fill = .data$Choice)) + geom_bar(position = 'dodge') + 
           ggtitle(col) + 
           theme(legend.position = c(0.8,0.8), legend.background = element_blank())
       }
)
```

```{r, echo=FALSE}
set.seed(321)
logfit3 <- step(glm(Choice ~ . -Last_purchase -First_purchase, 
                   data = train1, family = binomial), 
               direction = "backward", trace = 0)
```

```{r, echo=FALSE}
predprob_log <- predict(logfit3, newdata = test1, type = "response")
pr_class_log <- ifelse(predprob_log > 0.2, 1, 0)

log_CM_unbal <- caret::confusionMatrix(as.factor(pr_class_log), as.factor(test1$Choice), positive = '1')
```

```{r, echo=FALSE}
set.seed(321)
ldafit <- lda(Choice ~ ., data = train1)
```

```{r, echo=FALSE}
pr_class_lda <- predict(ldafit, test1)

lda_CM_unbal <- caret::confusionMatrix(as.factor(pr_class_lda$class), as.factor(test1$Choice), positive = "1")
```

```{r, echo=FALSE}
form1 <- Choice ~ .
```

```{r, echo=FALSE}
set.seed(321)
svmfit <- svm(formula = form1, data = train1, gamma = 0.02, cost = 0.5)
```

```{r, echo=FALSE}
svmpredict <- predict(svmfit, test1, type = 'response')
```

```{r, echo=FALSE}
SVM_CM <- caret::confusionMatrix(svmpredict, test1$Choice, positive = '1')
```

```{r, echo=FALSE}
set.seed(321)
trn_art = train1 %>% filter(Choice == '1')
trn_no_art = train1 %>% filter(Choice == '0')

tst_art = test1 %>% filter(Choice == '1')
tst_no_art = test1 %>% filter(Choice == '0')

sample_no_art_trn = sample_n(trn_no_art, nrow(trn_art))
train_bal = rbind(sample_no_art_trn,trn_art)

sample_no_art_tst = sample_n(tst_no_art, nrow(tst_art))
test_bal = rbind(sample_no_art_tst,tst_art)
```

```{r, echo=FALSE}
set.seed(321)
logfit_bal <- step(glm(Choice ~ ., 
                   data = train_bal, family = binomial), 
               direction = "both", trace = 0)
```

```{r, echo=FALSE}
predprob_log_bal <- predict(logfit_bal, newdata = test_bal, type = "response")
pr_class_log_bal <- ifelse(predprob_log_bal > 0.5, 1, 0)

log_CM_unbal_bal <- caret::confusionMatrix(as.factor(pr_class_log_bal), as.factor(test_bal$Choice), positive = '1')
```

```{r, echo=FALSE}
predprob_log_imbal <- predict(logfit_bal, newdata = test1, type = "response")
pr_class_log_imbal <- ifelse(predprob_log_imbal > 0.22, 1, 0)

log_CM_imbal <- caret::confusionMatrix(as.factor(pr_class_log_imbal), as.factor(test1$Choice), positive = '1')
```

```{r, echo=FALSE}
set.seed(321)
ldafit_bal <- lda(Choice ~ ., data = train_bal)
```

```{r, echo=FALSE}
pr_class_lda_bal <- predict(ldafit_bal, test_bal)

lda_CM_unbal_bal <- caret::confusionMatrix(as.factor(pr_class_lda_bal$class), as.factor(test_bal$Choice), positive = "1")
```

```{r, echo=FALSE}
pr_class_lda_imbal <- predict(ldafit_bal, test1)

lda_CM_imbal <- caret::confusionMatrix(as.factor(pr_class_lda_imbal$class), as.factor(test1$Choice), positive = "1")
```

```{r, echo=FALSE}
svmfit_bal <- svm(formula = form1, data = train_bal, gamma = 0.02, cost = 1.45)
```

```{r, echo=FALSE}
svmpredict_bal <- predict(svmfit_bal, test_bal, type = 'response')
```

```{r, echo=FALSE}
svmpredict_imbal <- predict(svmfit_bal, test1, type = 'response')
SVM_CM_imbal <- caret::confusionMatrix(svmpredict_imbal, test1$Choice, positive = '1')
```

```{r, echo=FALSE}
summary_table_svm <- combined %>% group_by(Choice) %>% 
  summarize(percent = n()/nrow(combined),
            newcnt = round(percent * 50000)) %>% as.data.frame() %>% 
  mutate(
    est_targets = ifelse(
      Choice == 0, round(newcnt*(1-SVM_CM_imbal$byClass[["Specificity"]])), round(newcnt*(SVM_CM_imbal$byClass[["Sensitivity"]]))
      ),
    mailercst = est_targets * 0.65,
    purchcst = ifelse(Choice == 1, 15 * 1.45 * est_targets, 0),
    revenue = ifelse(Choice == 1, 31.95 * est_targets, 0),
    profit = revenue - purchcst - mailercst
    )
```

```{r, echo=FALSE}
summ_tab_fun <- function(base_data, hcount, CM) {
  base_data %>% group_by(Choice) %>% 
  summarize(percent = n()/nrow(base_data),
            newcnt = round(percent * hcount)) %>% as.data.frame() %>% 
  mutate(
    est_targets = ifelse(
      Choice == 0, round(newcnt*(1-CM$byClass[["Specificity"]])), round(newcnt*(CM$byClass[["Sensitivity"]]))
      ),
    mailercst = est_targets * 0.65,
    purchcst = ifelse(Choice == 1, 15 * 1.45 * est_targets, 0),
    revenue = ifelse(Choice == 1, 31.95 * est_targets, 0),
    profit = revenue - purchcst - mailercst
    )
}
```

```{r, echo=FALSE}
summary_table_log <- summ_tab_fun(combined, 50000, log_CM_unbal)
```

```{r, echo=FALSE}
summary_table_lda <- summ_tab_fun(combined, 50000, lda_CM_imbal)
```

```{r, echo=FALSE}
naive_table <- combined %>% group_by(Choice) %>% 
  summarize(percent = n()/nrow(combined),
            newcnt = round(percent * 50000)) %>% as.data.frame() %>% 
  mutate(
    mailercst = newcnt * 0.65,
    purchcst = ifelse(Choice == 1, 15 * 1.45 * newcnt, 0),
    revenue = ifelse(Choice == 1, 31.95 * newcnt, 0),
    profit = revenue - purchcst - mailercst
    )
```

```{r, echo=FALSE}
prof_table <- data.frame(Model = c('Naive', 'LDA', 'Logit', 'SVM'), 
  Profit = c(
    sum(naive_table$profit),
    sum(summary_table_lda$profit),
    sum(summary_table_log$profit),
    sum(summary_table_svm$profit)
    )
  )
```

```{r, echo=FALSE, warning=FALSE}
thresh <- data.frame(threshold = -0.01, profit = 0)
for (i in seq(0, 1, by = 0.01)) {
  preds <- ifelse(predprob_log >= i, 1, 0)
  CM_for <- caret::confusionMatrix(as.factor(preds), as.factor(test1$Choice), positive = '1')
  summ_for <- summ_tab_fun(combined, 50000, CM_for)
  thresh = rbind(thresh, data.frame(threshold = i, profit = sum(summ_for$profit)))
}

thresh <- thresh %>% filter(threshold >= 0)
```

```{r, echo=FALSE}
thresh_plot <- thresh %>% ggplot(aes(x = threshold, y = profit)) +
  geom_line() +
  geom_point() +
  annotate('text', x = thresh[which(thresh$profit == max(thresh$profit)),]$threshold, y = thresh[which(thresh$profit == max(thresh$profit)),]$profit + 1800, label = paste0('Max Profit: $', thresh[which(thresh$profit == max(thresh$profit)),]$profit), size = 3) +
  annotate('point', x = thresh[which(thresh$profit == max(thresh$profit)),]$threshold, y = thresh[which(thresh$profit == max(thresh$profit)),]$profit, color = 'green', shape = 'diamond', size = 3) +
  theme_minimal() +
  ggtitle('Book Sales Profit Changes by Model Probability Threshold') +
  xlab('Logistic Model Probability Threshold') +
  ylab('Profit from Book Sales')

  
max_thresh <- thresh[which(thresh$profit == max(thresh$profit)),]
```

```{r, echo=FALSE}
predslog_best <- ifelse(predprob_log >= thresh[which(thresh$profit == max(thresh$profit)),]$threshold, 1, 0)
CMlog_best <- caret::confusionMatrix(as.factor(predslog_best), as.factor(test1$Choice), positive = '1')
```

# Executive Summary

This study aimed to develop a predictive model for the Bookbinders Book Club (BBBC) to determine which customers were likely to purchase The Art History of Florence following a direct mail campaign. The analysis was conducted on a dataset containing customer demographics, purchasing behavior, and preferences for different book genres. The key variables considered included gender, amount spent on BBBC books, frequency of purchases, and the number of specific genres purchased (e.g., children's books, cookbooks, art books).

Four different modeling techniques were evaluated: Linear Regression, LDA, Logit, and support vector machines (SVM). Given that the dependent variable was binary (purchase or no purchase), linear regression was found to be unsuitable as it attempts to predict a continuous outcome rather than a classification, leading to misleading results. Logistic regression and SVM were thus compared for their predictive performance on the unbalanced and balanced datasets.

Initial logistic regression results on the unbalanced data showed moderate accuracy (65.61%) and balanced accuracy (71.84%) but low sensitivity (17.78%). However, balancing the dataset improved sensitivity and overall predictive performance. Similarly, the SVM model initially underperformed due to the unbalanced nature of the data but saw significant improvement after balancing, with a final accuracy of 73.77%, sensitivity of 65.69%, and specificity of 81.86%.

The logit model, while having a lower overall accuracy (65.61%) compared to the LDA model (88.91%), shows a much higher sensitivity in detecting the minority class (79.41% vs. 37.75% for LDA). However, the LDA model excels in specificity (93.89% vs. 64.27% for logit) and achieves a higher Kappa value, indicating better agreement overall. Despite this, the logit model has a higher balanced accuracy (71.84% vs. 65.82%), suggesting a better trade-off between detecting both classes. Ultimately, the logit model is more effective at identifying the minority class, while the LDA model performs better for the majority class and overall accuracy.

A key insight was that improving sensitivity, even at the cost of specificity, was critical for maximizing revenue. By lowering the decision threshold in the logistic regression model, the sensitivity increased, resulting in a higher proportion of correctly identified purchasers. Despite the reduction in specificity, the model captured a larger number of likely buyers, ultimately leading to greater profit potential compared to a naïve approach.Capturing more buyers is crucial because the low cost of sending mailers is far outweighed by the potential revenue from correctly identifying additional purchasers. Increasing sensitivity ensures that more potential buyers receive offers, boosting the chances of converting them into sales. Even with some false positives, the higher number of actual buyers leads to greater overall profit compared to a more conservative approach that risks missing out on revenue opportunities.

The profitability analysis showed that while the logistic regression and SVM models performed better than random guessing, there is still room for improvement. Adjusting model parameters such as the decision threshold and considering alternative models, like LDA, could further enhance profitability by improving the balance between targeting the right customers and controlling campaign costs.

The logistic regression model with an optimized threshold provided a solid proof of concept by delivering the best balance of revenue and costs, demonstrating its potential for profitability. 

# Problem Statement

The task of this case study is to develop a predictive model to classify whether customers of the Bookbinders Book Club (BBBC) will purchase The Art History of Florence following a direct mail marketing campaign. The campaign involved sending a specially produced brochure to selected customers in Pennsylvania, New York, and Ohio, aiming to assess the likelihood of each customer making a purchase ('yes') or not ('no').

The goal is to accurately (or rather accurate enough to maximize profit) predict their likelihood of purchasing 'The Art History of Florence' based on various input variables, including demographic factors (such as gender) and past purchasing behaviors, including the total amount spent on BBBC books, the frequency of past purchases, and preferences for different book genres (such as children's books, cookbooks, do-it-yourself, and art books). These factors are believed to significantly influence the decision to purchase the featured book.

The primary objective is to build a classification model that can accurately predict customer purchases, enabling BBBC to target its marketing efforts more effectively. By identifying the most likely purchasers, BBBC can optimize resource allocation, reduce unnecessary mailer costs, and improve the overall conversion rate of its marketing campaigns.

# Additional Sources

<<<<<<< HEAD
Balanced datasets are critical for support vector machines (SVMs) because they help avoid bias toward the majority class, improving classification accuracy. When trained on imbalanced data, SVMs may favor the dominant class, reducing true positive rates for the minority class and affecting performance metrics like balanced accuracy, MCC, and AUC. Balanced training sets ensure more robust, consistent predictions across both classes, enhancing model reliability in bioinformatics tasks like mutation classification.

Citation: Wei Q, Dunbrack RL Jr. PLoS One. 2013;8(7)
. doi: 10.1371/journal.pone.0067863.
=======
Aldelemy, A., & Abd-Alhameed, R. A. (2023). Binary classification of customer’s online purchasing behavior using machine learning. Journal of Techniques, 5(2), 163–186. https://doi.org/10.51173/jt.v5i2.1226

This reference highlights the strong performance of logistic regression compared to other models, which supports our conclusion where logistic regression ultimately outperformed other methods

>>>>>>> origin/main

# Methodology

The analysis began with data preparation, where the dataset of 12 variables, both categorical and numeric, was cleaned and transformed. The categorical variable Gender was converted to a binary factor, and the target variable, Choice, which indicated whether a customer purchased The Art History of Florence, was transformed into a binary indicator (1 for purchase, 0 for no purchase). Variables representing different genres of books purchased, such as P_Child, P_Youth, P_Cook, P_DIY, and P_Art, were retained as numeric variables reflecting the number of books purchased in each category.

Exploratory data analysis (EDA) was conducted to examine the distribution and relationships within the data. Histograms and box plots were generated to visualize the distribution of numeric variables such as Amount_purchased, Frequency, and Last_purchase based on the outcome variable Choice. Bar plots were used to explore the frequency of categorical variables like Gender. A correlation matrix was constructed to identify relationships among numeric variables and to detect potential multicollinearity issues.

The dataset initially provided was two pre-split sets: one for training and one for testing. However, these datasets were later combined for exploratory data analysis (EDA), correlation analysis, and visualization. The training set contained 80% of the data, and the test set comprised 20%. The combination allowed for comprehensive analysis while ensuring that model evaluation was still conducted on unseen data.

Several modeling techniques were explored, including logistic regression, linear discriminant analysis (LDA), and support vector machines (SVM). Logistic regression was selected as the primary technique due to its suitability for binary classification and its flexibility in optimizing sensitivity and specificity. A stepwise backward selection method, based on the Akaike Information Criterion (AIC), was used to remove insignificant variables and select the most relevant predictors. To address potential multicollinearity, variables with high variance inflation factor (VIF) values were removed.

Model performance was evaluated using accuracy, sensitivity, and specificity, with results summarized in a confusion matrix. To further address class imbalance in the test set, the decision threshold for classifying customers was adjusted to optimize model performance. By iterating over different threshold values, an optimal cutoff was determined that balanced sensitivity and specificity, enhancing the model’s ability to predict both purchasers and non-purchasers effectively.


Finally, a profitability analysis was conducted to evaluate the financial impact of the model. The cost of sending mailers and the revenue from book purchases were calculated to determine the overall profit for each modeling approach.

# Data

The dataset used for this analysis contains a total of 12 variables across both training and testing sets. These variables represent customer demographics, purchasing behavior, and preferences for various book genres at the Bookbinders Book Club (BBBC). The key target variable is Choice, which indicates whether a customer purchased The Art History of Florence. The data consists of both categorical and numeric variables.

```{r, echo=FALSE}
skim(combined)
```

A check for missing values was performed (anyNA()), and no missing data was detected, so no further imputation or cleaning steps were necessary in that regard.

The variable Observation was removed not due to multicollinearity but because it served as a unique identifier for each record and did not provide any predictive value for the analysis.We then converted categorical variables (Choice and Gender) into factors, and combined the training and testing datasets for further analysis or visualization.

During our exploratory data analysis (EDA), various visualizations were used to examine the distributions and relationships within the dataset. A correlation plot was created to assess the relationships among numeric variables such as Amount_purchased, Frequency, Last_purchase, First_purchase, and the number of different types of books purchased (e.g., P_Child, P_Youth, P_Cook, P_DIY, P_Art). This helped to identify any strong correlations or multicollinearity between the numeric features.

```{r, echo=FALSE}
combined %>% select_if(is.numeric) %>% cor() %>% corrplot(method = 'number')
```

```{r, echo=FALSE}
combox[[1]] + combox[[2]]
combox[[3]] + combox[[4]]
```

Bar plots were used to explore the distribution of categorical variables such as Gender and Choice (purchase or non-purchase). For example, a bar plot was generated to visualize the relationship between gender and purchase behavior, displaying the frequency of purchases and non-purchases among males and females. These visualizations provided insights into the key factors that might influence the likelihood of a customer purchasing a book.

```{r, echo=FALSE}
combbar[[1]] + combbar[[2]]
combbar[[3]] + combbar[[4]]
combbar[[5]]
combined %>% mutate(
  Gender = ifelse(Gender == 0, 'Female', 'Male'),
  Choice = ifelse(Choice == 0, 'Non-purchase', 'purchase')
  ) %>% 
  ggplot(aes(x = Gender, fill = Choice)) +
  geom_bar() + ggtitle('Gender vs Purchase')
```


# Findings

Upon initial assesment of our SVM on balanced data, the sensitivity of the model was relatively low, but this wasn't a significant issue given our objective. The model predicted that 160 out of 408 observations would likely purchase the book. To ensure the integrity of our model and data, we applied appropriate transformations and balanced the responses in both the training and test sets. However, since we have no knowledge of the distribution of responses in the actual mailing list audience, we cannot assume that it will be balanced. Therefore, it was important to validate our model's performance on an unbalanced dataset to ensure it remained effective in real-world scenarios, where the distribution of purchasers and non-purchasers may differ.

```{r, echo=FALSE}
SVM_CM
```

After applying the SVM model to the original unbalanced test dataset, the sensitivity and specificity metrics remained consistent with those observed in the balanced dataset. This outcome is logical because the distribution between positive (purchasers) and negative (non-purchasers) cases only affects the overall prevalence, not the fundamental calculations of sensitivity and specificity. Each metric remained robust regardless of changes in class distribution because they were calculated independently within each class. As a result, we could apply these performance metrics to a hypothetical, unbalanced dataset of random customers.

```{r, echo=FALSE}
SVM_CM_imbal
```


However, our overall analysis revealed that the logistic regression outperformed the other models in terms of overall prediction accuracy, particularly after the decision threshold was optimized. By adjusting the threshold, the model's sensitivity significantly improved, allowing it to correctly identify a larger number of customers who were likely to purchase the featured book. While the support vector machine (SVM) model initially demonstrated poor sensitivity due to the unbalanced nature of the dataset, its performance improved once the data was balanced. The SVM model showed strong specificity, meaning it effectively reduced false positives, but this came at the cost of lower sensitivity. Linear discriminant analysis (LDA) performed similarly to logistic regression but did not achieve the same level of sensitivity as the threshold-optimized logistic model.

```{r, echo=FALSE}
log_CM_unbal
```

How we gathered these findings were by first using data from the training and test sets, the proportion of people who are expected to purchase the book out of the 50,000 people in the mailing audience is calculated then storing this estimate in a column called 'newcnt'. We then used the model to estimate how many of the individuals in both the purchasing and non-purchasing groups would be predicted to buy the book called 'est_targets'.

The cost of sending mailers was calculated by multiplying the number of predicted buyers ("est_targets") by \$0.65 (the cost of each mailer) and called 'mailercst'.

For those who are predicted to buy the book, the total cost of the books and overhead (calculated as \$15 x 1.45) was estimated. These costs are only applied to those predicted to purchase the book ("newcnt" where Choice == 1), "purchcst" variable. The total revenue from book sales is calculated by multiplying the number of predicted buyers by \$31.95 (the price of the book). 'Revenue' is only generated when Choice == 1, and 'Profit' is calculated by subtracting both the mailer and book purchase costs from the total revenue.

We see by summing up the values in the "profit" we get a total expected profit from the mailer campaign. We see the comparison results here:

```{r, echo=FALSE}
data.frame(Model = c('Naive', 'LDA', 'Logit', 'SVM'), 
  Profit = c(
    sum(naive_table$profit),
    sum(summary_table_lda$profit),
    sum(summary_table_log$profit),
    sum(summary_table_svm$profit)
    )
  )
```


One of the primary challenges in the dataset was the inherent class imbalance, with significantly more non-purchasers than purchasers. To address this, the dataset was balanced by oversampling the minority class (purchasers), which improved the performance of both logistic regression and SVM models, particularly in terms of sensitivity. Even when tested on the original unbalanced dataset, the sensitivity and specificity metrics for both models remained consistent, indicating that the models were robust against changes in class distribution.

The analysis also highlighted a trade-off between sensitivity and specificity. Improving sensitivity was crucial for identifying a greater proportion of potential purchasers, which is the primary goal of the direct mail campaign. However, this improvement came at the cost of specificity, meaning that some mailers would be sent to non-purchasers, resulting in false positives. Despite this, the increased sensitivity was considered an acceptable trade-off, as the cost of sending mailers to non-purchasers is relatively low compared to the revenue generated from correctly identified purchasers.

Finally, the profitability analysis showed that the logistic regression model with an optimized threshold provided the best balance between sensitivity and specificity, leading to the highest potential profit for the campaign. The SVM model, while strong in terms of specificity, identified fewer purchasers overall, limiting its potential revenue generation. This analysis demonstrated that balancing the dataset and fine-tuning the decision threshold were critical steps in maximizing the effectiveness and profitability of the direct mail campaign.

```{r, echo=FALSE}
thresh_plot
```

```{r, echo=FALSE}
thresh[which(thresh$profit == max(thresh$profit)),]
```
In a similar study on "Binary Classification of Customer’s Online Purchasing Behavior Using Machine Learning", the strength of logistic regression compared to other models was also found: "A comparative study of ten classifiers is presented in [18]. Their accuracy indicator, i.e., the area under the curve (AUC), highlighted logistic regression as the best classifier. Naive Bayes, neural network, and support vector machine classifiers followed as runners-up, while decision tree-based classifiers tended to underperform."

# Conclusion

In conclusion, the logistic regression model ultimately performed the best in predicting customer purchases for the mailer campaign. 

By iterating through different decision thresholds, we identified the optimal threshold that maximized profit, adjusting the threshold down to 0.2 for the logistic model. While the naive method yielded higher revenue by reaching all potential buyers, using a predictive model like logistic regression or LDA with an optimized threshold helped balance mailer costs and capture more actual purchasers. This approach resulted in higher overall profitability by efficiently targeting customers most likely to buy the book, demonstrating that a carefully tuned predictive model provides a more cost-effective solution than the naive approach.
